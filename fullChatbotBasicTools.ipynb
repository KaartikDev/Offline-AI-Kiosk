{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c03167",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "This notebook will goes beyond context() tools.\n",
    "\n",
    "#### clarify(query, max_questions=2)\n",
    "- Looks at the user’s free-text request.  \n",
    "- Generates 1–2 short follow-up questions to narrow down context (e.g., “Adult or child?”, “Do you have clean cloth?”).  \n",
    "- **Purpose:** prevent wrong advice by gathering minimal extra info.  \n",
    "\n",
    "#### context(query, top_k=5)\n",
    "- Searches the core knowledge pack’s vector database of text chunks.  \n",
    "- Returns the top matching snippets with IDs, text, and relevance scores.  \n",
    "- **Purpose:** retrieve authoritative guidance from curated docs.  \n",
    "\n",
    "#### checklist(topic | chunks)\n",
    "- Takes either a topic name (e.g., “severe bleeding”) or retrieved text chunks.  \n",
    "- Converts them into a step-by-step action list with clear, ordered instructions.  \n",
    "- **Purpose:** make guidance actionable and easy to follow in a stressful moment.  \n",
    "\n",
    "#### cite(chunk_id)\n",
    "- Given a chunk ID, looks up the original source document metadata.  \n",
    "- Returns title, file path, page number, and date.  \n",
    "- **Purpose:** provide proof and transparency for where advice came from.  \n",
    "\n",
    "#### translate(text, target_lang=\"en\")\n",
    "- Translates user input or outputs into the specified language (default English).  \n",
    "- Also indicates detected source language and a confidence score.  \n",
    "- **Purpose:** support multilingual households and communities.  \n",
    "\n",
    "#### getImage(query, top_k=3)\n",
    "- Searches an image-caption vector database for diagrams/photos relevant to the query.  \n",
    "- Returns likely matches with captions and file paths.  \n",
    "- **Purpose:** give visual reinforcement (e.g., bandage diagrams, water purification images).  \n",
    "\n",
    "#### knowledgeMeta()\n",
    "- Reads pack metadata (from a manifest file).  \n",
    "- Returns the current version and last-updated date of the knowledge pack.  \n",
    "- **Purpose:** build trust by showing how recent and reliable the knowledge is.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a332a",
   "metadata": {},
   "source": [
    "###  Setting up paths and loading vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d166f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0) Imports & manifest ===\n",
    "from pathlib import Path\n",
    "import yaml, json\n",
    "from typing import Optional, List, Dict\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools import tool\n",
    "from langchain.schema import Document\n",
    "\n",
    "# If you want the prebuilt ReAct agent:\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# If you prefer to also expose tools directly on the LLM:\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# --- Set your pack root + manifest ---\n",
    "ROOT = Path(\"/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/first_aid_pack_demo_v2\")\n",
    "MANIFEST = ROOT / \"manifest.yaml\"\n",
    "\n",
    "with open(MANIFEST, \"r\", encoding=\"utf-8\") as f:\n",
    "    manifest = yaml.safe_load(f)\n",
    "\n",
    "# --- Resolve FAISS paths from manifest ---\n",
    "faiss_dir = ROOT / manifest[\"precomputed_indices\"][\"text\"][\"faiss\"][\"dir\"]\n",
    "\n",
    "# --- Create embeddings *matching the store* ---\n",
    "embed_model_name = manifest[\"embedding_config\"][\"text\"][\"model\"]     # e.g., \"granite-embedding:30m\"\n",
    "emb = OllamaEmbeddings(model=embed_model_name)\n",
    "\n",
    "# --- Load FAISS store + retriever ---\n",
    "vs = FAISS.load_local(str(faiss_dir), emb, allow_dangerous_deserialization=True)\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 4})  # default k; tool will override if provided\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93926748",
   "metadata": {},
   "source": [
    "### Defining helper and context() Tool  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18ea3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) Helpers ===\n",
    "def format_chunk(doc: Document, max_chars: int = 400) -> Dict:\n",
    "    \"\"\"Return a dict with compact text + key metadata for prompting & audit.\"\"\"\n",
    "    txt = doc.page_content.strip()\n",
    "    if len(txt) > max_chars:\n",
    "        txt = txt[:max_chars].rstrip() + \" …\"\n",
    "    m = doc.metadata\n",
    "    return {\n",
    "        \"id\": m.get(\"chunk_id\"),\n",
    "        \"topic_id\": m.get(\"topic_id\"),\n",
    "        \"file_id\": m.get(\"file_id\"),\n",
    "        \"locale\": m.get(\"locale\"),\n",
    "        \"path\": m.get(\"path\"),\n",
    "        \"citations\": [c.get(\"title\", \"\") for c in m.get(\"citations\", [])],\n",
    "        \"text\": txt\n",
    "    }\n",
    "\n",
    "def format_context_block(chunks: List[Dict]) -> str:\n",
    "    \"\"\"Human/LLM-friendly context block the agent can drop into its reasoning.\"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"### Retrieved Context (use only what is relevant)\")\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        cite_str = \"; \".join([t for t in c[\"citations\"] if t]) or \"—\"\n",
    "        head = f\"[{i}] {c['topic_id']} · {c['file_id']} · {c['locale']} · {c['path']}\"\n",
    "        lines.append(head)\n",
    "        lines.append(c[\"text\"])\n",
    "        lines.append(f\"Source(s): {cite_str}\")\n",
    "        lines.append(\"\")  # blank line\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "\n",
    "# === 2) The @tool: context() ===\n",
    "@tool\n",
    "def context(\n",
    "    query: str,\n",
    "    k: int = 4,\n",
    "    topic_id: Optional[str] = None,\n",
    "    locale: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve up to k relevant knowledge-pack chunks for 'query' and return a formatted\n",
    "    context block + structured per-chunk data for citations. You must use this tool for any prompt that is \n",
    "    important to wellbeing or safety of user. \n",
    "\n",
    "    Args:\n",
    "        query: Natural language question or keywords.\n",
    "        k: Top-k chunks to return (default 4).\n",
    "        topic_id: Optional manifest topic filter (e.g., 'bleed-control').\n",
    "        locale: Optional locale filter (e.g., 'hi_en' or 'en').\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"query\": str,\n",
    "          \"k\": int,\n",
    "          \"filters\": {\"topic_id\":..., \"locale\":...},\n",
    "          \"context_block\": str,     # pasteable into prompts\n",
    "          \"chunks\": [ {id, topic_id, file_id, path, locale, citations[], text}, ... ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Build a metadata filter if provided\n",
    "    _filter = {}\n",
    "    # if topic_id:\n",
    "    #     _filter[\"topic_id\"] = topic_id\n",
    "    # if locale:\n",
    "    #     _filter[\"locale\"] = locale\n",
    "\n",
    "    # Run retrieval (override k)\n",
    "    local_ret = vs.as_retriever(search_kwargs={\"k\": k})\n",
    "    hits: List[Document] = local_ret.invoke(query) if not _filter else local_ret.invoke(query, filter=_filter)\n",
    "\n",
    "    formatted = [format_chunk(d) for d in hits]\n",
    "    ctx_block = format_context_block(formatted)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"k\": k,\n",
    "        \"filters\": _filter,\n",
    "        \"context_block\": ctx_block,\n",
    "        \"chunks\": formatted\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b651",
   "metadata": {},
   "source": [
    "### Math Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbe93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract first number by second number.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a : float, b: float) -> float:\n",
    "    \"\"\"Divide first number by second number.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return a/b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69135961",
   "metadata": {},
   "source": [
    "### Defining knowledgeMeta() tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b002ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from typing import Optional\n",
    "\n",
    "@tool\n",
    "def knowledgeMeta(pack_dir: Optional[str] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Read a knowledge pack manifest and return metadata for trust & recency display.\n",
    "\n",
    "    Args:\n",
    "      pack_dir: Absolute or relative path to the pack folder (containing manifest.yaml).\n",
    "                If omitted, uses the default ROOT pack path.\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"name\": str,\n",
    "        \"version\": str,\n",
    "        \"date\": str,\n",
    "        \"locales\": [..],\n",
    "        \"topics_count\": int,\n",
    "        \"manifest_path\": str\n",
    "      }\n",
    "    \"\"\"\n",
    "    # default to your earlier ROOT if not provided\n",
    "    base = Path(pack_dir) if pack_dir else ROOT\n",
    "    manifest_path = base / \"manifest.yaml\"\n",
    "    if not manifest_path.exists():\n",
    "        return {\"error\": f\"manifest.yaml not found at {manifest_path}\"}\n",
    "\n",
    "    with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        m = yaml.safe_load(f)\n",
    "\n",
    "    name = m.get(\"name\", str(base.name))\n",
    "    version = m.get(\"version\", \"unknown\")\n",
    "    date = m.get(\"date\", \"unknown\")\n",
    "    locales = m.get(\"locales\", [])\n",
    "    topics = m.get(\"index_of_topics\", []) or []\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"version\": version,\n",
    "        \"date\": date,\n",
    "        \"locales\": locales,\n",
    "        \"topics_count\": len(topics),\n",
    "        \"manifest_path\": str(manifest_path)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad48bd",
   "metadata": {},
   "source": [
    "### List of Available Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c9825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a Tools list for whichever orchestration you choose:\n",
    "TOOLS = [context,add,multiply,subtract,divide,knowledgeMeta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22fb2c",
   "metadata": {},
   "source": [
    "### Setting Up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a99997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=\"ollama:gpt-oss:20b\",       \n",
    "    temperature=0.2  # lower = more deterministic\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(TOOLS) #llm_with_tools is a new wrapped llm\n",
    "model = llm_with_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d161dfa",
   "metadata": {},
   "source": [
    "### Testing Tool Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e467a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What do if bleeding?\"\n",
    "# model.invoke(query) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0bf28",
   "metadata": {},
   "source": [
    "### Reason - Act Agent Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "450b94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Annotated, #Extra metadata\n",
    "    Sequence, #generic list container\n",
    "    TypedDict, #Lets you define a dictionary type with fixed keys and types\n",
    ") #a dictionary type has a specific format of what key and valye types can be \n",
    "from langchain_core.messages import BaseMessage #lang chain message format (Human,Systen,AI)\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    # add_messages is a reducer\n",
    "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "#define a state dictionary, 1 key names messages\n",
    "#messages is a list of langchain base messages\n",
    "#wrapping it in Annotated tells it to not replace entire list but use add_messages to merge it in\n",
    "#The accepected value  for this dictionary will be Sequence[BaseMessage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07e24e",
   "metadata": {},
   "source": [
    "### Defining nodes and edges in \"Lang graph\"\n",
    "\n",
    "Lang graph:\n",
    "- Imagine you're in a big construction truck and you have a map of stops\n",
    "- Each stop pick up something/do a \n",
    "- Folow the street signs to get to all the stops\n",
    "- Take a photo to save state during important jobs\n",
    "\n",
    "The big truck carries chat histroy. The road signs are edges between nodes where nodes are the stops. The photos are state changes you can reload to(checkpoint in video game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e467c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "tools_dict = {\"context\": context, \"knowledgeMeta\":knowledgeMeta,\"add\":add,\"subtract\":subtract,\"divide\":divide,\"multiply\":multiply}\n",
    "\n",
    "tools_by_name = tools_dict\n",
    "\n",
    "\n",
    "# Define our tool node\n",
    "def tool_node(state: AgentState): #Agent State is the conversation history\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls: #Look at the last message in history(should be AIMessage)\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"]) #Actually call the tool\n",
    "        outputs.append( #Add a json output to conco hsitory\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "# Define the node that calls the model\n",
    "def call_model(\n",
    "    state: AgentState,\n",
    "    config: RunnableConfig,\n",
    "):\n",
    "    temp_system_str = \"For ANY health or safety query, you MUST call the `context` tool.  Do not answer from memory.\"\n",
    "    # this is similar to customizing the create_react_agent with 'prompt' parameter, but is more flexible\n",
    "    system_prompt = SystemMessage(\n",
    "        \"You are a helpful AI assistant, please respond to the users query to the best of your ability!\" + temp_system_str\n",
    "    )\n",
    "    response = model.invoke([system_prompt] + state[\"messages\"], config)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the conditional edge that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745cd75",
   "metadata": {},
   "source": [
    "### Defining new Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94e222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'Exception'>\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"tools\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Now we can compile and visualize our graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is \n",
    "    print(Exception)\n",
    "    pass\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47485d2b",
   "metadata": {},
   "source": [
    "### Print output stream function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dcb80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting the stream nicely\n",
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        prev_msg = \"\"\n",
    "        if len(s['messages']) > 1:\n",
    "            prev_msg = s[\"messages\"][-2]\n",
    "\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "        # if prev_msg != \"\":\n",
    "        #     if isinstance(prev_msg, tuple):\n",
    "        #         print(prev_msg)\n",
    "        #     else:\n",
    "        #         prev_msg.pretty_print()\n",
    "\n",
    "\n",
    "# inputs = {\"messages\": [(\"user\", \"What to do if bleed? When Knowledge updated? What 42*321 and 32/31?\")]}\n",
    "# print_stream(graph.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cd475",
   "metadata": {},
   "source": [
    "### Infinte Loop Chat Bot\n",
    "\n",
    "After eahc exit please restart kernel and clear all outputs to ensure a fresh conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f493ab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a message. Commands: /reset, /exit\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! How can I help you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Subtract 123456 from 999999.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\\(999{,}999 - 123{,}456 = 876{,}543\\).\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is 2025 ÷ 31?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\\(2025 \\div 31 = 65\\) with a remainder of \\(10\\).\n",
      "\n",
      "In decimal form:\n",
      "\n",
      "\\[\n",
      "2025 \\div 31 \\approx 65.322580645\\ldots\n",
      "\\]\n",
      "\n",
      "So the exact result is \\(\\displaystyle \\frac{2025}{31}\\), which is about **65.3226**.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "good job\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thank you! If you have any more questions or need assistance, just let me know.\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "# === Simple REPL loop for Jupyter ===\n",
    "def chat_loop():\n",
    "    history = []\n",
    "    print(\"Type a message. Commands: /reset, /exit\\n\")\n",
    "    while True:\n",
    "        user = input(\"You: \").strip()\n",
    "        if not user:\n",
    "            continue\n",
    "        if user.lower() in {\"/exit\", \"/quit\"}:\n",
    "            print(\"Bye!\")\n",
    "            break\n",
    "        if user.lower() == \"/reset\":\n",
    "            history = []\n",
    "            print(\"(conversation cleared)\")\n",
    "            continue\n",
    "\n",
    "        # Append user turn\n",
    "        inputs = {\"messages\": history + [(\"user\", user)]}\n",
    "\n",
    "        # Use YOUR print_stream exactly as written\n",
    "        print_stream(graph.stream(inputs, stream_mode=\"values\"))\n",
    "\n",
    "        # Capture updated history so context persists\n",
    "        final_state = graph.invoke(inputs)\n",
    "        history = final_state[\"messages\"]\n",
    "\n",
    "# Run this cell in your notebook to start chatting\n",
    "chat_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ddfc1",
   "metadata": {},
   "source": [
    ":)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff1846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded796d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
