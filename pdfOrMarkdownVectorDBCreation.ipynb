{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82abbd78",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook embeds all PDF documents + citations avaible in a FAISS vector databse with langchain and nomic-embed-text:v1.5. Not needed to run beacon.\n",
    "```\n",
    "ollama pull nomic-embed-text:v1.5\n",
    "``` \n",
    "Ensure you have installed requirements.txt. WARNING: This notebook can take several minutes to run for large knowledge packs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d0e07",
   "metadata": {},
   "source": [
    "### Importing and Paths\n",
    "\n",
    "Change the ROOT  paths as needed. It should point to to the main knowledge pack dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad972735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Packs/Bihar India Support Kpack\n",
      "Knowledge Packs/Bihar India Support Kpack/manifest.yaml\n"
     ]
    }
   ],
   "source": [
    "# --- A. Imports & config ---\n",
    "from pathlib import Path\n",
    "import json, hashlib, uuid, yaml\n",
    "from typing import List, Dict\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "which_pack = input(\"Input 'FLORIDA' to embded CORE DOCUMENTS for hurricane kpack or'INDIA' kpack: \")\n",
    "ROOT = Path('.')\n",
    "if which_pack == 'FLORIDA':\n",
    "    ROOT = Path('Knowledge Packs/Pinellas County Floirda Hurricane Response Kpack')\n",
    "else:\n",
    "    ROOT = Path('Knowledge Packs/Bihar India Support Kpack')\n",
    "\n",
    "MANIFEST = ROOT / \"manifest.yaml\"\n",
    "print(ROOT)\n",
    "print(MANIFEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb7439",
   "metadata": {},
   "source": [
    "### Parsing YAML, Embedding Documents, and Creating Vector Store  \n",
    "\n",
    "**Run this step only once.**  \n",
    "\n",
    "**Notes:**  \n",
    "1. If a `vector_db` directory does not exist inside the knowledge pack, a new one will be created.  \n",
    "   - Example: `Knowledge Packs/Bihar India Support Kpack/vector_db`  \n",
    "   - This directory will contain the `.faiss` index and pickle metadata file.  \n",
    "\n",
    "2. If the directory already exists, the files inside will be **overwritten**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d36beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FAISS dir: Knowledge Packs/Bihar India Support Kpack/vector_db/text/faiss_index\n",
      "Embeddings JSONL: Knowledge Packs/Bihar India Support Kpack/vector_db/text/embeddings.jsonl\n",
      "Meta JSON: Knowledge Packs/Bihar India Support Kpack/vector_db/text/meta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting cache for 0 5692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! PDF has no extractable text (scanned images?): Knowledge Packs/Bihar India Support Kpack/core/rash/hi_en/IntegratedPartheniumManagement(Elglish)-Folder.pdf\n",
      "! PDF has no extractable text (scanned images?): Knowledge Packs/Bihar India Support Kpack/core/wild-animals/hi_en/Advisory for Priority of Action for State Govt-Human Wildlife Conflict_0.pdf\n",
      "! PDF has no extractable text (scanned images?): Knowledge Packs/Bihar India Support Kpack/core/transport/hi_en/SOP-OF-AMBULANCE2020IPTHHScompressed.pdf\n",
      "! PDF has no extractable text (scanned images?): Knowledge Packs/Bihar India Support Kpack/core/contacts/hi_en/phc_list_bihar.pdf\n",
      "! Skipping missing file: /core/education/hi_en/User_Manual_for_BSCCpdf1.pdf\n",
      "Prepared 6889 text chunks from 31 PDFs and 0 MD files (skipped empty/scan-only: 4)\n",
      "FAISS artifacts saved ✅ index.faiss & index.pkl\n",
      "JSONL/meta saved ✅\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import yaml\n",
    "\n",
    "# LangChain docs object (compat across versions)\n",
    "try:\n",
    "    from langchain_core.documents import Document\n",
    "except ImportError:\n",
    "    from langchain.schema import Document  # older LC\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "ans = input(\"EMBDED (may take 10+ mins) Y/N? \")\n",
    "if ans == \"Y\":\n",
    "    # === PDF & Markdown chunk & embed (manifest-driven) ===\n",
    "    # deps for PDFs: pip install pypdf\n",
    "\n",
    "    # -------------------- 0) Load manifest & resolve paths --------------------\n",
    "    with open(MANIFEST, \"r\", encoding=\"utf-8\") as f:\n",
    "        manifest = yaml.safe_load(f)\n",
    "\n",
    "    # Embedding + chunking config from manifest (with sensible fallbacks)\n",
    "    embed_model_name = manifest[\"embedding_config\"][\"text\"][\"model\"]  # e.g., \"granite-embedding:278m\"\n",
    "    normalize   = bool(manifest[\"embedding_config\"][\"text\"].get(\"normalize\", True))\n",
    "    max_tokens  = int(manifest[\"embedding_config\"][\"text\"][\"chunking\"].get(\"max_tokens\", 512))\n",
    "    overlap_toks= int(manifest[\"embedding_config\"][\"text\"][\"chunking\"].get(\"overlap_tokens\", 64))\n",
    "\n",
    "    # very rough char≈token conversion for character-based splitter\n",
    "    TOK_TO_CHAR = 4  # tune if your corpora are very different\n",
    "    chunk_size_chars = max(64, max_tokens * TOK_TO_CHAR)\n",
    "    overlap_chars    = max(0,  overlap_toks * TOK_TO_CHAR)\n",
    "\n",
    "    # Resolve precomputed index paths from manifest\n",
    "    text_idx_cfg        = manifest[\"precomputed_indices\"][\"text\"]\n",
    "    embeddings_path     = ROOT / text_idx_cfg[\"embeddings\"]             # \"vector_db/text/embeddings.jsonl\"\n",
    "    meta_path           = ROOT / text_idx_cfg[\"meta\"]                   # \"vector_db/text/meta.json\"\n",
    "    faiss_dir           = ROOT / text_idx_cfg[\"faiss\"][\"dir\"]           # \"vector_db/text/faiss_index\"\n",
    "    faiss_index_path    = ROOT / text_idx_cfg[\"faiss\"][\"index\"]         # \".../index.faiss\"\n",
    "    faiss_docstore_path = ROOT / text_idx_cfg[\"faiss\"][\"docstore\"]      # \".../index.pkl\"\n",
    "\n",
    "    faiss_dir.mkdir(parents=True, exist_ok=True)\n",
    "    embeddings_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    meta_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Using FAISS dir:\", faiss_dir)\n",
    "    print(\"Embeddings JSONL:\", embeddings_path)\n",
    "    print(\"Meta JSON:\", meta_path)\n",
    "\n",
    "    # -------------------- 1) Extraction helpers --------------------\n",
    "    # PDFs\n",
    "    from pypdf import PdfReader\n",
    "\n",
    "    def extract_pdf_pages(pdf_path: Path) -> List[Tuple[int, str]]:\n",
    "        \"\"\"Return list of (1-indexed page_number, text). Empty pages become ''.\"\"\"\n",
    "        pages: List[Tuple[int, str]] = []\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            txt = page.extract_text() or \"\"\n",
    "            txt = txt.replace(\"\\u00A0\", \" \").strip()\n",
    "            pages.append((i + 1, txt))\n",
    "        return pages\n",
    "\n",
    "    # Markdown\n",
    "    FM_RE = re.compile(r\"^\\s*---\\s*\\n(.*?)\\n---\\s*\\n?\", re.DOTALL)\n",
    "\n",
    "    def strip_markdown_syntax(md: str) -> str:\n",
    "        \"\"\"Lightweight MD→text. Keeps content, removes common syntax; OK for embeddings.\"\"\"\n",
    "        # remove code fences\n",
    "        md = re.sub(r\"```.*?```\", \"\", md, flags=re.DOTALL)\n",
    "        # remove inline code backticks\n",
    "        md = md.replace(\"`\", \"\")\n",
    "        # images/links: keep label + URL text-ish\n",
    "        md = re.sub(r\"!\\[([^\\]]*)\\]\\([^\\)]*\\)\", r\"\\1\", md)\n",
    "        md = re.sub(r\"\\[([^\\]]+)\\]\\([^\\)]*\\)\", r\"\\1\", md)\n",
    "        # headings/bold/italics\n",
    "        md = re.sub(r\"^\\s{0,3}#{1,6}\\s*\", \"\", md, flags=re.MULTILINE)\n",
    "        md = re.sub(r\"[*_]{1,3}([^*_]+)[*_]{1,3}\", r\"\\1\", md)\n",
    "        # blockquotes / lists / tables pipes\n",
    "        md = re.sub(r\"^\\s{0,3}>\\s?\", \"\", md, flags=re.MULTILINE)\n",
    "        md = re.sub(r\"^\\s*[-*+]\\s+\", \"\", md, flags=re.MULTILINE)\n",
    "        md = re.sub(r\"^\\s*\\d+\\.\\s+\", \"\", md, flags=re.MULTILINE)\n",
    "        md = md.replace(\"|\", \" \")\n",
    "        # collapse whitespace\n",
    "        md = re.sub(r\"[ \\t]+\", \" \", md)\n",
    "        md = re.sub(r\"\\n{3,}\", \"\\n\\n\", md)\n",
    "        return md.strip()\n",
    "\n",
    "    def extract_markdown_blocks(md_path: Path) -> Tuple[Optional[dict], str]:\n",
    "        \"\"\"\n",
    "        Returns (front_matter_dict_or_none, plain_text_body).\n",
    "        Front matter (if present) is parsed as YAML and removed from body.\n",
    "        \"\"\"\n",
    "        raw = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        fm_match = FM_RE.match(raw)\n",
    "        front = None\n",
    "        if fm_match:\n",
    "            try:\n",
    "                front = yaml.safe_load(fm_match.group(1)) or {}\n",
    "            except Exception:\n",
    "                front = {\"_parse_error\": \"front_matter\"}\n",
    "            raw = raw[fm_match.end():]\n",
    "        text = strip_markdown_syntax(raw)\n",
    "        return front, text\n",
    "\n",
    "    # -------------------- 2) Chunker (character-based with overlap) --------------------\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"। \", \". \", \"?\", \"!\", \" \"],\n",
    "        chunk_size=chunk_size_chars,\n",
    "        chunk_overlap=overlap_chars,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    def chunk_text(text: str) -> List[str]:\n",
    "        if not text:\n",
    "            return []\n",
    "        return [c.strip() for c in splitter.split_text(text) if c.strip()]\n",
    "\n",
    "    # -------------------- 3) Build Documents (PDF + MD aware) --------------------\n",
    "    docs: List[Document] = []\n",
    "    pack_name    = manifest.get(\"name\")\n",
    "    pack_ver     = manifest.get(\"version\")\n",
    "    pack_locales = manifest.get(\"locales\", [])\n",
    "\n",
    "    citations_by_id = {c[\"id\"]: c for c in manifest.get(\"citations\", [])}\n",
    "\n",
    "    num_pdf_files = 0\n",
    "    num_md_files = 0\n",
    "    num_skipped_empty = 0\n",
    "\n",
    "    def infer_locale(path_str: str, default: str = \"en\") -> str:\n",
    "        # Keep your existing heuristic; extend if you add more\n",
    "        return \"hi_en\" if \"/hi_en/\" in path_str else default\n",
    "\n",
    "    for topic in manifest.get(\"index_of_topics\", []):\n",
    "        topic_id = topic[\"id\"]\n",
    "        for fmeta in topic.get(\"core_files\", []):\n",
    "            fpath = ROOT / fmeta[\"path\"]\n",
    "            if not fpath.exists():\n",
    "                print(\"! Skipping missing file:\", fpath)\n",
    "                continue\n",
    "\n",
    "            media_type = (fmeta.get(\"media_type\") or fpath.suffix.lstrip(\".\")).lower()\n",
    "            is_pdf = (media_type == \"pdf\") or (fpath.suffix.lower() == \".pdf\")\n",
    "            is_md  = (media_type in {\"md\", \"markdown\"}) or (fpath.suffix.lower() in {\".md\", \".markdown\"})\n",
    "\n",
    "            if not (is_pdf or is_md):\n",
    "                # not a core text doc type we embed here\n",
    "                continue\n",
    "\n",
    "            locale = infer_locale(fmeta[\"path\"], fmeta.get(\"locale\", \"en\"))\n",
    "            c_full = [citations_by_id[cid] for cid in fmeta.get(\"citations\", []) if cid in citations_by_id]\n",
    "\n",
    "            file_chunk_counter = 0\n",
    "\n",
    "            if is_pdf:\n",
    "                pages = extract_pdf_pages(fpath)\n",
    "                if not any(p_txt for _, p_txt in pages):\n",
    "                    print(f\"! PDF has no extractable text (scanned images?): {fpath}\")\n",
    "                    num_skipped_empty += 1\n",
    "                    continue\n",
    "\n",
    "                for page_num, page_text in pages:\n",
    "                    for piece in chunk_text(page_text):\n",
    "                        docs.append(\n",
    "                            Document(\n",
    "                                page_content=piece,\n",
    "                                metadata={\n",
    "                                    \"pack_name\": pack_name,\n",
    "                                    \"pack_version\": pack_ver,\n",
    "                                    \"topic_id\": topic_id,\n",
    "                                    \"file_id\": fmeta[\"id\"],\n",
    "                                    \"path\": str(fmeta[\"path\"]),\n",
    "                                    \"media_type\": \"pdf\",\n",
    "                                    \"locale\": locale,\n",
    "                                    \"citations\": c_full,\n",
    "                                    \"page\": page_num,\n",
    "                                    \"chunk_index\": file_chunk_counter,\n",
    "                                    \"chunk_id\": f\"{fmeta['id']}::p{page_num}::chunk::{file_chunk_counter}\",\n",
    "                                    \"doc_type\": \"pdf\",\n",
    "                                },\n",
    "                            )\n",
    "                        )\n",
    "                        file_chunk_counter += 1\n",
    "                num_pdf_files += 1\n",
    "\n",
    "            elif is_md:\n",
    "                front_matter, body_text = extract_markdown_blocks(fpath)\n",
    "                if not body_text.strip():\n",
    "                    print(f\"! Markdown empty after stripping syntax: {fpath}\")\n",
    "                    num_skipped_empty += 1\n",
    "                    continue\n",
    "\n",
    "                for piece in chunk_text(body_text):\n",
    "                    docs.append(\n",
    "                        Document(\n",
    "                            page_content=piece,\n",
    "                            metadata={\n",
    "                                \"pack_name\": pack_name,\n",
    "                                \"pack_version\": pack_ver,\n",
    "                                \"topic_id\": topic_id,\n",
    "                                \"file_id\": fmeta[\"id\"],\n",
    "                                \"path\": str(fmeta[\"path\"]),\n",
    "                                \"media_type\": \"md\",\n",
    "                                \"locale\": locale,\n",
    "                                \"citations\": c_full,\n",
    "                                \"page\": None,  # no pages for MD\n",
    "                                \"front_matter\": front_matter or {},\n",
    "                                \"chunk_index\": file_chunk_counter,\n",
    "                                \"chunk_id\": f\"{fmeta['id']}::md::chunk::{file_chunk_counter}\",\n",
    "                                \"doc_type\": \"markdown\",\n",
    "                            },\n",
    "                        )\n",
    "                    )\n",
    "                    file_chunk_counter += 1\n",
    "                num_md_files += 1\n",
    "\n",
    "    print(\n",
    "        f\"Prepared {len(docs)} text chunks \"\n",
    "        f\"from {num_pdf_files} PDFs and {num_md_files} MD files \"\n",
    "        f\"(skipped empty/scan-only: {num_skipped_empty})\"\n",
    "    )\n",
    "\n",
    "    # -------------------- 4) Embeddings + FAISS persist --------------------\n",
    "    emb = OllamaEmbeddings(model=embed_model_name)\n",
    "    vs = FAISS.from_documents(docs, emb)\n",
    "    vs.save_local(str(faiss_dir))  # writes index.faiss + index.pkl (overwrites if they exist)\n",
    "\n",
    "    # sanity check\n",
    "    assert faiss_index_path.exists(), f\"Missing {faiss_index_path}\"\n",
    "    assert faiss_docstore_path.exists(), f\"Missing {faiss_docstore_path}\"\n",
    "    print(\"FAISS artifacts saved ✅\", faiss_index_path.name, \"&\", faiss_docstore_path.name)\n",
    "\n",
    "    # -------------------- 5) Export JSONL embeddings + meta (portable) --------------------\n",
    "    records = []\n",
    "    doc_items = getattr(vs.docstore, \"_dict\", {})  # (doc_id -> Document), common LC pattern\n",
    "\n",
    "    for doc_id, doc in doc_items.items():\n",
    "        vec = emb.embed_query(doc.page_content)  # dim depends on your Ollama embedding model\n",
    "        rec = {\n",
    "            \"id\": doc_id,\n",
    "            \"embedding\": vec,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"text\": doc.page_content,\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    with open(embeddings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"model\": embed_model_name,\n",
    "                \"dim\": manifest[\"embedding_config\"][\"text\"].get(\"dim\"),\n",
    "                \"normalize\": normalize,\n",
    "                \"count\": len(records),\n",
    "                \"pack\": {\"name\": pack_name, \"version\": pack_ver, \"locales\": pack_locales},\n",
    "                \"chunking\": {\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"overlap_tokens\": overlap_toks,\n",
    "                    \"approx_chars_per_token\": TOK_TO_CHAR,\n",
    "                    \"chunk_size_chars\": chunk_size_chars,\n",
    "                    \"overlap_chars\": overlap_chars,\n",
    "                },\n",
    "                \"supported_media_types\": [\"pdf\", \"md\", \"markdown\"],\n",
    "            },\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(\"JSONL/meta saved ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158dcd9",
   "metadata": {},
   "source": [
    "### Testing it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad040d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "103 \n",
      "D.10 BLEEDING FROM VARICOSE VEINS \n",
      "Swollen knobbly dilated veins, as often found in the legs, are called varices. Bleeding from \n",
      "varicose veins might be very profuse. \n",
      "In case of a varicose vein bleeding: \n",
      "1. Ask the person to lie down on the floor. \n",
      " \n",
      "2. Raise and support the affected leg (thi ...\n",
      "\n",
      "[2]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "87 \n",
      " \n",
      "Alternatively, if possible, ask the injured to press on the bleeding wound himself to stop \n",
      "the bleeding. \n",
      " \n",
      " \n",
      "8. If you have a piece of clean (cotton) cloth, then cover the wound with it. \n",
      "If you have no bandages, improvise with other materials. \n",
      " \n",
      "9. You can also wrap a bandage around the wo ...\n",
      "\n",
      "[3]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "91 \n",
      " \n",
      " \n",
      "6. If th e bandage becomes soaked in blood, do not remove it, but add another \n",
      "bandage on top of it and continue to apply pressure. \n",
      "7. Take off jewels or anything else in the area of the wound that may cut off blood \n",
      "flow because of swelling. Keep the jewels and belongi ngs with the owner o ...\n",
      "\n",
      "[4]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "88 \n",
      "Only apply a tourniquet: \n",
      " if the bleeding of an external limb cannot be stopped by putting direct pressure \n",
      "on the wound, or \n",
      " if there are many casualties you have to give help to, and \n",
      " the first aider has been well trained on how to apply a tourniquet. \n",
      "If a tourniquet is applied on a ble ...\n",
      "\n",
      "[5]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "85 \n",
      "D.4 FIRST AID FOR BLEEDING (IN GENERAL) \n",
      "D.4.1 WHAT DO I SEE AND ENQUIRE? \n",
      "A person who has an injury which is bleeding severely is in a life-threatening situation and n e e d s \n",
      "immediate help. Therefore, stopping the bleeding is a core first aid activity. In addition, \n",
      "bleeding in the face or  ...\n",
      "\n",
      "[6]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "90 \n",
      " \n",
      "3. Build up padding around the object until you can bandage over it without pressing \n",
      "down. \n",
      " \n",
      "4. Bandage the material above and below the object with a piece of clean (cotton) \n",
      "cloth or improvise with other materials. \n",
      "Make sure the bandage is firm enough so it stops the bleeding but doesn’t  ...\n",
      "\n",
      "[7]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "86 \n",
      "D.4.2 WHAT DO I DO? \n",
      "D.4.2.1 SAFETY FIRST AND CALL FOR HELP \n",
      "1. Make sure there is no danger to you and the person. \n",
      "2. The person urgently needs help.  Shout or call for help if you are alone but do not \n",
      "leave the person  unattended. Ask a bystander to seek help or to arrange urgent \n",
      "transport  ...\n",
      "\n",
      "[8]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "96 \n",
      "D.6 SMALL CUTS AND ABRASIONS \n",
      "Even if the injured person has a small cut or abrasions, you  will still need to take care of the \n",
      "wound to stop the bleeding and to prevent infection. \n",
      "D.6.1 WHAT DO I SEE AND ENQUIRE ?  \n",
      "You might observe the following signs on a person with a cut or abrasions: \n",
      " ...\n",
      "\n",
      "[9]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "100 \n",
      "In a few cases a nose bleed can be serious and lead to death.  \n",
      "The person should seek medical help if: \n",
      " blood is still coming from the nose after 20 minutes; \n",
      " the nosebleed was caused by a hard punch on the nose; a fall, a road \n",
      "accident, etc.; \n",
      " blood spurts from the nose; \n",
      " the person  ...\n",
      "\n",
      "[10]\n",
      "Topic: bleeding\n",
      "File: bleeding-ifrc-guidelines-2020\n",
      "Locale: hi_en\n",
      "Citations: ['International First Aid and Resuscitation Guidelines 2020 — International Federation of Red Cross and Red Crescent Societies']\n",
      "Chunk text:\n",
      "dressing should be applied with direct pressure.*\n",
      "• First aid providers should not use pressure points for severe, life-threatening external bleeding.**\n",
      "Good practice points\n",
      "• Emergency medical services (EMS) should be accessed for all severe bleeding.\n",
      "• The first aid provider should protect themsel ...\n"
     ]
    }
   ],
   "source": [
    "# Typical retriever usage\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 10})  # if you used Option A 'vs'\n",
    "query = \"What to do for bleeding?\"  #\n",
    "hits = retriever.invoke(query)\n",
    "\n",
    "for i, d in enumerate(hits, 1):\n",
    "    print(f\"\\n[{i}]\")\n",
    "    print(\"Topic:\", d.metadata[\"topic_id\"])\n",
    "    print(\"File:\", d.metadata[\"file_id\"])\n",
    "    print(\"Locale:\", d.metadata[\"locale\"])\n",
    "    print(\"Citations:\", [c[\"title\"] for c in d.metadata.get(\"citations\", [])])\n",
    "    print(\"Chunk text:\")\n",
    "    print(d.page_content[:300], \"...\" if len(d.page_content) > 300 else \"\")\n",
    "\n",
    "# Filter to a topic or locale:\n",
    "# hits = retriever.invoke(\"tourniquet steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe1544",
   "metadata": {},
   "source": [
    ":)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2a8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
