{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82abbd78",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook embeds all PDF documents + citations avaible in a FAISS vector databse with langchain and nomic-embed-text:v1.5. Expands on previosu text embeddings notebook.\n",
    "\n",
    "```\n",
    "ollama pull nomic-embed-text:v1.5\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d0e07",
   "metadata": {},
   "source": [
    "### Importing and Paths\n",
    "\n",
    "Change the ROOT  paths as needed. It should point to to the main knowledge pack dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad972735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Packs/Bihar India Support Kpack\n",
      "Knowledge Packs/Bihar India Support Kpack/manifest.yaml\n"
     ]
    }
   ],
   "source": [
    "# --- A. Imports & config ---\n",
    "from pathlib import Path\n",
    "import json, hashlib, uuid, yaml\n",
    "from typing import List, Dict\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "which_pack = input(\"Input 'FLORIDA' to embded CORE DOCUMENTS for hurricane kpack or'INDIA' kpack: \")\n",
    "ROOT = Path('.')\n",
    "if which_pack == 'FLORIDA':\n",
    "    ROOT = Path('Knowledge Packs/Pinellas County Floirda Hurricane Response Kpack')\n",
    "else:\n",
    "    ROOT = Path('Knowledge Packs/Bihar India Support Kpack')\n",
    "\n",
    "MANIFEST = ROOT / \"manifest.yaml\"\n",
    "print(ROOT)\n",
    "print(MANIFEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb7439",
   "metadata": {},
   "source": [
    "### Parsing YAML, Embedding Documents, and Creating Vector Store\n",
    "\n",
    "NOTES: \n",
    "1. If avector_db directory inisde the knolwedge does not excist a new one will be created:\n",
    "- Example: first_aid_pack_demo_v2/vector_db/... <br>\n",
    "This directory will have the actual .faiss store and index pickle file\n",
    "\n",
    "2. Else the files will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d36beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FAISS dir: Knowledge Packs/Bihar India Support Kpack/vector_db/text/faiss_index\n",
      "Embeddings JSONL: Knowledge Packs/Bihar India Support Kpack/vector_db/text/embeddings.jsonl\n",
      "Meta JSON: Knowledge Packs/Bihar India Support Kpack/vector_db/text/meta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting cache for 0 5692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! PDF has no extractable text (scanned images?): Knowledge Packs/Bihar India Support Kpack/core/rash/hi_en/IntegratedPartheniumManagement(Elglish)-Folder.pdf\n",
      "! PDF has no extractable text (scanned images?): Knowledge Packs/Bihar India Support Kpack/core/wild-animals/hi_en/Advisory for Priority of Action for State Govt-Human Wildlife Conflict_0.pdf\n",
      "! PDF has no extractable text (scanned images?): Knowledge Packs/Bihar India Support Kpack/core/transport/hi_en/SOP-OF-AMBULANCE2020IPTHHScompressed.pdf\n",
      "! PDF has no extractable text (scanned images?): Knowledge Packs/Bihar India Support Kpack/core/contacts/hi_en/phc_list_bihar.pdf\n",
      "! Skipping missing file: /core/education/hi_en/User_Manual_for_BSCCpdf1.pdf\n",
      "Prepared 6889 text chunks from 31 PDFs and 0 MD files (skipped empty/scan-only: 4)\n",
      "FAISS artifacts saved ✅ index.faiss & index.pkl\n",
      "JSONL/meta saved ✅\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import yaml\n",
    "\n",
    "# LangChain docs object (compat across versions)\n",
    "try:\n",
    "    from langchain_core.documents import Document\n",
    "except ImportError:\n",
    "    from langchain.schema import Document  # older LC\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "ans = input(\"EMBDED (may take 10+ mins) Y/N? \")\n",
    "if ans == \"Y\":\n",
    "    # === PDF & Markdown chunk & embed (manifest-driven) ===\n",
    "    # deps for PDFs: pip install pypdf\n",
    "\n",
    "    # -------------------- 0) Load manifest & resolve paths --------------------\n",
    "    with open(MANIFEST, \"r\", encoding=\"utf-8\") as f:\n",
    "        manifest = yaml.safe_load(f)\n",
    "\n",
    "    # Embedding + chunking config from manifest (with sensible fallbacks)\n",
    "    embed_model_name = manifest[\"embedding_config\"][\"text\"][\"model\"]  # e.g., \"granite-embedding:278m\"\n",
    "    normalize   = bool(manifest[\"embedding_config\"][\"text\"].get(\"normalize\", True))\n",
    "    max_tokens  = int(manifest[\"embedding_config\"][\"text\"][\"chunking\"].get(\"max_tokens\", 512))\n",
    "    overlap_toks= int(manifest[\"embedding_config\"][\"text\"][\"chunking\"].get(\"overlap_tokens\", 64))\n",
    "\n",
    "    # very rough char≈token conversion for character-based splitter\n",
    "    TOK_TO_CHAR = 4  # tune if your corpora are very different\n",
    "    chunk_size_chars = max(64, max_tokens * TOK_TO_CHAR)\n",
    "    overlap_chars    = max(0,  overlap_toks * TOK_TO_CHAR)\n",
    "\n",
    "    # Resolve precomputed index paths from manifest\n",
    "    text_idx_cfg        = manifest[\"precomputed_indices\"][\"text\"]\n",
    "    embeddings_path     = ROOT / text_idx_cfg[\"embeddings\"]             # \"vector_db/text/embeddings.jsonl\"\n",
    "    meta_path           = ROOT / text_idx_cfg[\"meta\"]                   # \"vector_db/text/meta.json\"\n",
    "    faiss_dir           = ROOT / text_idx_cfg[\"faiss\"][\"dir\"]           # \"vector_db/text/faiss_index\"\n",
    "    faiss_index_path    = ROOT / text_idx_cfg[\"faiss\"][\"index\"]         # \".../index.faiss\"\n",
    "    faiss_docstore_path = ROOT / text_idx_cfg[\"faiss\"][\"docstore\"]      # \".../index.pkl\"\n",
    "\n",
    "    faiss_dir.mkdir(parents=True, exist_ok=True)\n",
    "    embeddings_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    meta_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Using FAISS dir:\", faiss_dir)\n",
    "    print(\"Embeddings JSONL:\", embeddings_path)\n",
    "    print(\"Meta JSON:\", meta_path)\n",
    "\n",
    "    # -------------------- 1) Extraction helpers --------------------\n",
    "    # PDFs\n",
    "    from pypdf import PdfReader\n",
    "\n",
    "    def extract_pdf_pages(pdf_path: Path) -> List[Tuple[int, str]]:\n",
    "        \"\"\"Return list of (1-indexed page_number, text). Empty pages become ''.\"\"\"\n",
    "        pages: List[Tuple[int, str]] = []\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            txt = page.extract_text() or \"\"\n",
    "            txt = txt.replace(\"\\u00A0\", \" \").strip()\n",
    "            pages.append((i + 1, txt))\n",
    "        return pages\n",
    "\n",
    "    # Markdown\n",
    "    FM_RE = re.compile(r\"^\\s*---\\s*\\n(.*?)\\n---\\s*\\n?\", re.DOTALL)\n",
    "\n",
    "    def strip_markdown_syntax(md: str) -> str:\n",
    "        \"\"\"Lightweight MD→text. Keeps content, removes common syntax; OK for embeddings.\"\"\"\n",
    "        # remove code fences\n",
    "        md = re.sub(r\"```.*?```\", \"\", md, flags=re.DOTALL)\n",
    "        # remove inline code backticks\n",
    "        md = md.replace(\"`\", \"\")\n",
    "        # images/links: keep label + URL text-ish\n",
    "        md = re.sub(r\"!\\[([^\\]]*)\\]\\([^\\)]*\\)\", r\"\\1\", md)\n",
    "        md = re.sub(r\"\\[([^\\]]+)\\]\\([^\\)]*\\)\", r\"\\1\", md)\n",
    "        # headings/bold/italics\n",
    "        md = re.sub(r\"^\\s{0,3}#{1,6}\\s*\", \"\", md, flags=re.MULTILINE)\n",
    "        md = re.sub(r\"[*_]{1,3}([^*_]+)[*_]{1,3}\", r\"\\1\", md)\n",
    "        # blockquotes / lists / tables pipes\n",
    "        md = re.sub(r\"^\\s{0,3}>\\s?\", \"\", md, flags=re.MULTILINE)\n",
    "        md = re.sub(r\"^\\s*[-*+]\\s+\", \"\", md, flags=re.MULTILINE)\n",
    "        md = re.sub(r\"^\\s*\\d+\\.\\s+\", \"\", md, flags=re.MULTILINE)\n",
    "        md = md.replace(\"|\", \" \")\n",
    "        # collapse whitespace\n",
    "        md = re.sub(r\"[ \\t]+\", \" \", md)\n",
    "        md = re.sub(r\"\\n{3,}\", \"\\n\\n\", md)\n",
    "        return md.strip()\n",
    "\n",
    "    def extract_markdown_blocks(md_path: Path) -> Tuple[Optional[dict], str]:\n",
    "        \"\"\"\n",
    "        Returns (front_matter_dict_or_none, plain_text_body).\n",
    "        Front matter (if present) is parsed as YAML and removed from body.\n",
    "        \"\"\"\n",
    "        raw = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        fm_match = FM_RE.match(raw)\n",
    "        front = None\n",
    "        if fm_match:\n",
    "            try:\n",
    "                front = yaml.safe_load(fm_match.group(1)) or {}\n",
    "            except Exception:\n",
    "                front = {\"_parse_error\": \"front_matter\"}\n",
    "            raw = raw[fm_match.end():]\n",
    "        text = strip_markdown_syntax(raw)\n",
    "        return front, text\n",
    "\n",
    "    # -------------------- 2) Chunker (character-based with overlap) --------------------\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"। \", \". \", \"?\", \"!\", \" \"],\n",
    "        chunk_size=chunk_size_chars,\n",
    "        chunk_overlap=overlap_chars,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    def chunk_text(text: str) -> List[str]:\n",
    "        if not text:\n",
    "            return []\n",
    "        return [c.strip() for c in splitter.split_text(text) if c.strip()]\n",
    "\n",
    "    # -------------------- 3) Build Documents (PDF + MD aware) --------------------\n",
    "    docs: List[Document] = []\n",
    "    pack_name    = manifest.get(\"name\")\n",
    "    pack_ver     = manifest.get(\"version\")\n",
    "    pack_locales = manifest.get(\"locales\", [])\n",
    "\n",
    "    citations_by_id = {c[\"id\"]: c for c in manifest.get(\"citations\", [])}\n",
    "\n",
    "    num_pdf_files = 0\n",
    "    num_md_files = 0\n",
    "    num_skipped_empty = 0\n",
    "\n",
    "    def infer_locale(path_str: str, default: str = \"en\") -> str:\n",
    "        # Keep your existing heuristic; extend if you add more\n",
    "        return \"hi_en\" if \"/hi_en/\" in path_str else default\n",
    "\n",
    "    for topic in manifest.get(\"index_of_topics\", []):\n",
    "        topic_id = topic[\"id\"]\n",
    "        for fmeta in topic.get(\"core_files\", []):\n",
    "            fpath = ROOT / fmeta[\"path\"]\n",
    "            if not fpath.exists():\n",
    "                print(\"! Skipping missing file:\", fpath)\n",
    "                continue\n",
    "\n",
    "            media_type = (fmeta.get(\"media_type\") or fpath.suffix.lstrip(\".\")).lower()\n",
    "            is_pdf = (media_type == \"pdf\") or (fpath.suffix.lower() == \".pdf\")\n",
    "            is_md  = (media_type in {\"md\", \"markdown\"}) or (fpath.suffix.lower() in {\".md\", \".markdown\"})\n",
    "\n",
    "            if not (is_pdf or is_md):\n",
    "                # not a core text doc type we embed here\n",
    "                continue\n",
    "\n",
    "            locale = infer_locale(fmeta[\"path\"], fmeta.get(\"locale\", \"en\"))\n",
    "            c_full = [citations_by_id[cid] for cid in fmeta.get(\"citations\", []) if cid in citations_by_id]\n",
    "\n",
    "            file_chunk_counter = 0\n",
    "\n",
    "            if is_pdf:\n",
    "                pages = extract_pdf_pages(fpath)\n",
    "                if not any(p_txt for _, p_txt in pages):\n",
    "                    print(f\"! PDF has no extractable text (scanned images?): {fpath}\")\n",
    "                    num_skipped_empty += 1\n",
    "                    continue\n",
    "\n",
    "                for page_num, page_text in pages:\n",
    "                    for piece in chunk_text(page_text):\n",
    "                        docs.append(\n",
    "                            Document(\n",
    "                                page_content=piece,\n",
    "                                metadata={\n",
    "                                    \"pack_name\": pack_name,\n",
    "                                    \"pack_version\": pack_ver,\n",
    "                                    \"topic_id\": topic_id,\n",
    "                                    \"file_id\": fmeta[\"id\"],\n",
    "                                    \"path\": str(fmeta[\"path\"]),\n",
    "                                    \"media_type\": \"pdf\",\n",
    "                                    \"locale\": locale,\n",
    "                                    \"citations\": c_full,\n",
    "                                    \"page\": page_num,\n",
    "                                    \"chunk_index\": file_chunk_counter,\n",
    "                                    \"chunk_id\": f\"{fmeta['id']}::p{page_num}::chunk::{file_chunk_counter}\",\n",
    "                                    \"doc_type\": \"pdf\",\n",
    "                                },\n",
    "                            )\n",
    "                        )\n",
    "                        file_chunk_counter += 1\n",
    "                num_pdf_files += 1\n",
    "\n",
    "            elif is_md:\n",
    "                front_matter, body_text = extract_markdown_blocks(fpath)\n",
    "                if not body_text.strip():\n",
    "                    print(f\"! Markdown empty after stripping syntax: {fpath}\")\n",
    "                    num_skipped_empty += 1\n",
    "                    continue\n",
    "\n",
    "                for piece in chunk_text(body_text):\n",
    "                    docs.append(\n",
    "                        Document(\n",
    "                            page_content=piece,\n",
    "                            metadata={\n",
    "                                \"pack_name\": pack_name,\n",
    "                                \"pack_version\": pack_ver,\n",
    "                                \"topic_id\": topic_id,\n",
    "                                \"file_id\": fmeta[\"id\"],\n",
    "                                \"path\": str(fmeta[\"path\"]),\n",
    "                                \"media_type\": \"md\",\n",
    "                                \"locale\": locale,\n",
    "                                \"citations\": c_full,\n",
    "                                \"page\": None,  # no pages for MD\n",
    "                                \"front_matter\": front_matter or {},\n",
    "                                \"chunk_index\": file_chunk_counter,\n",
    "                                \"chunk_id\": f\"{fmeta['id']}::md::chunk::{file_chunk_counter}\",\n",
    "                                \"doc_type\": \"markdown\",\n",
    "                            },\n",
    "                        )\n",
    "                    )\n",
    "                    file_chunk_counter += 1\n",
    "                num_md_files += 1\n",
    "\n",
    "    print(\n",
    "        f\"Prepared {len(docs)} text chunks \"\n",
    "        f\"from {num_pdf_files} PDFs and {num_md_files} MD files \"\n",
    "        f\"(skipped empty/scan-only: {num_skipped_empty})\"\n",
    "    )\n",
    "\n",
    "    # -------------------- 4) Embeddings + FAISS persist --------------------\n",
    "    emb = OllamaEmbeddings(model=embed_model_name)\n",
    "    vs = FAISS.from_documents(docs, emb)\n",
    "    vs.save_local(str(faiss_dir))  # writes index.faiss + index.pkl (overwrites if they exist)\n",
    "\n",
    "    # sanity check\n",
    "    assert faiss_index_path.exists(), f\"Missing {faiss_index_path}\"\n",
    "    assert faiss_docstore_path.exists(), f\"Missing {faiss_docstore_path}\"\n",
    "    print(\"FAISS artifacts saved ✅\", faiss_index_path.name, \"&\", faiss_docstore_path.name)\n",
    "\n",
    "    # -------------------- 5) Export JSONL embeddings + meta (portable) --------------------\n",
    "    records = []\n",
    "    doc_items = getattr(vs.docstore, \"_dict\", {})  # (doc_id -> Document), common LC pattern\n",
    "\n",
    "    for doc_id, doc in doc_items.items():\n",
    "        vec = emb.embed_query(doc.page_content)  # dim depends on your Ollama embedding model\n",
    "        rec = {\n",
    "            \"id\": doc_id,\n",
    "            \"embedding\": vec,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"text\": doc.page_content,\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    with open(embeddings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"model\": embed_model_name,\n",
    "                \"dim\": manifest[\"embedding_config\"][\"text\"].get(\"dim\"),\n",
    "                \"normalize\": normalize,\n",
    "                \"count\": len(records),\n",
    "                \"pack\": {\"name\": pack_name, \"version\": pack_ver, \"locales\": pack_locales},\n",
    "                \"chunking\": {\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"overlap_tokens\": overlap_toks,\n",
    "                    \"approx_chars_per_token\": TOK_TO_CHAR,\n",
    "                    \"chunk_size_chars\": chunk_size_chars,\n",
    "                    \"overlap_chars\": overlap_chars,\n",
    "                },\n",
    "                \"supported_media_types\": [\"pdf\", \"md\", \"markdown\"],\n",
    "            },\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(\"JSONL/meta saved ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0ed59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1158dcd9",
   "metadata": {},
   "source": [
    "### Testing it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad040d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1]\n",
      "Topic: infected-wounds\n",
      "File: wounds-asha-induction\n",
      "Locale: hi_en\n",
      "Citations: ['ASHA Induction Training Module — National Health Systems Resource Centre']\n",
      "Chunk text:\n",
      "125Induction Training Module for ASHAs\n",
      "Observation Checklist for Sub-Centre \n",
      "General information \n",
      "Name of the sub-centres village_____________ \n",
      "Total population covered by the sub-centre_____________ Distance from the \n",
      "PHC _____________ \n",
      "Availability of staff at the sub-centre \n",
      "l Is there an ANM ava ...\n",
      "\n",
      "[2]\n",
      "Topic: maternal\n",
      "File: maternal-asha-module\n",
      "Locale: hi_en\n",
      "Citations: ['ASHA Training Module 6 — Ministry of Health and Family Welfare, Government of India']\n",
      "Chunk text:\n",
      "ASHA Module 6\n",
      "Simple Skills that Save Lives\n",
      "24\n",
      "Where are ANC services provided? \n",
      "The nearest place for ANC services for a woman is at the AWC during the \n",
      "monthly VHND. The pregnant w oman could also go to the Sub-Centre, \n",
      "where the \n",
      "ANM will provide ANC services. ANC services are also provided \n",
      "at t ...\n",
      "\n",
      "[3]\n",
      "Topic: wild-animals\n",
      "File: wildlife-moefcc-strategy\n",
      "Locale: hi_en\n",
      "Citations: ['National Human–Wildlife Conflict Mitigation Strategy and Action Plan — Ministry of Environment, Forest and Climate Change']\n",
      "Chunk text:\n",
      "The Area of Operation of each RRTs will need to be \n",
      "fixed in such a way that the RRT can travel within two \n",
      "hours / any other time-limit as may deem fit by the \n",
      "respective SFD, from their HQ to the incidence site. A \n",
      "common mobile app used by all RRTs and PRTs within \n",
      "the state, will help in ironing ...\n",
      "\n",
      "[4]\n",
      "Topic: wild-animals\n",
      "File: wildlife-moefcc-strategy\n",
      "Locale: hi_en\n",
      "Citations: ['National Human–Wildlife Conflict Mitigation Strategy and Action Plan — Ministry of Environment, Forest and Climate Change']\n",
      "Chunk text:\n",
      "situations, required action and communication \n",
      "requirements by the control room. \n",
      "- Maintaining the resource database (nearby \n",
      "hospitals, vehicles, fire brigade, local hotspots) \n",
      "mapped on a Geographic Information System (GIS) \n",
      "platform\n",
      "- Identifying opportunities for radio-collaring of key \n",
      "conflic ...\n",
      "\n",
      "[5]\n",
      "Topic: joint-pain\n",
      "File: joint-ilo-ergonomic\n",
      "Locale: hi_en\n",
      "Citations: ['Ergonomic Checkpoints in Agriculture — International Labour Organization']\n",
      "Chunk text:\n",
      "CHECKPOINT 54\n",
      "Supply sufficient airflow to silos, and other confined\n",
      "places where oxygen deficiencies may occur, before\n",
      "entering them.\n",
      "WHY\n",
      "Confined places that are airtight, or are poorly\n",
      "ventilated, can be very dangerous, as workers entering\n",
      "them are exposed to oxygen deficiency , toxic gases or\n",
      "ha ...\n",
      "\n",
      "[6]\n",
      "Topic: heat\n",
      "File: heat-ahmedabad-hap\n",
      "Locale: hi_en\n",
      "Citations: ['Ahmedabad Heat Action Plan — NRDC and Indian Institute of Public Health, Gandhinagar']\n",
      "Chunk text:\n",
      "9 \n",
      " \n",
      "Agency Action Checklists \n",
      " \n",
      " \n",
      "Checklist for AMC Nodal Officer \n",
      "Pre-Summer \n",
      "✓ Designate heat-health point-of-contact for each department  \n",
      "✓ Reengage key agencies to facilitate communications and schedule \n",
      "monthly meetings \n",
      "✓ Establish heat mortality tracking system and update datasets \n",
      "✓ Establ ...\n",
      "\n",
      "[7]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "management. \n",
      " Have the capability to think practically and be able to improvise, if required. \n",
      " Obtain the full particulars nature of the accident or disaster and the affected site. \n",
      " Coordinate with the appropriate governmental authorities. \n",
      " Coordinate with other local agencies, other institut ...\n",
      "\n",
      "[8]\n",
      "Topic: joint-pain\n",
      "File: joint-ilo-ergonomic\n",
      "Locale: hi_en\n",
      "Citations: ['Ergonomic Checkpoints in Agriculture — International Labour Organization']\n",
      "Chunk text:\n",
      "Environmental protection\n",
      "149\n",
      "Figure 68a. Make it a\n",
      "rule to separate wastes\n",
      "when they are collected.\n",
      "Attach labels showing\n",
      "the types of waste to be\n",
      "collected. Learn, from\n",
      "good examples in your\n",
      "community , how to set\n",
      "up separate waste\n",
      "containers.\n",
      "Figure 68b. Use carts for carrying waste containers. Th ...\n",
      "\n",
      "[9]\n",
      "Topic: wild-animals\n",
      "File: wildlife-moefcc-strategy\n",
      "Locale: hi_en\n",
      "Citations: ['National Human–Wildlife Conflict Mitigation Strategy and Action Plan — Ministry of Environment, Forest and Climate Change']\n",
      "Chunk text:\n",
      "Emergency Response Mechanism: \n",
      "- Receiving and channelizing the information. \n",
      "- detection of an incident/early warning signal. \n",
      "- first responder/Incident Response Person (IRP): \n",
      "- The IRP informs the control room, who in turn \n",
      "activates the Range Rapid Response Team \n",
      "Range RRT. \n",
      "- Range RRT instruc ...\n",
      "\n",
      "[10]\n",
      "Topic: joint-pain\n",
      "File: joint-ilo-ergonomic\n",
      "Locale: hi_en\n",
      "Citations: ['Ergonomic Checkpoints in Agriculture — International Labour Organization']\n",
      "Chunk text:\n",
      "Physical environment\n",
      "117\n",
      "Figure 54a. Establish a rule that only\n",
      "trained workers with an entry permit\n",
      "can enter a designated confined space.\n",
      "The oxygen concentration and the\n",
      "presence of toxic gases must be\n",
      "monitored before entering the space.\n",
      "Figure 54c. Always have people outside,\n",
      "ready to rescue, i ...\n"
     ]
    }
   ],
   "source": [
    "# Typical retriever usage\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 10})  # if you used Option A 'vs'\n",
    "query = \"Show me evac center\"  #\n",
    "hits = retriever.invoke(query)\n",
    "\n",
    "for i, d in enumerate(hits, 1):\n",
    "    print(f\"\\n[{i}]\")\n",
    "    print(\"Topic:\", d.metadata[\"topic_id\"])\n",
    "    print(\"File:\", d.metadata[\"file_id\"])\n",
    "    print(\"Locale:\", d.metadata[\"locale\"])\n",
    "    print(\"Citations:\", [c[\"title\"] for c in d.metadata.get(\"citations\", [])])\n",
    "    print(\"Chunk text:\")\n",
    "    print(d.page_content[:300], \"...\" if len(d.page_content) > 300 else \"\")\n",
    "\n",
    "# Filter to a topic or locale:\n",
    "# hits = retriever.invoke(\"tourniquet steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe1544",
   "metadata": {},
   "source": [
    ":)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2a8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiofiles==24.1.0\n",
      "aiohappyeyeballs==2.6.1\n",
      "aiohttp==3.12.15\n",
      "aiosignal==1.4.0\n",
      "annotated-types==0.7.0\n",
      "anyio==4.10.0\n",
      "appnope==0.1.4\n",
      "argon2-cffi==25.1.0\n",
      "argon2-cffi-bindings==25.1.0\n",
      "arrow==1.3.0\n",
      "asttokens==3.0.0\n",
      "async-lru==2.0.5\n",
      "attrs==25.3.0\n",
      "audioop-lts==0.2.2\n",
      "babel==2.17.0\n",
      "beautifulsoup4==4.13.4\n",
      "bleach==6.2.0\n",
      "Brotli==1.1.0\n",
      "certifi==2025.8.3\n",
      "cffi==1.17.1\n",
      "charset-normalizer==3.4.3\n",
      "click==8.2.1\n",
      "comm==0.2.3\n",
      "dataclasses-json==0.6.7\n",
      "debugpy==1.8.16\n",
      "decorator==5.2.1\n",
      "defusedxml==0.7.1\n",
      "executing==2.2.0\n",
      "faiss-cpu==1.12.0\n",
      "fastapi==0.116.1\n",
      "fastjsonschema==2.21.2\n",
      "ffmpy==0.6.1\n",
      "filelock==3.19.1\n",
      "fqdn==1.5.1\n",
      "frozenlist==1.7.0\n",
      "fsspec==2025.7.0\n",
      "gradio==5.44.1\n",
      "gradio_client==1.12.1\n",
      "groovy==0.1.2\n",
      "h11==0.16.0\n",
      "hf-xet==1.1.9\n",
      "httpcore==1.0.9\n",
      "httpx==0.28.1\n",
      "httpx-sse==0.4.1\n",
      "huggingface-hub==0.34.4\n",
      "idna==3.10\n",
      "ipykernel==6.30.1\n",
      "ipython==9.4.0\n",
      "ipython_pygments_lexers==1.1.1\n",
      "ipywidgets==8.1.7\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.2\n",
      "Jinja2==3.1.6\n",
      "json5==0.12.1\n",
      "jsonpatch==1.33\n",
      "jsonpointer==3.0.0\n",
      "jsonschema==4.25.1\n",
      "jsonschema-specifications==2025.4.1\n",
      "jupyter==1.1.1\n",
      "jupyter-console==6.6.3\n",
      "jupyter-events==0.12.0\n",
      "jupyter-lsp==2.2.6\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.8.1\n",
      "jupyter_server==2.16.0\n",
      "jupyter_server_terminals==0.5.3\n",
      "jupyterlab==4.4.6\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.27.3\n",
      "jupyterlab_widgets==3.0.15\n",
      "langchain==0.3.27\n",
      "langchain-community==0.3.27\n",
      "langchain-core==0.3.74\n",
      "langchain-ollama==0.3.6\n",
      "langchain-text-splitters==0.3.9\n",
      "langgraph==0.6.6\n",
      "langgraph-checkpoint==2.1.1\n",
      "langgraph-prebuilt==0.6.4\n",
      "langgraph-sdk==0.2.3\n",
      "langsmith==0.4.14\n",
      "lark==1.2.2\n",
      "llvmlite==0.44.0\n",
      "markdown-it-py==4.0.0\n",
      "MarkupSafe==3.0.2\n",
      "marshmallow==3.26.1\n",
      "matplotlib-inline==0.1.7\n",
      "mdurl==0.1.2\n",
      "mistune==3.1.3\n",
      "more-itertools==10.7.0\n",
      "mpmath==1.3.0\n",
      "multidict==6.6.4\n",
      "mypy_extensions==1.1.0\n",
      "nbclient==0.10.2\n",
      "nbconvert==7.16.6\n",
      "nbformat==5.10.4\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.5\n",
      "notebook==7.4.5\n",
      "notebook_shim==0.2.4\n",
      "numba==0.61.2\n",
      "numpy==2.2.6\n",
      "ollama==0.5.3\n",
      "openai-whisper==20250625\n",
      "orjson==3.11.2\n",
      "ormsgpack==1.10.0\n",
      "overrides==7.7.0\n",
      "packaging==25.0\n",
      "pandas==2.3.2\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.4\n",
      "pexpect==4.9.0\n",
      "pillow==11.3.0\n",
      "platformdirs==4.3.8\n",
      "prometheus_client==0.22.1\n",
      "prompt_toolkit==3.0.51\n",
      "propcache==0.3.2\n",
      "psutil==7.0.0\n",
      "ptyprocess==0.7.0\n",
      "pure_eval==0.2.3\n",
      "pycparser==2.22\n",
      "pydantic==2.11.7\n",
      "pydantic-settings==2.10.1\n",
      "pydantic_core==2.33.2\n",
      "pydub==0.25.1\n",
      "Pygments==2.19.2\n",
      "pypdf==6.0.0\n",
      "python-dateutil==2.9.0.post0\n",
      "python-dotenv==1.1.1\n",
      "python-json-logger==3.3.0\n",
      "python-multipart==0.0.20\n",
      "pytz==2025.2\n",
      "PyYAML==6.0.2\n",
      "pyzmq==27.0.1\n",
      "referencing==0.36.2\n",
      "regex==2025.8.29\n",
      "requests==2.32.5\n",
      "requests-toolbelt==1.0.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rfc3987-syntax==1.1.0\n",
      "rich==14.1.0\n",
      "rpds-py==0.27.0\n",
      "ruff==0.12.11\n",
      "safehttpx==0.1.6\n",
      "semantic-version==2.10.0\n",
      "Send2Trash==1.8.3\n",
      "setuptools==80.9.0\n",
      "shellingham==1.5.4\n",
      "six==1.17.0\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.7\n",
      "SQLAlchemy==2.0.43\n",
      "stack-data==0.6.3\n",
      "starlette==0.47.3\n",
      "sympy==1.14.0\n",
      "tenacity==9.1.2\n",
      "terminado==0.18.1\n",
      "tiktoken==0.11.0\n",
      "tinycss2==1.4.0\n",
      "tomlkit==0.13.3\n",
      "torch==2.8.0\n",
      "tornado==6.5.2\n",
      "tqdm==4.67.1\n",
      "traitlets==5.14.3\n",
      "typer==0.17.3\n",
      "types-python-dateutil==2.9.0.20250809\n",
      "typing-inspect==0.9.0\n",
      "typing-inspection==0.4.1\n",
      "typing_extensions==4.14.1\n",
      "tzdata==2025.2\n",
      "uri-template==1.3.0\n",
      "urllib3==2.5.0\n",
      "uvicorn==0.35.0\n",
      "wcwidth==0.2.13\n",
      "webcolors==24.11.1\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.8.0\n",
      "websockets==15.0.1\n",
      "widgetsnbextension==4.0.14\n",
      "xxhash==3.5.0\n",
      "yarl==1.20.1\n",
      "zstandard==0.24.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609f501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
