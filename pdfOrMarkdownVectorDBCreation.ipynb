{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82abbd78",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook embeds all PDF documents + citations avaible in a FAISS vector databse with langchain and nomic-embed-text:v1.5. Expands on previosu text embeddings notebook.\n",
    "\n",
    "```\n",
    "ollama pull nomic-embed-text:v1.5\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d0e07",
   "metadata": {},
   "source": [
    "### Importing and Paths\n",
    "\n",
    "Change the ROOT  paths as needed. It should point to to the main knowledge pack dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad972735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/hurricane_disaster_response_pack\n",
      "/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/hurricane_disaster_response_pack/manifest.yaml\n"
     ]
    }
   ],
   "source": [
    "# --- A. Imports & config ---\n",
    "from pathlib import Path\n",
    "import json, hashlib, uuid, yaml\n",
    "from typing import List, Dict\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Paths (adapt for your pack root)\n",
    "ROOT = Path('/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/hurricane_disaster_response_pack')\n",
    "MANIFEST = ROOT / \"manifest.yaml\"\n",
    "print(ROOT)\n",
    "print(MANIFEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb7439",
   "metadata": {},
   "source": [
    "### Parsing YAML, Embedding Documents, and Creating Vector Store\n",
    "\n",
    "NOTES: \n",
    "1. Below cell will create a new directory inisde the knolwedge pack:\n",
    "- Example: first_aid_pack_demo_v2/vector_db/text/faiss_index <br>\n",
    "This directory will have the actual .faiss store and index pickle file\n",
    "\n",
    "2. embeddings.jsonl, index.bin, and meta.json under first_aid_pack_demo_v2/vector_db/text/faiss_index will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d36beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FAISS dir: /Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/hurricane_disaster_response_pack/vector_db/text/faiss_index\n",
      "Embeddings JSONL: /Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/hurricane_disaster_response_pack/vector_db/text/embeddings.jsonl\n",
      "Meta JSON: /Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/hurricane_disaster_response_pack/vector_db/text/meta.json\n",
      "Prepared 263 text chunks from 8 PDFs and 8 MD files (skipped empty/scan-only: 0)\n",
      "FAISS artifacts saved ✅ index.faiss & index.pkl\n",
      "JSONL/meta saved ✅\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import yaml\n",
    "\n",
    "# LangChain docs object (compat across versions)\n",
    "try:\n",
    "    from langchain_core.documents import Document\n",
    "except ImportError:\n",
    "    from langchain.schema import Document  # older LC\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Expect ROOT and MANIFEST to be defined in your environment\n",
    "# ROOT = Path(\"first_aid_knowledge_pack_v3\")\n",
    "# MANIFEST = ROOT / \"manifest.yaml\"\n",
    "\n",
    "ans = input(\"Y/N\")\n",
    "if ans == \"Y\":\n",
    "    # === PDF & Markdown chunk & embed (manifest-driven) ===\n",
    "    # deps for PDFs: pip install pypdf\n",
    "\n",
    "    # -------------------- 0) Load manifest & resolve paths --------------------\n",
    "    with open(MANIFEST, \"r\", encoding=\"utf-8\") as f:\n",
    "        manifest = yaml.safe_load(f)\n",
    "\n",
    "    # Embedding + chunking config from manifest (with sensible fallbacks)\n",
    "    embed_model_name = manifest[\"embedding_config\"][\"text\"][\"model\"]  # e.g., \"granite-embedding:278m\"\n",
    "    normalize   = bool(manifest[\"embedding_config\"][\"text\"].get(\"normalize\", True))\n",
    "    max_tokens  = int(manifest[\"embedding_config\"][\"text\"][\"chunking\"].get(\"max_tokens\", 512))\n",
    "    overlap_toks= int(manifest[\"embedding_config\"][\"text\"][\"chunking\"].get(\"overlap_tokens\", 64))\n",
    "\n",
    "    # very rough char≈token conversion for character-based splitter\n",
    "    TOK_TO_CHAR = 4  # tune if your corpora are very different\n",
    "    chunk_size_chars = max(64, max_tokens * TOK_TO_CHAR)\n",
    "    overlap_chars    = max(0,  overlap_toks * TOK_TO_CHAR)\n",
    "\n",
    "    # Resolve precomputed index paths from manifest\n",
    "    text_idx_cfg        = manifest[\"precomputed_indices\"][\"text\"]\n",
    "    embeddings_path     = ROOT / text_idx_cfg[\"embeddings\"]             # \"vector_db/text/embeddings.jsonl\"\n",
    "    meta_path           = ROOT / text_idx_cfg[\"meta\"]                   # \"vector_db/text/meta.json\"\n",
    "    faiss_dir           = ROOT / text_idx_cfg[\"faiss\"][\"dir\"]           # \"vector_db/text/faiss_index\"\n",
    "    faiss_index_path    = ROOT / text_idx_cfg[\"faiss\"][\"index\"]         # \".../index.faiss\"\n",
    "    faiss_docstore_path = ROOT / text_idx_cfg[\"faiss\"][\"docstore\"]      # \".../index.pkl\"\n",
    "\n",
    "    faiss_dir.mkdir(parents=True, exist_ok=True)\n",
    "    embeddings_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    meta_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Using FAISS dir:\", faiss_dir)\n",
    "    print(\"Embeddings JSONL:\", embeddings_path)\n",
    "    print(\"Meta JSON:\", meta_path)\n",
    "\n",
    "    # -------------------- 1) Extraction helpers --------------------\n",
    "    # PDFs\n",
    "    from pypdf import PdfReader\n",
    "\n",
    "    def extract_pdf_pages(pdf_path: Path) -> List[Tuple[int, str]]:\n",
    "        \"\"\"Return list of (1-indexed page_number, text). Empty pages become ''.\"\"\"\n",
    "        pages: List[Tuple[int, str]] = []\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            txt = page.extract_text() or \"\"\n",
    "            txt = txt.replace(\"\\u00A0\", \" \").strip()\n",
    "            pages.append((i + 1, txt))\n",
    "        return pages\n",
    "\n",
    "    # Markdown\n",
    "    FM_RE = re.compile(r\"^\\s*---\\s*\\n(.*?)\\n---\\s*\\n?\", re.DOTALL)\n",
    "\n",
    "    def strip_markdown_syntax(md: str) -> str:\n",
    "        \"\"\"Lightweight MD→text. Keeps content, removes common syntax; OK for embeddings.\"\"\"\n",
    "        # remove code fences\n",
    "        md = re.sub(r\"```.*?```\", \"\", md, flags=re.DOTALL)\n",
    "        # remove inline code backticks\n",
    "        md = md.replace(\"`\", \"\")\n",
    "        # images/links: keep label + URL text-ish\n",
    "        md = re.sub(r\"!\\[([^\\]]*)\\]\\([^\\)]*\\)\", r\"\\1\", md)\n",
    "        md = re.sub(r\"\\[([^\\]]+)\\]\\([^\\)]*\\)\", r\"\\1\", md)\n",
    "        # headings/bold/italics\n",
    "        md = re.sub(r\"^\\s{0,3}#{1,6}\\s*\", \"\", md, flags=re.MULTILINE)\n",
    "        md = re.sub(r\"[*_]{1,3}([^*_]+)[*_]{1,3}\", r\"\\1\", md)\n",
    "        # blockquotes / lists / tables pipes\n",
    "        md = re.sub(r\"^\\s{0,3}>\\s?\", \"\", md, flags=re.MULTILINE)\n",
    "        md = re.sub(r\"^\\s*[-*+]\\s+\", \"\", md, flags=re.MULTILINE)\n",
    "        md = re.sub(r\"^\\s*\\d+\\.\\s+\", \"\", md, flags=re.MULTILINE)\n",
    "        md = md.replace(\"|\", \" \")\n",
    "        # collapse whitespace\n",
    "        md = re.sub(r\"[ \\t]+\", \" \", md)\n",
    "        md = re.sub(r\"\\n{3,}\", \"\\n\\n\", md)\n",
    "        return md.strip()\n",
    "\n",
    "    def extract_markdown_blocks(md_path: Path) -> Tuple[Optional[dict], str]:\n",
    "        \"\"\"\n",
    "        Returns (front_matter_dict_or_none, plain_text_body).\n",
    "        Front matter (if present) is parsed as YAML and removed from body.\n",
    "        \"\"\"\n",
    "        raw = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        fm_match = FM_RE.match(raw)\n",
    "        front = None\n",
    "        if fm_match:\n",
    "            try:\n",
    "                front = yaml.safe_load(fm_match.group(1)) or {}\n",
    "            except Exception:\n",
    "                front = {\"_parse_error\": \"front_matter\"}\n",
    "            raw = raw[fm_match.end():]\n",
    "        text = strip_markdown_syntax(raw)\n",
    "        return front, text\n",
    "\n",
    "    # -------------------- 2) Chunker (character-based with overlap) --------------------\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"। \", \". \", \"?\", \"!\", \" \"],\n",
    "        chunk_size=chunk_size_chars,\n",
    "        chunk_overlap=overlap_chars,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    def chunk_text(text: str) -> List[str]:\n",
    "        if not text:\n",
    "            return []\n",
    "        return [c.strip() for c in splitter.split_text(text) if c.strip()]\n",
    "\n",
    "    # -------------------- 3) Build Documents (PDF + MD aware) --------------------\n",
    "    docs: List[Document] = []\n",
    "    pack_name    = manifest.get(\"name\")\n",
    "    pack_ver     = manifest.get(\"version\")\n",
    "    pack_locales = manifest.get(\"locales\", [])\n",
    "\n",
    "    citations_by_id = {c[\"id\"]: c for c in manifest.get(\"citations\", [])}\n",
    "\n",
    "    num_pdf_files = 0\n",
    "    num_md_files = 0\n",
    "    num_skipped_empty = 0\n",
    "\n",
    "    def infer_locale(path_str: str, default: str = \"en\") -> str:\n",
    "        # Keep your existing heuristic; extend if you add more\n",
    "        return \"hi_en\" if \"/hi_en/\" in path_str else default\n",
    "\n",
    "    for topic in manifest.get(\"index_of_topics\", []):\n",
    "        topic_id = topic[\"id\"]\n",
    "        for fmeta in topic.get(\"core_files\", []):\n",
    "            fpath = ROOT / fmeta[\"path\"]\n",
    "            if not fpath.exists():\n",
    "                print(\"! Skipping missing file:\", fpath)\n",
    "                continue\n",
    "\n",
    "            media_type = (fmeta.get(\"media_type\") or fpath.suffix.lstrip(\".\")).lower()\n",
    "            is_pdf = (media_type == \"pdf\") or (fpath.suffix.lower() == \".pdf\")\n",
    "            is_md  = (media_type in {\"md\", \"markdown\"}) or (fpath.suffix.lower() in {\".md\", \".markdown\"})\n",
    "\n",
    "            if not (is_pdf or is_md):\n",
    "                # not a core text doc type we embed here\n",
    "                continue\n",
    "\n",
    "            locale = infer_locale(fmeta[\"path\"], fmeta.get(\"locale\", \"en\"))\n",
    "            c_full = [citations_by_id[cid] for cid in fmeta.get(\"citations\", []) if cid in citations_by_id]\n",
    "\n",
    "            file_chunk_counter = 0\n",
    "\n",
    "            if is_pdf:\n",
    "                pages = extract_pdf_pages(fpath)\n",
    "                if not any(p_txt for _, p_txt in pages):\n",
    "                    print(f\"! PDF has no extractable text (scanned images?): {fpath}\")\n",
    "                    num_skipped_empty += 1\n",
    "                    continue\n",
    "\n",
    "                for page_num, page_text in pages:\n",
    "                    for piece in chunk_text(page_text):\n",
    "                        docs.append(\n",
    "                            Document(\n",
    "                                page_content=piece,\n",
    "                                metadata={\n",
    "                                    \"pack_name\": pack_name,\n",
    "                                    \"pack_version\": pack_ver,\n",
    "                                    \"topic_id\": topic_id,\n",
    "                                    \"file_id\": fmeta[\"id\"],\n",
    "                                    \"path\": str(fmeta[\"path\"]),\n",
    "                                    \"media_type\": \"pdf\",\n",
    "                                    \"locale\": locale,\n",
    "                                    \"citations\": c_full,\n",
    "                                    \"page\": page_num,\n",
    "                                    \"chunk_index\": file_chunk_counter,\n",
    "                                    \"chunk_id\": f\"{fmeta['id']}::p{page_num}::chunk::{file_chunk_counter}\",\n",
    "                                    \"doc_type\": \"pdf\",\n",
    "                                },\n",
    "                            )\n",
    "                        )\n",
    "                        file_chunk_counter += 1\n",
    "                num_pdf_files += 1\n",
    "\n",
    "            elif is_md:\n",
    "                front_matter, body_text = extract_markdown_blocks(fpath)\n",
    "                if not body_text.strip():\n",
    "                    print(f\"! Markdown empty after stripping syntax: {fpath}\")\n",
    "                    num_skipped_empty += 1\n",
    "                    continue\n",
    "\n",
    "                for piece in chunk_text(body_text):\n",
    "                    docs.append(\n",
    "                        Document(\n",
    "                            page_content=piece,\n",
    "                            metadata={\n",
    "                                \"pack_name\": pack_name,\n",
    "                                \"pack_version\": pack_ver,\n",
    "                                \"topic_id\": topic_id,\n",
    "                                \"file_id\": fmeta[\"id\"],\n",
    "                                \"path\": str(fmeta[\"path\"]),\n",
    "                                \"media_type\": \"md\",\n",
    "                                \"locale\": locale,\n",
    "                                \"citations\": c_full,\n",
    "                                \"page\": None,  # no pages for MD\n",
    "                                \"front_matter\": front_matter or {},\n",
    "                                \"chunk_index\": file_chunk_counter,\n",
    "                                \"chunk_id\": f\"{fmeta['id']}::md::chunk::{file_chunk_counter}\",\n",
    "                                \"doc_type\": \"markdown\",\n",
    "                            },\n",
    "                        )\n",
    "                    )\n",
    "                    file_chunk_counter += 1\n",
    "                num_md_files += 1\n",
    "\n",
    "    print(\n",
    "        f\"Prepared {len(docs)} text chunks \"\n",
    "        f\"from {num_pdf_files} PDFs and {num_md_files} MD files \"\n",
    "        f\"(skipped empty/scan-only: {num_skipped_empty})\"\n",
    "    )\n",
    "\n",
    "    # -------------------- 4) Embeddings + FAISS persist --------------------\n",
    "    emb = OllamaEmbeddings(model=embed_model_name)\n",
    "    vs = FAISS.from_documents(docs, emb)\n",
    "    vs.save_local(str(faiss_dir))  # writes index.faiss + index.pkl (overwrites if they exist)\n",
    "\n",
    "    # sanity check\n",
    "    assert faiss_index_path.exists(), f\"Missing {faiss_index_path}\"\n",
    "    assert faiss_docstore_path.exists(), f\"Missing {faiss_docstore_path}\"\n",
    "    print(\"FAISS artifacts saved ✅\", faiss_index_path.name, \"&\", faiss_docstore_path.name)\n",
    "\n",
    "    # -------------------- 5) Export JSONL embeddings + meta (portable) --------------------\n",
    "    records = []\n",
    "    doc_items = getattr(vs.docstore, \"_dict\", {})  # (doc_id -> Document), common LC pattern\n",
    "\n",
    "    for doc_id, doc in doc_items.items():\n",
    "        vec = emb.embed_query(doc.page_content)  # dim depends on your Ollama embedding model\n",
    "        rec = {\n",
    "            \"id\": doc_id,\n",
    "            \"embedding\": vec,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"text\": doc.page_content,\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    with open(embeddings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"model\": embed_model_name,\n",
    "                \"dim\": manifest[\"embedding_config\"][\"text\"].get(\"dim\"),\n",
    "                \"normalize\": normalize,\n",
    "                \"count\": len(records),\n",
    "                \"pack\": {\"name\": pack_name, \"version\": pack_ver, \"locales\": pack_locales},\n",
    "                \"chunking\": {\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"overlap_tokens\": overlap_toks,\n",
    "                    \"approx_chars_per_token\": TOK_TO_CHAR,\n",
    "                    \"chunk_size_chars\": chunk_size_chars,\n",
    "                    \"overlap_chars\": overlap_chars,\n",
    "                },\n",
    "                \"supported_media_types\": [\"pdf\", \"md\", \"markdown\"],\n",
    "            },\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(\"JSONL/meta saved ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0ed59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1158dcd9",
   "metadata": {},
   "source": [
    "### Testing it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad040d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1]\n",
      "Topic: flooding-storm-safety\n",
      "File: all-hazard-preparedness\n",
      "Locale: en\n",
      "Citations: ['Pinellas County: All-Hazard Preparedness Guide']\n",
      "Chunk text:\n",
      "PAGE 8\n",
      "   ALL HAZARDS PREPAREDNESS GUIDE    ALL HAZARDS PREPAREDNESS GUIDE\n",
      "PINELLAS COUNTY EVACUATION ZONE MAP\n",
      "E\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "Up to 35’\n",
      "Up to 28’\n",
      "Up to 20’\n",
      "Up to 15’\n",
      "Up to 11’\n",
      "Potential Surge Heights (in feet)\n",
      "Areas shown in white are non-evacuation zones.\n",
      "Surge height will vary depending on ground eleva ...\n",
      "\n",
      "[2]\n",
      "Topic: hurricane-readiness\n",
      "File: all-hazard-guide\n",
      "Locale: en\n",
      "Citations: ['Pinellas County: All-Hazard Preparedness Guide (Liberty copy)']\n",
      "Chunk text:\n",
      "You can volunteer to help staff an Emergency Evacuation Shelter. The shelters  \n",
      "need assistance once an evacuation is called and the shelters open, as well as during \n",
      "the storm and possibly weeks afterward, depending on the damage to personal \n",
      "property. Volunteers are trained to help with registrati ...\n",
      "\n",
      "[3]\n",
      "Topic: flooding-storm-safety\n",
      "File: all-hazard-preparedness\n",
      "Locale: en\n",
      "Citations: ['Pinellas County: All-Hazard Preparedness Guide']\n",
      "Chunk text:\n",
      "mobile home, \n",
      "you will ALWAYS \n",
      "have to evacuate.\n",
      "Ways to Find Your Evacuation Zone:\n",
      "• V isit www.pinellascounty.org/emergency ,  \n",
      "click on “Know Your Zone.”\n",
      "•\n",
      " L\n",
      "ook at “Know Your Zone” at http://egis.\n",
      "pinellascounty.org/apps/knowyourzone/.\n",
      "•\n",
      " L\n",
      "ook up the Storm Surge Protector interactive \n",
      "tool at  ...\n",
      "\n",
      "[4]\n",
      "Topic: hurricane-readiness\n",
      "File: all-hazard-guide\n",
      "Locale: en\n",
      "Citations: ['Pinellas County: All-Hazard Preparedness Guide (Liberty copy)']\n",
      "Chunk text:\n",
      "program.\n",
      "Using the registration list, personnel from an emergency \n",
      "response unit will contact you if there is a hurricane \n",
      "threatening our area and evacuations are possible. \n",
      "This phone call is to ask you if you still need the \n",
      "transportation assistance. If you do need it, and an \n",
      "evacuation is call ...\n",
      "\n",
      "[5]\n",
      "Topic: hurricane-readiness\n",
      "File: all-hazard-guide\n",
      "Locale: en\n",
      "Citations: ['Pinellas County: All-Hazard Preparedness Guide (Liberty copy)']\n",
      "Chunk text:\n",
      "Because evacuations take time to ensure everyone can get to safety, they are called \n",
      "well in advance of the storm. Evacuations have a beginning and end time. You must \n",
      "be in a safe location by the end of the evacuation period, well before  \n",
      "the storm surge and high winds arrive.\n",
      "Evacuation orders is ...\n",
      "\n",
      "[6]\n",
      "Topic: flooding-storm-safety\n",
      "File: all-hazard-preparedness\n",
      "Locale: en\n",
      "Citations: ['Pinellas County: All-Hazard Preparedness Guide']\n",
      "Chunk text:\n",
      "you have, \n",
      "whether \n",
      "a house, \n",
      "condo, \n",
      "business or other property. There may even be a \n",
      "car parked in the driveway. As you click through the \n",
      "different evacuation levels, the picture changes to \n",
      "show the levels of rising water and what it would \n",
      "look like on your property. People are usually \n",
      "surpris ...\n",
      "\n",
      "[7]\n",
      "Topic: flooding-storm-safety\n",
      "File: all-hazard-preparedness\n",
      "Locale: en\n",
      "Citations: ['Pinellas County: All-Hazard Preparedness Guide']\n",
      "Chunk text:\n",
      "PAGE 4\n",
      "   ALL HAZARDS PREPAREDNESS GUIDE    ALL HAZARDS PREPAREDNESS GUIDE\n",
      "There are changes this year to the evacuation \n",
      "map, so make sure you know your zone and \n",
      "update your plans if necessary.\n",
      "Knowing your evacuation zone is the first step in \n",
      "planning. Evacuation zones are based on how high \n",
      "wat ...\n",
      "\n",
      "[8]\n",
      "Topic: flooding-storm-safety\n",
      "File: all-hazard-preparedness\n",
      "Locale: en\n",
      "Citations: ['Pinellas County: All-Hazard Preparedness Guide']\n",
      "Chunk text:\n",
      "can follow those orders. \n",
      "•\n",
      " S\n",
      "tay tuned to local news and get your weather \n",
      "radio ready.\n",
      "•\n",
      " C\n",
      "omplete final preparations to evacuate or to \n",
      "shelter in your home.\n",
      "•\n",
      " I\n",
      "f your plan is to travel out of the local area and \n",
      "you can leave at this point, go.\n",
      "•\n",
      " I\n",
      "f you are registered for transportation to ...\n",
      "\n",
      "[9]\n",
      "Topic: hurricane-readiness\n",
      "File: all-hazard-guide\n",
      "Locale: en\n",
      "Citations: ['Pinellas County: All-Hazard Preparedness Guide (Liberty copy)']\n",
      "Chunk text:\n",
      "www.pinellascounty.org for updates to evacuation \n",
      "zones and shelter information.\n",
      "Hurricane warning \n",
      "(36 hours ahead)\n",
      " ࡟Stay tuned to local news and get your weather \n",
      "radio ready.\n",
      " ࡟Complete final preparations to evacuate or to shelter \n",
      "in your home.\n",
      " ࡟If your plan is to travel out of the local area  ...\n",
      "\n",
      "[10]\n",
      "Topic: flooding-storm-safety\n",
      "File: all-hazard-preparedness\n",
      "Locale: en\n",
      "Citations: ['Pinellas County: All-Hazard Preparedness Guide']\n",
      "Chunk text:\n",
      "637; Frontier Channel 44; WOW! Channel 18\n",
      "Citizens Information Center: (727) 464-4333,  \n",
      "V/TDD (727) 464-3075\n",
      "Pinellas County Facebook, Twitter (search  \n",
      "for Pinellas County) and Nextdoor (sign up at  \n",
      "www.nextdoor.com)\n",
      "Remember ... Knowledge is Power.  \n",
      "• Si gn up for Alert Pinellas  \n",
      "•\n",
      " W\n",
      "eather a ...\n"
     ]
    }
   ],
   "source": [
    "# Typical retriever usage\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 10})  # if you used Option A 'vs'\n",
    "query = \"Show me evac center\"  #\n",
    "hits = retriever.invoke(query)\n",
    "\n",
    "for i, d in enumerate(hits, 1):\n",
    "    print(f\"\\n[{i}]\")\n",
    "    print(\"Topic:\", d.metadata[\"topic_id\"])\n",
    "    print(\"File:\", d.metadata[\"file_id\"])\n",
    "    print(\"Locale:\", d.metadata[\"locale\"])\n",
    "    print(\"Citations:\", [c[\"title\"] for c in d.metadata.get(\"citations\", [])])\n",
    "    print(\"Chunk text:\")\n",
    "    print(d.page_content[:300], \"...\" if len(d.page_content) > 300 else \"\")\n",
    "\n",
    "# Filter to a topic or locale:\n",
    "# hits = retriever.invoke(\"tourniquet steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe1544",
   "metadata": {},
   "source": [
    ":)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609f501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
