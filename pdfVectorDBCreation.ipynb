{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82abbd78",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook embeds all PDF documents + citations avaible in a FAISS vector databse with langchain and nomic-embed-text:v1.5. Expands on previosu text embeddings notebook.\n",
    "\n",
    "```\n",
    "ollama pull nomic-embed-text:v1.5\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d0e07",
   "metadata": {},
   "source": [
    "### Importing and Paths\n",
    "\n",
    "Change the ROOT  paths as needed. It should point to to the main knowledge pack dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad972735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/first_aid_knowledge_pack_v3\n",
      "/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/first_aid_knowledge_pack_v3/manifest.yaml\n"
     ]
    }
   ],
   "source": [
    "# --- A. Imports & config ---\n",
    "from pathlib import Path\n",
    "import json, hashlib, uuid, yaml\n",
    "from typing import List, Dict\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Paths (adapt for your pack root)\n",
    "ROOT = Path('/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/first_aid_knowledge_pack_v3')\n",
    "MANIFEST = ROOT / \"manifest.yaml\"\n",
    "print(ROOT)\n",
    "print(MANIFEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb7439",
   "metadata": {},
   "source": [
    "### Parsing YAML, Embedding Documents, and Creating Vector Store\n",
    "\n",
    "NOTES: \n",
    "1. Below cell will create a new directory inisde the knolwedge pack:\n",
    "- Example: first_aid_pack_demo_v2/vector_db/text/faiss_index <br>\n",
    "This directory will have the actual .faiss store and index pickle file\n",
    "\n",
    "2. embeddings.jsonl, index.bin, and meta.json under first_aid_pack_demo_v2/vector_db/text/faiss_index will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38d36beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import yaml\n",
    "\n",
    "# LangChain docs object (compat across versions)\n",
    "try:\n",
    "    from langchain_core.documents import Document\n",
    "except ImportError:\n",
    "    from langchain.schema import Document  # older LC\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "ans = input(\"Y/N\")\n",
    "if ans == \"Y\":\n",
    "    # === PDF chunk & embed (manifest-driven) ===\n",
    "    # deps: pip install pypdf\n",
    "\n",
    "    # -------------------- 0) Load manifest & resolve paths --------------------\n",
    "    with open(MANIFEST, \"r\", encoding=\"utf-8\") as f:\n",
    "        manifest = yaml.safe_load(f)\n",
    "\n",
    "    # Embedding + chunking config from manifest (with sensible fallbacks)\n",
    "    embed_model_name = manifest[\"embedding_config\"][\"text\"][\"model\"]  # e.g., \"granite-embedding:278m\"\n",
    "    normalize   = bool(manifest[\"embedding_config\"][\"text\"].get(\"normalize\", True))\n",
    "    max_tokens  = int(manifest[\"embedding_config\"][\"text\"][\"chunking\"].get(\"max_tokens\", 512))\n",
    "    overlap_toks= int(manifest[\"embedding_config\"][\"text\"][\"chunking\"].get(\"overlap_tokens\", 64))\n",
    "\n",
    "    # very rough char≈token conversion for character-based splitter\n",
    "    TOK_TO_CHAR = 4  # tune if your corpora are very different\n",
    "    chunk_size_chars = max(64, max_tokens * TOK_TO_CHAR)\n",
    "    overlap_chars    = max(0,  overlap_toks * TOK_TO_CHAR)\n",
    "\n",
    "    # Resolve precomputed index paths from manifest\n",
    "    text_idx_cfg        = manifest[\"precomputed_indices\"][\"text\"]\n",
    "    embeddings_path     = ROOT / text_idx_cfg[\"embeddings\"]             # \"vector_db/text/embeddings.jsonl\"\n",
    "    meta_path           = ROOT / text_idx_cfg[\"meta\"]                   # \"vector_db/text/meta.json\"\n",
    "    faiss_dir           = ROOT / text_idx_cfg[\"faiss\"][\"dir\"]           # \"vector_db/text/faiss_index\"\n",
    "    faiss_index_path    = ROOT / text_idx_cfg[\"faiss\"][\"index\"]         # \".../index.faiss\"\n",
    "    faiss_docstore_path = ROOT / text_idx_cfg[\"faiss\"][\"docstore\"]      # \".../index.pkl\"\n",
    "\n",
    "    faiss_dir.mkdir(parents=True, exist_ok=True)\n",
    "    embeddings_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    meta_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Using FAISS dir:\", faiss_dir)\n",
    "    print(\"Embeddings JSONL:\", embeddings_path)\n",
    "    print(\"Meta JSON:\", meta_path)\n",
    "\n",
    "    # -------------------- 1) PDF extraction helpers --------------------\n",
    "    from pypdf import PdfReader\n",
    "\n",
    "    def extract_pdf_pages(pdf_path: Path) -> List[Tuple[int, str]]:\n",
    "        \"\"\"Return list of (1-indexed page_number, text). Empty pages become ''.\"\"\"\n",
    "        pages: List[Tuple[int, str]] = []\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            txt = page.extract_text() or \"\"\n",
    "            txt = txt.replace(\"\\u00A0\", \" \").strip()\n",
    "            pages.append((i + 1, txt))\n",
    "        return pages\n",
    "\n",
    "    # -------------------- 2) Chunker (character-based with overlap) --------------------\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"। \", \". \", \"?\", \"!\", \" \"],\n",
    "        chunk_size=chunk_size_chars,\n",
    "        chunk_overlap=overlap_chars,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    def chunk_page_text(page_text: str) -> List[str]:\n",
    "        if not page_text:\n",
    "            return []\n",
    "        return [c.strip() for c in splitter.split_text(page_text) if c.strip()]\n",
    "\n",
    "    # -------------------- 3) Build Documents (PDF-aware) --------------------\n",
    "    docs: List[Document] = []\n",
    "    pack_name    = manifest.get(\"name\")\n",
    "    pack_ver     = manifest.get(\"version\")\n",
    "    pack_locales = manifest.get(\"locales\", [])\n",
    "\n",
    "    # map citation id -> full citation object (preserves your manifest’s structure)\n",
    "    citations_by_id = {c[\"id\"]: c for c in manifest.get(\"citations\", [])}\n",
    "\n",
    "    num_files = 0\n",
    "    num_skipped_empty = 0\n",
    "\n",
    "    for topic in manifest.get(\"index_of_topics\", []):\n",
    "        topic_id = topic[\"id\"]\n",
    "        for fmeta in topic.get(\"core_files\", []):\n",
    "            # Only process PDFs (by media_type or extension)\n",
    "            media_type = fmeta.get(\"media_type\", \"\").lower()\n",
    "            fpath = ROOT / fmeta[\"path\"]\n",
    "            if not fpath.exists():\n",
    "                print(\"! Skipping missing file:\", fpath)\n",
    "                continue\n",
    "            if not (media_type == \"pdf\" or fpath.suffix.lower() == \".pdf\"):\n",
    "                # skip non-PDFs in this cell; handle elsewhere if needed\n",
    "                continue\n",
    "\n",
    "            pages = extract_pdf_pages(fpath)\n",
    "            if not any(p_txt for _, p_txt in pages):\n",
    "                print(f\"! PDF has no extractable text (scanned images?): {fpath}\")\n",
    "                num_skipped_empty += 1\n",
    "                continue\n",
    "\n",
    "            # crude locale inference: keep your existing heuristic\n",
    "            locale = \"hi_en\" if \"/hi_en/\" in fmeta[\"path\"] else fmeta.get(\"locale\", \"en\")\n",
    "\n",
    "            # expand citations\n",
    "            c_full = [citations_by_id[cid] for cid in fmeta.get(\"citations\", []) if cid in citations_by_id]\n",
    "\n",
    "            file_chunk_counter = 0\n",
    "            for page_num, page_text in pages:\n",
    "                for piece in chunk_page_text(page_text):\n",
    "                    docs.append(\n",
    "                        Document(\n",
    "                            page_content=piece,\n",
    "                            metadata={\n",
    "                                \"pack_name\": pack_name,\n",
    "                                \"pack_version\": pack_ver,\n",
    "                                \"topic_id\": topic_id,\n",
    "                                \"file_id\": fmeta[\"id\"],\n",
    "                                \"path\": str(fmeta[\"path\"]),\n",
    "                                \"media_type\": media_type or \"pdf\",\n",
    "                                \"locale\": locale,\n",
    "                                \"citations\": c_full,\n",
    "                                \"page\": page_num,\n",
    "                                \"chunk_index\": file_chunk_counter,\n",
    "                                \"chunk_id\": f\"{fmeta['id']}::p{page_num}::chunk::{file_chunk_counter}\",\n",
    "                                \"doc_type\": \"pdf\",\n",
    "                            },\n",
    "                        )\n",
    "                    )\n",
    "                    file_chunk_counter += 1\n",
    "\n",
    "            num_files += 1\n",
    "\n",
    "    print(f\"Prepared {len(docs)} PDF chunks from {num_files} PDFs (skipped empty-text PDFs: {num_skipped_empty})\")\n",
    "\n",
    "    # -------------------- 4) Embeddings + FAISS persist --------------------\n",
    "    # Local Ollama embeddings (e.g., granite-embedding:278m)\n",
    "    emb = OllamaEmbeddings(model=embed_model_name)\n",
    "\n",
    "    vs = FAISS.from_documents(docs, emb)\n",
    "    vs.save_local(str(faiss_dir))  # writes index.faiss + index.pkl (overwrites if they exist)\n",
    "\n",
    "    # sanity check\n",
    "    assert faiss_index_path.exists(), f\"Missing {faiss_index_path}\"\n",
    "    assert faiss_docstore_path.exists(), f\"Missing {faiss_docstore_path}\"\n",
    "    print(\"FAISS artifacts saved ✅\", faiss_index_path.name, \"&\", faiss_docstore_path.name)\n",
    "\n",
    "    # -------------------- 5) Export JSONL embeddings + meta (portable) --------------------\n",
    "    # Note: This re-embeds each chunk for JSONL output (simple + explicit).\n",
    "    records = []\n",
    "    doc_items = getattr(vs.docstore, \"_dict\", {})  # (doc_id -> Document), common LC pattern\n",
    "\n",
    "    for doc_id, doc in doc_items.items():\n",
    "        vec = emb.embed_query(doc.page_content)  # dim depends on your Ollama embedding model\n",
    "        rec = {\n",
    "            \"id\": doc_id,\n",
    "            \"embedding\": vec,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"text\": doc.page_content,\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    with open(embeddings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"model\": embed_model_name,\n",
    "                \"dim\": manifest[\"embedding_config\"][\"text\"].get(\"dim\"),  # keep your manifest’s declared dim\n",
    "                \"normalize\": normalize,\n",
    "                \"count\": len(records),\n",
    "                \"pack\": {\"name\": pack_name, \"version\": pack_ver, \"locales\": pack_locales},\n",
    "                \"chunking\": {\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"overlap_tokens\": overlap_toks,\n",
    "                    \"approx_chars_per_token\": TOK_TO_CHAR,\n",
    "                    \"chunk_size_chars\": chunk_size_chars,\n",
    "                    \"overlap_chars\": overlap_chars,\n",
    "                },\n",
    "            },\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(\"JSONL/meta saved ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0ed59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1158dcd9",
   "metadata": {},
   "source": [
    "### Testing it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad040d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "New Delhi – 110001 \n",
      "Ph.: 011-23322237 \n",
      "       011-23720143 \n",
      "       011-23717789 \n",
      " \n",
      "45 The Commissioner \n",
      "Tamil Nadu Brigade Wing St John \n",
      "Ambulance Association (India),  \n",
      "No. 1, Mayor V.R. Ramanathan Road \n",
      "(East), Chetput,  \n",
      "Chennai – 600031 \n",
      " \n",
      "40 The Commissioner \n",
      "Haryana Brigade Wing St. John \n",
      "Ambu ...\n",
      "\n",
      "[2]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "350 \n",
      "37 Chief Medical Director, Honorary \n",
      "Secretary & The Addl. Commissioner \n",
      "Northern Railway Centre St. John \n",
      "Ambulance Association (India), \n",
      "Baroda House, Medical Dept, 1st \n",
      "Floor,  \n",
      "New Delhi – 110001 \n",
      " \n",
      "43 The Commissioner \n",
      "Punjab Brigade Wing St. John \n",
      "Ambulance Association (India), \n",
      "Punjab Re ...\n",
      "\n",
      "[3]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "Raipur, Chhattisgarh – 492001 \n",
      "Ph.: 0771-4091982 \n",
      " \n",
      "11 The Secretary \n",
      "Himachal Pradesh State Centre St. \n",
      "John Ambulance Association, \n",
      "Red Cross Bhawan, Barnes Court, Near \n",
      "Raj Bhawan, \n",
      "Shimla – 171002 \n",
      "Ph.: 0177-2621868 \n",
      "6 The Secretary  \n",
      "Dadra Nagar Haveli U.T. Branch St. \n",
      "John Ambulance Associatio ...\n",
      "\n",
      "[4]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "349 \n",
      "25 The Honorary Secretary  \n",
      "U.P. State Centre St. John \n",
      "Ambulance Association (India), \n",
      "Red Cross Bhawan, Raja Nawab Ali \n",
      "Road, \n",
      "Lucknow - 226001 \n",
      "Ph.: 0522-2231672  Fax: 0522-\n",
      "2625159 \n",
      " \n",
      "31 The Addl. Chief Medical \n",
      "Superintendent \n",
      "North Western Railway St. John \n",
      "Ambulance Association (India),  ...\n",
      "\n",
      "[5]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "Fax: 0172-2549434 \n",
      " 35 The Honorary Secretary                                          \n",
      "Uttar Pradesh State Branch                                              \n",
      "Indian Red Cross Society    \n",
      "Raja Nawab Ali Road \n",
      "Lucknow 226001    \n",
      "Ph.: 0522-2231672  \n",
      "Fax: 0522-2625159 \n",
      "30 The Administrator \n",
      "Rajasthan ...\n",
      "\n",
      "[6]\n",
      "Topic: contacts\n",
      "File: contacts-serviceplus-manual\n",
      "Locale: hi_en\n",
      "Citations: ['ServicePlus Applicant User Manual — Government of Bihar']\n",
      "Chunk text:\n",
      "Panchayat Samiti / Ji la Parishad) , Block , Circle, Urban Local Body (Nagar \n",
      "Panchayat / Nagar Parishad / Nagar Nigam), Sub-Division, District, Division, \n",
      "Department etc) - as per their jurisdiction. \n",
      " \n",
      " Executive Assistants (EA) are advised to go through “ Applicant User Manual ” \n",
      "before applying ...\n",
      "\n",
      "[7]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "346 \n",
      "25 The Secretary  \n",
      "Mizoram State Branch  \n",
      "Indian Red Cross Society \n",
      "Chaltang Dawrkawn  \n",
      "Aizwal 796 001 \n",
      "Ph.: 0389-2316325 \n",
      "Fax: 0389-2316325; 2320169 \n",
      " 31 The Honorary Secretary                                          \n",
      "Sikkim Branch                                                   \n",
      "Indian Red ...\n",
      "\n",
      "[8]\n",
      "Topic: pesticides\n",
      "File: pesticides-fao-ipm-ffs\n",
      "Locale: hi_en\n",
      "Citations: ['Farmer Field School: Integrated Pest Management — FAO and Directorate of Plant Protection, Quarantine and Storage']\n",
      "Chunk text:\n",
      "activities.\n",
      "2. Invitations to be made to the lead farmers, village \n",
      "head/panchayat members/ state government \n",
      "functionaries\n",
      "Coordination with stakeholders\n",
      "Farmers are the major stakeholders of IPM-FFS \n",
      "programme. Hence, to identify the specific village \n",
      "and target groups are very much important. It  ...\n",
      "\n",
      "[9]\n",
      "Topic: contacts\n",
      "File: contacts-bihar-police-sdpo\n",
      "Locale: hi_en\n",
      "Citations: ['Contact Details of Sub-Divisional Police Officers — Bihar Police']\n",
      "Chunk text:\n",
      "Sheikhpura \n",
      "S.D.P.O., Shekhpura Sadar  6341  223347     94318-00023  \n",
      "Sitamarhi \n",
      "S.D.P.O., Belsand  6226  281269  251224    94318-00084  \n",
      "S.D.P.O., Sitamarhi East       94318-00085  \n",
      "S.D.P.O., Sitamarhi Sadar  6226   250219    94318-00086  \n",
      "Siwan \n",
      "S.D.P.O., Maharajganj  6153  242333  242333    94318 ...\n",
      "\n",
      "[10]\n",
      "Topic: bleeding\n",
      "File: bleeding-ircs-manual\n",
      "Locale: hi_en\n",
      "Citations: ['First Aid Manual — Indian Red Cross Society']\n",
      "Chunk text:\n",
      "Assam State Branch    \n",
      "Indian Red Cross Society                                   \n",
      "Chandmari   \n",
      "Guwahati 781 003 \n",
      "Ph.: 0361-2665114; 2545114; 2516293 \n",
      "Fax: 0361-2660210; 2558440   \n",
      " 11 The Honorary Secretary                                             \n",
      "NCT of Delhi Branch                             ...\n"
     ]
    }
   ],
   "source": [
    "# Typical retriever usage\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 10})  # if you used Option A 'vs'\n",
    "query = \"Subdivisional Police Officers contacts S.D.P.O., Forbesganj?\"  #\n",
    "hits = retriever.invoke(query)\n",
    "\n",
    "for i, d in enumerate(hits, 1):\n",
    "    print(f\"\\n[{i}]\")\n",
    "    print(\"Topic:\", d.metadata[\"topic_id\"])\n",
    "    print(\"File:\", d.metadata[\"file_id\"])\n",
    "    print(\"Locale:\", d.metadata[\"locale\"])\n",
    "    print(\"Citations:\", [c[\"title\"] for c in d.metadata.get(\"citations\", [])])\n",
    "    print(\"Chunk text:\")\n",
    "    print(d.page_content[:300], \"...\" if len(d.page_content) > 300 else \"\")\n",
    "\n",
    "# Filter to a topic or locale:\n",
    "# hits = retriever.invoke(\"tourniquet steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe1544",
   "metadata": {},
   "source": [
    ":)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609f501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
