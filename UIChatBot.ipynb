{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c03167",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "This notebook explores how to wrap the chatbot in a gradio chat interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a332a",
   "metadata": {},
   "source": [
    "###  Setting up paths and loading vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d166f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0) Imports & manifest ===\n",
    "from pathlib import Path\n",
    "import yaml, json\n",
    "from typing import Optional, List, Dict\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools import tool\n",
    "from langchain.schema import Document\n",
    "\n",
    "# If you want the prebuilt ReAct agent:\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# If you prefer to also expose tools directly on the LLM:\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# --- Set your pack root + manifest ---\n",
    "ROOT = Path(\"/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/hurricane_disaster_response_pack\")\n",
    "MANIFEST = ROOT / \"manifest.yaml\"\n",
    "\n",
    "with open(MANIFEST, \"r\", encoding=\"utf-8\") as f:\n",
    "    manifest = yaml.safe_load(f)\n",
    "\n",
    "# --- Resolve FAISS paths from manifest ---\n",
    "faiss_dir_text = ROOT / manifest[\"precomputed_indices\"][\"text\"][\"faiss\"][\"dir\"]\n",
    "faiss_dir_image = ROOT / manifest[\"precomputed_indices\"][\"images\"][\"faiss\"][\"dir\"]\n",
    "# --- Create embeddings *matching the store* ---\n",
    "embed_model_name = manifest[\"embedding_config\"][\"text\"][\"model\"]     #same for text and image\n",
    "emb = OllamaEmbeddings(model=embed_model_name)\n",
    "\n",
    "# --- Load FAISS store + retriever ---\n",
    "text_vs = FAISS.load_local(str(faiss_dir_text), emb, allow_dangerous_deserialization=True)\n",
    "text_retriever = text_vs.as_retriever(search_kwargs={\"k\": 4})  # default k; tool will override if provided\n",
    "\n",
    "image_vs = FAISS.load_local(str(faiss_dir_image), emb, allow_dangerous_deserialization=True)\n",
    "# results = image_vs.similarity_search_with_score(query, k=4)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93926748",
   "metadata": {},
   "source": [
    "### Defining helper and context() Tool  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a18ea3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) Helpers ===\n",
    "def format_chunk(doc: Document, max_chars: int = 400) -> Dict:\n",
    "    \"\"\"Return a dict with compact text + key metadata for prompting & audit.\"\"\"\n",
    "    txt = doc.page_content.strip()\n",
    "    if len(txt) > max_chars:\n",
    "        txt = txt[:max_chars].rstrip() + \" …\"\n",
    "    m = doc.metadata\n",
    "    return {\n",
    "        \"id\": m.get(\"chunk_id\"),\n",
    "        \"topic_id\": m.get(\"topic_id\"),\n",
    "        \"file_id\": m.get(\"file_id\"),\n",
    "        \"locale\": m.get(\"locale\"),\n",
    "        \"path\": m.get(\"path\"),\n",
    "        \"citations\": [c.get(\"title\", \"\") for c in m.get(\"citations\", [])],\n",
    "        \"text\": txt\n",
    "    }\n",
    "\n",
    "def format_context_block(chunks: List[Dict]) -> str:\n",
    "    \"\"\"Human/LLM-friendly context block the agent can drop into its reasoning.\"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"### Retrieved Context (use only what is relevant)\")\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        cite_str = \"; \".join([t for t in c[\"citations\"] if t]) or \"—\"\n",
    "        head = f\"[{i}] {c['topic_id']} · {c['file_id']} · {c['locale']} · {c['path']}\"\n",
    "        lines.append(head)\n",
    "        lines.append(c[\"text\"])\n",
    "        lines.append(f\"Source(s): {cite_str}\")\n",
    "        lines.append(\"\")  # blank line\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "\n",
    "# === 2) The @tool: context() ===\n",
    "@tool\n",
    "def context(\n",
    "    query: str,\n",
    "    k: int = 4,\n",
    "    topic_id: Optional[str] = None,\n",
    "    locale: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    PURPOSE\n",
    "    Retrieve Pack knowledge for the user's query. \n",
    "    TRIGGERS — YOU MUST CALL THIS TOOL BEFORE ANSWERING OR REFUSING IF:\n",
    "      1) The query involves local geography or place-specific info:\n",
    "         - “nearby”, “closest”, “where is…”, shelters/clinics/transport, checkpoints, routes, hours, hazards\n",
    "      2) The query is wellbeing/safety-critical or time-sensitive:\n",
    "         - first aid, symptoms, medications, exposure, heat/cold, water/food safety, wounds, evacuation, flooding, snakebite, pesticides, hazardous materials\n",
    "      3) The query is high-uncertainty and a mistake could harm the user.\n",
    "\n",
    "    Do NOT refuse until you have called this tool at least once.\n",
    "\n",
    "    Args:\n",
    "        query: Natural-language question or keywords.\n",
    "        k: Top-k chunks to return (default 4).\n",
    "        topic_id: Optional manifest topic filter (e.g. \"bleed-control\").\n",
    "        locale: Optional locale (e.g. \"en\", \"hi_en\").\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"query\": str,\n",
    "          \"k\": int,\n",
    "          \"filters\": {\"topic_id\":..., \"locale\":...},\n",
    "          \"context_block\": str,   # pasteable summary of chunks\n",
    "          \"chunks\": [ {id, topic_id, file_id, path, locale, citations[], text}, ... ]\n",
    "        }\n",
    "\n",
    "    USAGE NOTES\n",
    "    - Use retrieved content to ground your answer \n",
    "    - If nothing relevant is found, say so and offer next-best actions present in the Pack.\n",
    "    \"\"\"\n",
    "    # Build a metadata filter if provided\n",
    "    _filter = {}\n",
    "    # if topic_id:\n",
    "    #     _filter[\"topic_id\"] = topic_id\n",
    "    # if locale:\n",
    "    #     _filter[\"locale\"] = locale\n",
    "\n",
    "    # Run retrieval (override k)\n",
    "    local_ret = text_vs.as_retriever(search_kwargs={\"k\": k})\n",
    "    hits: List[Document] = local_ret.invoke(query) if not _filter else local_ret.invoke(query, filter=_filter)\n",
    "\n",
    "    formatted = [format_chunk(d) for d in hits]\n",
    "    ctx_block = format_context_block(formatted)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"k\": k,\n",
    "        \"filters\": _filter,\n",
    "        \"context_block\": ctx_block,\n",
    "        \"chunks\": formatted\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b651",
   "metadata": {},
   "source": [
    "### Math Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fbe93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract first number by second number.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a : float, b: float) -> float:\n",
    "    \"\"\"Divide first number by second number.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return a/b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69135961",
   "metadata": {},
   "source": [
    "### Defining knowledgeMeta() tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b002ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from typing import Optional\n",
    "\n",
    "@tool\n",
    "def knowledgeMeta(pack_dir: Optional[str] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Read a knowledge pack manifest and return metadata for trust and recency. \n",
    "\n",
    "    Args:\n",
    "      pack_dir: Absolute or relative path to the pack folder (containing manifest.yaml).\n",
    "                If omitted, uses the default ROOT pack path.\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"name\": str,\n",
    "        \"version\": str,\n",
    "        \"date\": str,\n",
    "        \"locales\": [..],\n",
    "        \"topics_count\": int,\n",
    "        \"manifest_path\": str\n",
    "      }\n",
    "\n",
    "    \"\"\"\n",
    "    # default to your earlier ROOT if not provided\n",
    "    base = Path(pack_dir) if pack_dir else ROOT\n",
    "    manifest_path = base / \"manifest.yaml\"\n",
    "    if not manifest_path.exists():\n",
    "        return {\"error\": f\"manifest.yaml not found at {manifest_path}\"}\n",
    "\n",
    "    with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        m = yaml.safe_load(f)\n",
    "\n",
    "    name = m.get(\"name\", str(base.name))\n",
    "    version = m.get(\"version\", \"unknown\")\n",
    "    date = m.get(\"date\", \"unknown\")\n",
    "    locales = m.get(\"locales\", [])\n",
    "    topics = m.get(\"index_of_topics\", []) or []\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"version\": version,\n",
    "        \"date\": date,\n",
    "        \"locales\": locales,\n",
    "        \"topics_count\": len(topics),\n",
    "        \"manifest_path\": str(manifest_path)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781c4fc",
   "metadata": {},
   "source": [
    "### getImage tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d162578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "HIGH_SCORE_THRESHOLD = 0.55  # tune as needed\n",
    "\n",
    "@tool\n",
    "def getImage(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    PURPOSE\n",
    "    Retrieve a single high-confidence Pack image for the query. DO NOT DISPLAY IMAGE, IT IS DONE SO AUTOMATICALLY AT THE TOP OF YOUR MESSAGE.\n",
    "\n",
    "    TRIGGERS — YOU MUST CALL THIS TOOL WHEN:\n",
    "      1) The user asks to \"show\" or \"see\" something (e.g., “show me the Heimlich position”),\n",
    "      2) The user requests a diagram or visual (diagram / illustrate / picture / visual / map),\n",
    "      3) A visual guide would materially improve a physical technique (CPR posture, tourniquet placement, splinting, boiling water, wound cleaning, snakebite immobilization).\n",
    "\n",
    "    If no suitable image is found, return NO_IMAGE and proceed with clear step-by-step text (and cite “context” if used).\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"status\": \"OK\" | \"NO_IMAGE\",\n",
    "          \"version\": str,\n",
    "          \"date\": str,\n",
    "          \"locales\": [str],\n",
    "          \"pack_name\": str,\n",
    "          \"image_path\": str,      # absolute or pack-relative path\n",
    "          \"score\": float | None,\n",
    "          \"citations\": list       # e.g., [{\"title\": \"...\", ...}]\n",
    "        }\n",
    "    \"\"\"\n",
    "    q = (query or \"\").strip()\n",
    "    pack_name    = manifest.get(\"name\", \"\")\n",
    "    pack_ver     = manifest.get(\"version\", \"\")\n",
    "    pack_date    = manifest.get(\"date\", \"\")\n",
    "    pack_locales = manifest.get(\"locales\", [])\n",
    "\n",
    "    if not q:\n",
    "        return {\n",
    "            \"status\": \"NO_IMAGE\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": \"\",\n",
    "            \"score\": None,\n",
    "            \"citations\": []\n",
    "        }\n",
    "\n",
    "    # Use similarity_search_with_score to get confidence scores\n",
    "    foundImage = False\n",
    "    minScore = 0.32\n",
    "    finalDoc  = Document(page_content=\"\")\n",
    "    results = image_vs.similarity_search_with_score(query, k=4)\n",
    "    finScore = 0\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    for i, (d, score) in enumerate(results, 1):\n",
    "        \n",
    "        if score >= minScore:\n",
    "            finScore = score\n",
    "            finalDoc = d\n",
    "            foundImage = True\n",
    "            break\n",
    "    \n",
    "\n",
    "    if not foundImage:\n",
    "        print(\"NOT FOUND\")\n",
    "        return {\n",
    "            \"status\": \"NO_IMAGE\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": \"\",\n",
    "            \"score\": None,\n",
    "            \"citations\": []\n",
    "        }\n",
    "    else:\n",
    "        print(\"Found FOUND\")\n",
    "        img_path = ROOT / finalDoc.metadata['path']\n",
    "        # try:\n",
    "        #     display(Image(filename=img_path))\n",
    "        # except Exception:\n",
    "        #     pass\n",
    "            \n",
    "        return {\n",
    "            \"status\": \"OK\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": str(img_path),\n",
    "            \"score\": float(finScore),\n",
    "            \"citations\": finalDoc.metadata.get(\"citations\", [])\n",
    "        }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad48bd",
   "metadata": {},
   "source": [
    "### List of Available Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14c9825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a Tools list for whichever orchestration you choose:\n",
    "TOOLS = [context,add,multiply,subtract,divide,knowledgeMeta, getImage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22fb2c",
   "metadata": {},
   "source": [
    "### Setting Up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14a99997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from gradio import ChatMessage\n",
    "import gradio as gr\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "lightModel = init_chat_model(\n",
    "    model=\"ollama:llama3.1\",       \n",
    "    temperature=0.2, # lower = more deterministic\n",
    "    streaming = True  \n",
    ")\n",
    "\n",
    "heavyModel = init_chat_model(\n",
    "    model=\"ollama:gpt-oss:20b\",       \n",
    "    temperature=0.2  # lower = more deterministic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b2695b",
   "metadata": {},
   "source": [
    "### Using gradio with langchain\n",
    "\n",
    "\n",
    "\n",
    "This is a simple general-purpose chatbot built on top of LangChain and Gradio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb3340ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage  \n",
    "import gradio as gr\n",
    "\n",
    "lightModel = init_chat_model(\n",
    "    model=\"ollama:llama3.1\",       \n",
    "    temperature=0.2  # lower = more deterministic\n",
    ")\n",
    "\n",
    "def predict(message, history):\n",
    "    history_langchain_format = []\n",
    "    for msg in history:\n",
    "        if msg['role'] == \"user\":\n",
    "            history_langchain_format.append(HumanMessage(content=msg['content']))\n",
    "        elif msg['role'] == \"assistant\":\n",
    "            history_langchain_format.append(AIMessage(content=msg['content']))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "    llm_response = lightModel.invoke(history_langchain_format)\n",
    "    return llm_response.content\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    predict,\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ce7e4",
   "metadata": {},
   "source": [
    "### Working UI with tool calling and sources\n",
    "\n",
    "Images is next "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ded796d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools = TOOLS\n",
    "\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from gradio import ChatMessage\n",
    "import gradio as gr\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools.render import render_text_description\n",
    "\n",
    "# Render tool descriptions and names\n",
    "tool_desc = render_text_description(TOOLS)         # Render the tool name and description in plain text.\n",
    "tool_names = \", \".join([t.name for t in TOOLS])    # exact callable names\n",
    "\n",
    "# Build agent prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"You are Beacon, a helpful assistant that answers using the active Knowledge Pack (domain- and locale-specific, offline-first).\n",
    "\n",
    "HARD RULES — TOOL USE\n",
    "1) You MUST call the tool named “context” BEFORE answering or refusing if:\n",
    "   • The user asks about local geography, “near me/nearby/closest”, directions, hours, routes, checkpoints, shelters, clinics, water points, transport, or place-specific availability, OR\n",
    "   • The user’s request affects health, safety, or wellbeing (first aid, symptoms, medications, exposure, heat/cold, water safety, food safety, wound care, evacuation decisions, flooding, snakebite, pesticides, hazardous materials), OR\n",
    "   • The query is high-stakes, time-sensitive, or ambiguous in a way that could impact safety.\n",
    "   → Do not refuse UNTIL you have called “context”.\n",
    "\n",
    "2) You MUST call the tool named “getImage” when:\n",
    "   • The user says “show me …”, “what does … look like?”, “diagram”, “visual”, “illustrate”, OR\n",
    "   • A visual guide would materially improve understanding for a physical technique (e.g., CPR posture, Heimlich, tourniquet placement, splinting, water boiling steps, winding a bandage).\n",
    "   If “getImage” returns NO_IMAGE, continue with clear, step-by-step text and cite sources from “context” if available.\n",
    "\n",
    "3) Tool names must match exactly from {tool_names}. Prefer the smallest sufficient k. If a Pack locale or topic is clear, pass it.\n",
    "\n",
    "4) Citations: When “context” is used, include a short “Sources” line drawn from its returned citations when you give final guidance.\n",
    "\n",
    "5) Refusals: Only refuse after calling “context” if (a) the Pack lacks relevant guidance or (b) the request is outside your guardrails. In refusals, suggest the nearest safe alternative or escalation path if the Pack provides one.\n",
    "\n",
    "OUTPUT STYLE\n",
    "• Be concise, stepwise, and actionable. If life/safety-critical, front-load DOs and DON’Ts and emphasize time-critical steps.\n",
    "• If the Pack is ambiguous, state assumptions briefly and continue.\n",
    "• NEVER hallucinate locations or phone numbers—use “context”. If none found, say so and offer next-best actions present in the Pack.\n",
    "\n",
    "You MUST use the following tools at least once if the above rules apply:\n",
    "{tools}\n",
    "(Always call tools by exact name from: {tool_names}. If no rule triggers, you may answer directly.)\n",
    "\"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "]).partial(tools=tool_desc, tool_names=tool_names)\n",
    "\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=heavyModel,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,        # helpful while wiring things up\n",
    ").with_config({\"run_name\": \"Agent\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6aaba0",
   "metadata": {},
   "source": [
    "### DEV UI (OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99779dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new Agent chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `context` with `{'k': 4, 'locale': 'en', 'query': 'evacuation points'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'evacuation points', 'k': 4, 'filters': {}, 'context_block': '### Retrieved Context (use only what is relevant)\\n[1] hurricane-readiness · all-hazard-guide · en · core/hurricane-readiness/en/All_Hazard_Guide-002.pdf\\nBecause evacuations take time to ensure everyone can get to safety, they are called \\nwell in advance of the storm. Evacuations have a beginning and end time. You must \\nbe in a safe location by the end of the evacuation period, well before  \\nthe storm surge and high winds arrive.\\nEvacuation orders issued by Pinellas County are for the entire county, including  cities \\nand unincorporated areas.\\nList …\\nSource(s): Pinellas County: All-Hazard Preparedness Guide (Liberty copy)\\n\\n[2] flooding-storm-safety · all-hazard-preparedness · en · core/flood-storm-safety/en/Pinellas-County-All-Hazard-Guide.pdf\\nPAGE 8\\n   ALL HAZARDS PREPAREDNESS GUIDE    ALL HAZARDS PREPAREDNESS GUIDE\\nPINELLAS COUNTY EVACUATION ZONE MAP\\nE\\nA\\nB\\nC\\nD\\nUp to 35’\\nUp to 28’\\nUp to 20’\\nUp to 15’\\nUp to 11’\\nPotential Surge Heights (in feet)\\nAreas shown in white are non-evacuation zones.\\nSurge height will vary depending on ground elevation.\\nEvacuate red areas and all\\nmobile homes\\nEvacuate red and orange areas \\nand all mobile homes\\nEv …\\nSource(s): Pinellas County: All-Hazard Preparedness Guide\\n\\n[3] flooding-storm-safety · all-hazard-preparedness · en · core/flood-storm-safety/en/Pinellas-County-All-Hazard-Guide.pdf\\ncan follow those orders. \\n•\\n S\\ntay tuned to local news and get your weather \\nradio ready.\\n•\\n C\\nomplete final preparations to evacuate or to \\nshelter in your home.\\n•\\n I\\nf your plan is to travel out of the local area and \\nyou can leave at this point, go.\\n•\\n I\\nf you are registered for transportation to \\na public shelter, be ready to leave. Rescue \\nworkers will begin pickups shortly after an \\nevacuati …\\nSource(s): Pinellas County: All-Hazard Preparedness Guide\\n\\n[4] hurricane-readiness · all-hazard-guide · en · core/hurricane-readiness/en/All_Hazard_Guide-002.pdf\\n\\u085fLogin information and passwords\\nWhen it’s time to go\\nEvacuations are called according to evacuation zones. They \\napply to all types of properties. When an evacuation is \\nordered, businesses in the evacuation zone must leave  \\nas well.\\n \\u085fBefore you go, clear your property of debris and loose \\nitems. Take down signs that could be damaged in \\nhigh winds.\\n \\u085fPrepare the building by covering windows an …\\nSource(s): Pinellas County: All-Hazard Preparedness Guide (Liberty copy)', 'chunks': [{'id': 'all-hazard-guide::p2::chunk::3', 'topic_id': 'hurricane-readiness', 'file_id': 'all-hazard-guide', 'locale': 'en', 'path': 'core/hurricane-readiness/en/All_Hazard_Guide-002.pdf', 'citations': ['Pinellas County: All-Hazard Preparedness Guide (Liberty copy)'], 'text': 'Because evacuations take time to ensure everyone can get to safety, they are called \\nwell in advance of the storm. Evacuations have a beginning and end time. You must \\nbe in a safe location by the end of the evacuation period, well before  \\nthe storm surge and high winds arrive.\\nEvacuation orders issued by Pinellas County are for the entire county, including  cities \\nand unincorporated areas.\\nList …'}, {'id': 'all-hazard-preparedness::p8::chunk::28', 'topic_id': 'flooding-storm-safety', 'file_id': 'all-hazard-preparedness', 'locale': 'en', 'path': 'core/flood-storm-safety/en/Pinellas-County-All-Hazard-Guide.pdf', 'citations': ['Pinellas County: All-Hazard Preparedness Guide'], 'text': 'PAGE 8\\n   ALL HAZARDS PREPAREDNESS GUIDE    ALL HAZARDS PREPAREDNESS GUIDE\\nPINELLAS COUNTY EVACUATION ZONE MAP\\nE\\nA\\nB\\nC\\nD\\nUp to 35’\\nUp to 28’\\nUp to 20’\\nUp to 15’\\nUp to 11’\\nPotential Surge Heights (in feet)\\nAreas shown in white are non-evacuation zones.\\nSurge height will vary depending on ground elevation.\\nEvacuate red areas and all\\nmobile homes\\nEvacuate red and orange areas \\nand all mobile homes\\nEv …'}, {'id': 'all-hazard-preparedness::p5::chunk::18', 'topic_id': 'flooding-storm-safety', 'file_id': 'all-hazard-preparedness', 'locale': 'en', 'path': 'core/flood-storm-safety/en/Pinellas-County-All-Hazard-Guide.pdf', 'citations': ['Pinellas County: All-Hazard Preparedness Guide'], 'text': 'can follow those orders. \\n•\\n S\\ntay tuned to local news and get your weather \\nradio ready.\\n•\\n C\\nomplete final preparations to evacuate or to \\nshelter in your home.\\n•\\n I\\nf your plan is to travel out of the local area and \\nyou can leave at this point, go.\\n•\\n I\\nf you are registered for transportation to \\na public shelter, be ready to leave. Rescue \\nworkers will begin pickups shortly after an \\nevacuati …'}, {'id': 'all-hazard-guide::p9::chunk::28', 'topic_id': 'hurricane-readiness', 'file_id': 'all-hazard-guide', 'locale': 'en', 'path': 'core/hurricane-readiness/en/All_Hazard_Guide-002.pdf', 'citations': ['Pinellas County: All-Hazard Preparedness Guide (Liberty copy)'], 'text': '\\u085fLogin information and passwords\\nWhen it’s time to go\\nEvacuations are called according to evacuation zones. They \\napply to all types of properties. When an evacuation is \\nordered, businesses in the evacuation zone must leave  \\nas well.\\n \\u085fBefore you go, clear your property of debris and loose \\nitems. Take down signs that could be damaged in \\nhigh winds.\\n \\u085fPrepare the building by covering windows an …'}]}\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `getImage` with `{'query': 'evacuation zone map'}`\n",
      "\n",
      "\n",
      "\u001b[0mFound FOUND\n",
      "\u001b[36;1m\u001b[1;3m{'status': 'OK', 'version': '0.1.0', 'date': '2025-09-08', 'locales': ['en'], 'pack_name': 'Pinellas County – Hurricane Disaster Response', 'image_path': '/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/hurricane_disaster_response_pack/assets/hurricane-readiness/EvacMap-Shelters_North-1.png', 'score': 0.5465622544288635, 'citations': [{'id': 'palm-harbor-evac-map', 'title': 'Palm Harbor FD – Evacuation Map (North Pinellas)', 'url': 'https://www.palmharborfd.com/wp-content/uploads/2017/09/EvacMap-Shelters_North.pdf', 'license': 'All rights reserved'}]}\u001b[0m**Sources**\n",
      "- [Palm Harbor FD – Evacuation Map (North Pinellas)](https://www.palmharborfd.com/wp-content/uploads/2017/09/EvacMap-Shelters_North.pdf) · All rights reserved\n",
      "\u001b[32;1m\u001b[1;3mHere’s the evacuation zone map for the area.  \n",
      "*(The image is displayed automatically by the system.)*\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new Agent chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `knowledgeMeta` with `{'pack_dir': None}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m{'name': 'Pinellas County – Hurricane Disaster Response', 'version': '0.1.0', 'date': '2025-09-08', 'locales': ['en'], 'topics_count': 8, 'manifest_path': '/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/hurricane_disaster_response_pack/manifest.yaml'}\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mMy knowledge is current as of **September 8, 2025** (the latest update to the Pinellas County – Hurricane Disaster Response pack).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any, List\n",
    "\n",
    "def _format_sources_md(obs: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Render Sources from manifest-style citations only.\n",
    "    Accepts tool observations shaped like:\n",
    "      - getImage: { citations: [{title, id, url, license}, ...] }\n",
    "      - context:  { chunks: [{..., citations:[{title,id,url,license}, ...]} , ...] }\n",
    "\n",
    "    We ignore paths / file_ids and only show (title, url, license).\n",
    "    \"\"\"\n",
    "    if not isinstance(obs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    # Collect citation dicts from:\n",
    "    #  A) top-level obs[\"citations\"]\n",
    "    #  B) each chunk's [\"citations\"] (if present)\n",
    "    collected: List[Dict[str, Any]] = []\n",
    "\n",
    "    top_cites = obs.get(\"citations\", [])\n",
    "    if isinstance(top_cites, list):\n",
    "        for c in top_cites:\n",
    "            if isinstance(c, dict):\n",
    "                collected.append(c)\n",
    "\n",
    "    chunks = obs.get(\"chunks\", [])\n",
    "    if isinstance(chunks, list):\n",
    "        for ch in chunks:\n",
    "            if not isinstance(ch, dict):\n",
    "                continue\n",
    "            ch_cites = ch.get(\"citations\", [])\n",
    "            if isinstance(ch_cites, list):\n",
    "                for c in ch_cites:\n",
    "                    if isinstance(c, dict):\n",
    "                        collected.append(c)\n",
    "\n",
    "    # Deduplicate by (url or title), then format\n",
    "    seen = set()\n",
    "    lines: List[str] = []\n",
    "    for c in collected:\n",
    "        title = (c.get(\"title\") or c.get(\"id\") or \"Source\").strip()\n",
    "        url   = (c.get(\"url\") or \"\").strip()\n",
    "        lic   = (c.get(\"license\") or \"—\").strip()\n",
    "\n",
    "        key = (url or title).lower()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "\n",
    "        if url:\n",
    "            lines.append(f\"- [{title}]({url}) · {lic}\")\n",
    "        else:\n",
    "            lines.append(f\"- {title} · {lic}\")\n",
    "\n",
    "    if not lines:\n",
    "        return \"\"\n",
    "\n",
    "    return \"**Sources**\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def _maybe_render_image_from_obs(obs) -> ChatMessage | None:\n",
    "    \"\"\"\n",
    "    Accepts a tool observation dict and returns a ChatMessage with an image\n",
    "    if it finds a usable image path/url. Otherwise returns None.\n",
    "    \"\"\"\n",
    "    if not isinstance(obs, dict):\n",
    "        return None\n",
    "\n",
    "    # Common keys your chain/logs might produce\n",
    "    path = (\n",
    "        obs.get(\"image_path\")\n",
    "        or obs.get(\"path\")\n",
    "        or obs.get(\"file_path\")\n",
    "        or obs.get(\"local_path\")\n",
    "        or None\n",
    "    )\n",
    "    url = obs.get(\"image_url\") or obs.get(\"url\")\n",
    "\n",
    "    # Prefer local path if available; Gradio will serve files inside the app folder.\n",
    "    media = path or url\n",
    "    if not media:\n",
    "        return None\n",
    "\n",
    "    # Option A: return a component (nice rendering)\n",
    "    return ChatMessage(role=\"assistant\", content=gr.Image(value=media))\n",
    "\n",
    "\n",
    "# --- Async streaming handler for Gradio Chatbot(type=\"messages\") ---\n",
    "async def interact_with_langchain_agent(user_text, history):\n",
    "    # history is a list[ChatMessage]\n",
    "    history.append(ChatMessage(role=\"user\", content=user_text))\n",
    "    yield history\n",
    "\n",
    "    # Stream agent steps & final output\n",
    "    async for chunk in agent_executor.astream({\"input\": user_text}):\n",
    "        if \"steps\" in chunk:\n",
    "            for step in chunk[\"steps\"]:\n",
    "                history.append(ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=step.action.log,\n",
    "                    metadata={\"title\": f\"🛠️ Used tool {step.action.tool}\"}\n",
    "                ))\n",
    "\n",
    "\n",
    "                obs = getattr(step, \"observation\", None)\n",
    "                if obs is not None:\n",
    "                    # Try rendering an image if present (unchanged)\n",
    "                    img_msg = _maybe_render_image_from_obs(obs)\n",
    "                    if img_msg is not None:\n",
    "                        history.append(img_msg)\n",
    "                        yield history\n",
    "\n",
    "                    # NEW: one merged sources renderer for both getImage + context\n",
    "                    sources_md = _format_sources_md(obs)  # ← no second argument now\n",
    "                    print(sources_md)\n",
    "                    if sources_md:\n",
    "                        history.append(ChatMessage(role=\"assistant\", content=sources_md))\n",
    "                        yield history\n",
    "\n",
    "                yield history\n",
    "                #     else:\n",
    "                #         # For other tools you can also render their observations if you want\n",
    "                #         pass\n",
    "\n",
    "\n",
    "                # yield history\n",
    "        if \"output\" in chunk:\n",
    "            history.append(ChatMessage(role=\"assistant\", content=chunk[\"output\"]))\n",
    "            yield history\n",
    "\n",
    "\n",
    "# --- Gradio UI ---\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Beacon - Knowledge Agent (Pinellas County, FL)\")\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        type=\"messages\",\n",
    "        label=\"Agent\",\n",
    "        avatar_images=(None, \"/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/avatar.png\"),\n",
    "        height=650,\n",
    "    )\n",
    "    textbox = gr.Textbox(lines=1, label=\"Chat Message\", placeholder=\"Ask something…\")\n",
    "    # Clear the textbox after submit so it feels chatty\n",
    "    def _clear_now(_msg, _chat):\n",
    "        return gr.update(value=\"\")\n",
    "\n",
    "    # streaming submit\n",
    "    textbox.submit(\n",
    "        interact_with_langchain_agent,\n",
    "        inputs=[textbox, chatbot],\n",
    "        outputs=[chatbot],\n",
    "    )\n",
    "\n",
    "    # instant clear\n",
    "    textbox.submit(\n",
    "        _clear_now,\n",
    "        inputs=[textbox, chatbot],   # must match the event’s inputs (2 args)\n",
    "        outputs=[textbox],\n",
    "        queue=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "demo.queue().launch()   # queue() is recommended for async callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902a19c",
   "metadata": {},
   "source": [
    "### CURRENT UI NO TOOL CALLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea942e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new Agent chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello! How can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# --- citations: manifest-only (title, url, license) ---\n",
    "def _format_sources_md(obs: Dict[str, Any]) -> str:\n",
    "    if not isinstance(obs, dict):\n",
    "        return \"\"\n",
    "    collected: List[Dict[str, Any]] = []\n",
    "\n",
    "    # top-level citations (e.g., getImage)\n",
    "    top = obs.get(\"citations\")\n",
    "    if isinstance(top, list):\n",
    "        for c in top:\n",
    "            if isinstance(c, dict):\n",
    "                collected.append(c)\n",
    "\n",
    "    # chunk-level citations (e.g., context)\n",
    "    chunks = obs.get(\"chunks\")\n",
    "    if isinstance(chunks, list):\n",
    "        for ch in chunks:\n",
    "            if not isinstance(ch, dict):\n",
    "                continue\n",
    "            ch_cites = ch.get(\"citations\")\n",
    "            if isinstance(ch_cites, list):\n",
    "                for c in ch_cites:\n",
    "                    if isinstance(c, dict):\n",
    "                        collected.append(c)\n",
    "\n",
    "    seen = set()\n",
    "    lines: List[str] = []\n",
    "    for c in collected:\n",
    "        title = (c.get(\"title\") or c.get(\"id\") or \"Source\").strip()\n",
    "        url   = (c.get(\"url\") or \"\").strip()\n",
    "        lic   = (c.get(\"license\") or \"—\").strip()\n",
    "        key = (url or title).lower()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        if url:\n",
    "            lines.append(f\"- [{title}]({url}) · {lic}\")\n",
    "        else:\n",
    "            lines.append(f\"- {title} · {lic}\")\n",
    "\n",
    "    return \"**Sources**\\n\" + \"\\n\".join(lines) if lines else \"\"\n",
    "\n",
    "\n",
    "def _maybe_render_image_from_obs(obs) -> ChatMessage | None:\n",
    "    if not isinstance(obs, dict):\n",
    "        return None\n",
    "    path = (\n",
    "        obs.get(\"image_path\")\n",
    "        or obs.get(\"path\")\n",
    "        or obs.get(\"file_path\")\n",
    "        or obs.get(\"local_path\")\n",
    "        or None\n",
    "    )\n",
    "    url = obs.get(\"image_url\") or obs.get(\"url\")\n",
    "    media = path or url\n",
    "    if not media:\n",
    "        return None\n",
    "    return ChatMessage(role=\"assistant\", content=gr.Image(value=media))\n",
    "\n",
    "\n",
    "# --- Async streaming handler for Gradio Chatbot(type=\"messages\") ---\n",
    "async def interact_with_langchain_agent(user_text, history):\n",
    "    \"\"\"\n",
    "    Streams a conversation turn:\n",
    "      - append user msg\n",
    "      - show ⏳ placeholder\n",
    "      - stream agent tool results (image + Sources only)\n",
    "      - stream final answer\n",
    "    Robust to exceptions; will surface errors in-chat and close cleanly.\n",
    "    \"\"\"\n",
    "    # 1) user message\n",
    "    history.append(ChatMessage(role=\"user\", content=user_text))\n",
    "    yield history\n",
    "\n",
    "    # 2) thinking placeholder\n",
    "    thinking_msg = ChatMessage(role=\"assistant\", content=\"⏳ Thinking...\")\n",
    "    history.append(thinking_msg)\n",
    "    yield history\n",
    "\n",
    "    try:\n",
    "        # 3) stream agent\n",
    "        async for chunk in agent_executor.astream({\"input\": user_text}):\n",
    "\n",
    "            # remove placeholder on first real activity\n",
    "            if thinking_msg in history:\n",
    "                try:\n",
    "                    history.remove(thinking_msg)\n",
    "                except ValueError:\n",
    "                    pass  # already removed elsewhere\n",
    "\n",
    "            # show ONLY user-facing results from tools\n",
    "            if \"steps\" in chunk:\n",
    "                for step in chunk[\"steps\"]:\n",
    "                    # DO NOT append step.action.log (keeps tool calls hidden)\n",
    "                    obs = getattr(step, \"observation\", None)\n",
    "                    if not isinstance(obs, dict):\n",
    "                        continue\n",
    "\n",
    "                    # image\n",
    "                    img_msg = _maybe_render_image_from_obs(obs)\n",
    "                    if img_msg is not None:\n",
    "                        history.append(img_msg)\n",
    "                        yield history\n",
    "\n",
    "                    # sources\n",
    "                    sources_md = _format_sources_md(obs)\n",
    "                    if sources_md:\n",
    "                        history.append(ChatMessage(role=\"assistant\", content=sources_md))\n",
    "                        yield history\n",
    "\n",
    "                # yield after processing this chunk\n",
    "                yield history\n",
    "\n",
    "            # final assistant output\n",
    "            if \"output\" in chunk:\n",
    "                history.append(ChatMessage(role=\"assistant\", content=chunk[\"output\"]))\n",
    "                yield history\n",
    "\n",
    "        # finished normally — ensure placeholder is gone\n",
    "        if thinking_msg in history:\n",
    "            try:\n",
    "                history.remove(thinking_msg)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        # surface the real error so you can see it instead of a vague aclose warning\n",
    "        if thinking_msg in history:\n",
    "            try:\n",
    "                history.remove(thinking_msg)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        history.append(ChatMessage(role=\"assistant\", content=f\"⚠️ Error: {e}\"))\n",
    "        yield history\n",
    "\n",
    "    finally:\n",
    "        # give Gradio a tick to close the async generator cleanly\n",
    "        await asyncio.sleep(0)\n",
    "        return\n",
    "\n",
    "\n",
    "# --- Gradio UI ---\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Beacon - Knowledge Agent (Village Bihar, India)\")\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        type=\"messages\",\n",
    "        label=\"Agent\",\n",
    "        avatar_images=(None, \"/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/avatar.png\"),\n",
    "        height=650,\n",
    "    )\n",
    "    textbox = gr.Textbox(lines=1, label=\"Chat Message\", placeholder=\"Ask something…\")\n",
    "\n",
    "    # Clear the textbox after submit so it feels chatty\n",
    "    def _clear_now(_msg, _chat):\n",
    "        return gr.update(value=\"\")\n",
    "\n",
    "    # streaming submit\n",
    "    textbox.submit(\n",
    "        interact_with_langchain_agent,\n",
    "        inputs=[textbox, chatbot],\n",
    "        outputs=[chatbot],\n",
    "    )\n",
    "\n",
    "    # instant clear\n",
    "    textbox.submit(\n",
    "        _clear_now,\n",
    "        inputs=[textbox, chatbot],   # must match the event’s inputs (2 args)\n",
    "        outputs=[textbox],\n",
    "        queue=False,\n",
    "    )\n",
    "\n",
    "demo.queue().launch()  # queue() is recommended for async callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a28f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e1ee5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46401930",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d076a8fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e3ba199",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
