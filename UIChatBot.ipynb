{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c03167",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "This notebook explores how to wrap the chatbot in a gradio chat interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a332a",
   "metadata": {},
   "source": [
    "###  Setting up paths and loading vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d166f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0) Imports & manifest ===\n",
    "from pathlib import Path\n",
    "import yaml, json\n",
    "from typing import Optional, List, Dict\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools import tool\n",
    "from langchain.schema import Document\n",
    "\n",
    "# If you want the prebuilt ReAct agent:\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# If you prefer to also expose tools directly on the LLM:\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# --- Set your pack root + manifest ---\n",
    "ROOT = Path(\"/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/first_aid_pack_demo_v2\")\n",
    "MANIFEST = ROOT / \"manifest.yaml\"\n",
    "\n",
    "with open(MANIFEST, \"r\", encoding=\"utf-8\") as f:\n",
    "    manifest = yaml.safe_load(f)\n",
    "\n",
    "# --- Resolve FAISS paths from manifest ---\n",
    "faiss_dir_text = ROOT / manifest[\"precomputed_indices\"][\"text\"][\"faiss\"][\"dir\"]\n",
    "faiss_dir_image = ROOT / manifest[\"precomputed_indices\"][\"images\"][\"faiss\"][\"dir\"]\n",
    "# --- Create embeddings *matching the store* ---\n",
    "embed_model_name = manifest[\"embedding_config\"][\"text\"][\"model\"]     #same for text and image\n",
    "emb = OllamaEmbeddings(model=embed_model_name)\n",
    "\n",
    "# --- Load FAISS store + retriever ---\n",
    "text_vs = FAISS.load_local(str(faiss_dir_text), emb, allow_dangerous_deserialization=True)\n",
    "text_retriever = text_vs.as_retriever(search_kwargs={\"k\": 4})  # default k; tool will override if provided\n",
    "\n",
    "image_vs = FAISS.load_local(str(faiss_dir_image), emb, allow_dangerous_deserialization=True)\n",
    "# results = image_vs.similarity_search_with_score(query, k=4)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93926748",
   "metadata": {},
   "source": [
    "### Defining helper and context() Tool  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18ea3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) Helpers ===\n",
    "def format_chunk(doc: Document, max_chars: int = 400) -> Dict:\n",
    "    \"\"\"Return a dict with compact text + key metadata for prompting & audit.\"\"\"\n",
    "    txt = doc.page_content.strip()\n",
    "    if len(txt) > max_chars:\n",
    "        txt = txt[:max_chars].rstrip() + \" …\"\n",
    "    m = doc.metadata\n",
    "    return {\n",
    "        \"id\": m.get(\"chunk_id\"),\n",
    "        \"topic_id\": m.get(\"topic_id\"),\n",
    "        \"file_id\": m.get(\"file_id\"),\n",
    "        \"locale\": m.get(\"locale\"),\n",
    "        \"path\": m.get(\"path\"),\n",
    "        \"citations\": [c.get(\"title\", \"\") for c in m.get(\"citations\", [])],\n",
    "        \"text\": txt\n",
    "    }\n",
    "\n",
    "def format_context_block(chunks: List[Dict]) -> str:\n",
    "    \"\"\"Human/LLM-friendly context block the agent can drop into its reasoning.\"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"### Retrieved Context (use only what is relevant)\")\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        cite_str = \"; \".join([t for t in c[\"citations\"] if t]) or \"—\"\n",
    "        head = f\"[{i}] {c['topic_id']} · {c['file_id']} · {c['locale']} · {c['path']}\"\n",
    "        lines.append(head)\n",
    "        lines.append(c[\"text\"])\n",
    "        lines.append(f\"Source(s): {cite_str}\")\n",
    "        lines.append(\"\")  # blank line\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "\n",
    "# === 2) The @tool: context() ===\n",
    "@tool\n",
    "def context(\n",
    "    query: str,\n",
    "    k: int = 4,\n",
    "    topic_id: Optional[str] = None,\n",
    "    locale: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve up to k relevant knowledge-pack chunks for 'query' and return a formatted\n",
    "    context block + structured per-chunk data for citations. You must use this tool for any prompt that is \n",
    "    important to wellbeing or safety of user. \n",
    "\n",
    "    Args:\n",
    "        query: Natural language question or keywords.\n",
    "        k: Top-k chunks to return (default 4).\n",
    "        topic_id: Optional manifest topic filter (e.g., 'bleed-control').\n",
    "        locale: Optional locale filter (e.g., 'hi_en' or 'en').\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"query\": str,\n",
    "          \"k\": int,\n",
    "          \"filters\": {\"topic_id\":..., \"locale\":...},\n",
    "          \"context_block\": str,     # pasteable into prompts\n",
    "          \"chunks\": [ {id, topic_id, file_id, path, locale, citations[], text}, ... ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Build a metadata filter if provided\n",
    "    _filter = {}\n",
    "    # if topic_id:\n",
    "    #     _filter[\"topic_id\"] = topic_id\n",
    "    # if locale:\n",
    "    #     _filter[\"locale\"] = locale\n",
    "\n",
    "    # Run retrieval (override k)\n",
    "    local_ret = text_vs.as_retriever(search_kwargs={\"k\": k})\n",
    "    hits: List[Document] = local_ret.invoke(query) if not _filter else local_ret.invoke(query, filter=_filter)\n",
    "\n",
    "    formatted = [format_chunk(d) for d in hits]\n",
    "    ctx_block = format_context_block(formatted)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"k\": k,\n",
    "        \"filters\": _filter,\n",
    "        \"context_block\": ctx_block,\n",
    "        \"chunks\": formatted\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b651",
   "metadata": {},
   "source": [
    "### Math Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbe93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract first number by second number.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a : float, b: float) -> float:\n",
    "    \"\"\"Divide first number by second number.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return a/b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69135961",
   "metadata": {},
   "source": [
    "### Defining knowledgeMeta() tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b002ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from typing import Optional\n",
    "\n",
    "@tool\n",
    "def knowledgeMeta(pack_dir: Optional[str] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Read a knowledge pack manifest and return metadata for trust and recency. \n",
    "\n",
    "    Args:\n",
    "      pack_dir: Absolute or relative path to the pack folder (containing manifest.yaml).\n",
    "                If omitted, uses the default ROOT pack path.\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"name\": str,\n",
    "        \"version\": str,\n",
    "        \"date\": str,\n",
    "        \"locales\": [..],\n",
    "        \"topics_count\": int,\n",
    "        \"manifest_path\": str\n",
    "      }\n",
    "    \"\"\"\n",
    "    # default to your earlier ROOT if not provided\n",
    "    base = Path(pack_dir) if pack_dir else ROOT\n",
    "    manifest_path = base / \"manifest.yaml\"\n",
    "    if not manifest_path.exists():\n",
    "        return {\"error\": f\"manifest.yaml not found at {manifest_path}\"}\n",
    "\n",
    "    with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        m = yaml.safe_load(f)\n",
    "\n",
    "    name = m.get(\"name\", str(base.name))\n",
    "    version = m.get(\"version\", \"unknown\")\n",
    "    date = m.get(\"date\", \"unknown\")\n",
    "    locales = m.get(\"locales\", [])\n",
    "    topics = m.get(\"index_of_topics\", []) or []\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"version\": version,\n",
    "        \"date\": date,\n",
    "        \"locales\": locales,\n",
    "        \"topics_count\": len(topics),\n",
    "        \"manifest_path\": str(manifest_path)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781c4fc",
   "metadata": {},
   "source": [
    "### getImage tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d162578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "HIGH_SCORE_THRESHOLD = 0.55  # tune as needed\n",
    "\n",
    "@tool\n",
    "def getImage(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve and display exactly one image for the user's query.\n",
    "    Applies a high-confidence threshold; returns NO_IMAGE if nothing clears the bar.\n",
    "    \"\"\"\n",
    "    q = (query or \"\").strip()\n",
    "    pack_name    = manifest.get(\"name\", \"\")\n",
    "    pack_ver     = manifest.get(\"version\", \"\")\n",
    "    pack_date    = manifest.get(\"date\", \"\")\n",
    "    pack_locales = manifest.get(\"locales\", [])\n",
    "\n",
    "    if not q:\n",
    "        return {\n",
    "            \"status\": \"NO_IMAGE\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": \"\",\n",
    "            \"score\": None,\n",
    "            \"citations\": []\n",
    "        }\n",
    "\n",
    "    # Use similarity_search_with_score to get confidence scores\n",
    "    foundImage = False\n",
    "    minScore = 0.32\n",
    "    finalDoc  = Document(page_content=\"\")\n",
    "    results = image_vs.similarity_search_with_score(query, k=4)\n",
    "    finScore = 0\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    for i, (d, score) in enumerate(results, 1):\n",
    "        \n",
    "        if score >= minScore:\n",
    "            finScore = score\n",
    "            finalDoc = d\n",
    "            foundImage = True\n",
    "            break\n",
    "    \n",
    "\n",
    "    if not foundImage:\n",
    "        print(\"NOT FOUND\")\n",
    "        return {\n",
    "            \"status\": \"NO_IMAGE\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": \"\",\n",
    "            \"score\": None,\n",
    "            \"citations\": []\n",
    "        }\n",
    "    else:\n",
    "        print(\"Found FOUND\")\n",
    "        img_path = ROOT / finalDoc.metadata['path']\n",
    "        # try:\n",
    "        #     display(Image(filename=img_path))\n",
    "        # except Exception:\n",
    "        #     pass\n",
    "            \n",
    "        return {\n",
    "            \"status\": \"OK\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": str(img_path),\n",
    "            \"score\": float(finScore),\n",
    "            \"citations\": finalDoc.metadata.get(\"citations\", [])\n",
    "        }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad48bd",
   "metadata": {},
   "source": [
    "### List of Available Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c9825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a Tools list for whichever orchestration you choose:\n",
    "TOOLS = [context,add,multiply,subtract,divide,knowledgeMeta, getImage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22fb2c",
   "metadata": {},
   "source": [
    "### Setting Up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a99997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=\"ollama:gpt-oss:20b\",       \n",
    "    temperature=0.2  # lower = more deterministic\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(TOOLS) #llm_with_tools is a new wrapped llm\n",
    "model = llm_with_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d161dfa",
   "metadata": {},
   "source": [
    "### Testing Tool Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e467a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import HumanMessage\n",
    "# #message classes on lang chain inlcude human massage ,ai message, system message, and tool message\n",
    "# query = \"call get image to show torniquet on leg\"\n",
    "\n",
    "# messages = [HumanMessage(query)]\n",
    "\n",
    "# ai_msg = llm_with_tools.invoke(messages) #llm_with_tools looks at history(currently only 1 human message) and then builds prompt\n",
    "# print(\"AI MESSAEGE CALLS\")\n",
    "# print(ai_msg)\n",
    "# print(\"JUST AI TOOL CALLS\")\n",
    "# print(ai_msg.tool_calls) \n",
    "# #Now we add the bots message to the chat history\n",
    "# messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3d18ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tool_call in ai_msg.tool_calls: #Actually running all the tool calls ai requested in last cell\n",
    "#     tools_dict = {\"context\": context, \"knowledgeMeta\":knowledgeMeta,\"add\":add,\n",
    "#             \"subtract\":subtract,\"divide\":divide,\"multiply\":multiply, \"getImage\":getImage}\n",
    "#     selected_tool = tools_dict[tool_call[\"name\"]]\n",
    "#     tool_msg = selected_tool.invoke(tool_call)\n",
    "#     messages.append(tool_msg) #Add the tool message to chat history\n",
    "\n",
    "# messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0bf28",
   "metadata": {},
   "source": [
    "### Reason - Act Agent Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "450b94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Annotated, #Extra metadata\n",
    "    Sequence, #generic list container\n",
    "    TypedDict, #Lets you define a dictionary type with fixed keys and types\n",
    ") #a dictionary type has a specific format of what key and valye types can be \n",
    "from langchain_core.messages import BaseMessage #lang chain message format (Human,Systen,AI)\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    # add_messages is a reducer\n",
    "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "#define a state dictionary, 1 key names messages\n",
    "#messages is a list of langchain base messages\n",
    "#wrapping it in Annotated tells it to not replace entire list but use add_messages to merge it in\n",
    "#The accepected value  for this dictionary will be Sequence[BaseMessage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07e24e",
   "metadata": {},
   "source": [
    "### Defining nodes and edges in \"Lang graph\"\n",
    "\n",
    "Lang graph:\n",
    "- Imagine you're in a big construction truck and you have a map of stops\n",
    "- Each stop pick up something/do a \n",
    "- Folow the street signs to get to all the stops\n",
    "- Take a photo to save state during important jobs\n",
    "\n",
    "The big truck carries chat histroy. The road signs are edges between nodes where nodes are the stops. The photos are state changes you can reload to(checkpoint in video game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1e467c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "tools_dict = {\"context\": context, \"knowledgeMeta\":knowledgeMeta,\"add\":add,\n",
    "            \"subtract\":subtract,\"divide\":divide,\"multiply\":multiply, \"getImage\":getImage}\n",
    "\n",
    "tools_by_name = tools_dict\n",
    "\n",
    "\n",
    "# Define our tool node\n",
    "def tool_node(state: AgentState): #Agent State is the conversation history\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls: #Look at the last message in history(should be AIMessage)\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"]) #Actually call the tool\n",
    "        outputs.append( #Add a json output to conco hsitory\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "# Define the node that calls the model\n",
    "def call_model(\n",
    "    state: AgentState,\n",
    "    config: RunnableConfig,\n",
    "):\n",
    "    temp_system_str = \"For ANY health or safety query, you MUST call the `context` tool.  Do not answer from memory.\"\n",
    "    # this is similar to customizing the create_react_agent with 'prompt' parameter, but is more flexible\n",
    "    system_prompt = SystemMessage(\n",
    "        \"You are a helpful AI assistant, please respond to the users query to the best of your ability!\" + temp_system_str\n",
    "    )\n",
    "    response = model.invoke([system_prompt] + state[\"messages\"], config)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the conditional edge that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745cd75",
   "metadata": {},
   "source": [
    "### Defining new Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b94e222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'Exception'>\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"tools\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Now we can compile and visualize our graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is \n",
    "    print(Exception)\n",
    "    pass\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47485d2b",
   "metadata": {},
   "source": [
    "### Print output stream function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dcb80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting the stream nicely\n",
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        prev_msg = \"\"\n",
    "        if len(s['messages']) > 1:\n",
    "            prev_msg = s[\"messages\"][-2]\n",
    "\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "        # if prev_msg != \"\":\n",
    "        #     if isinstance(prev_msg, tuple):\n",
    "        #         print(prev_msg)\n",
    "        #     else:\n",
    "        #         prev_msg.pretty_print()\n",
    "\n",
    "\n",
    "# inputs = {\"messages\": [(\"user\", \"What to do if bleed? When Knowledge updated? What 42*321 and 32/31?\")]}\n",
    "# print_stream(graph.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cd475",
   "metadata": {},
   "source": [
    "### Infinte Loop Chat Bot\n",
    "\n",
    "After eahc exit please restart kernel and clear all outputs to ensure a fresh conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f493ab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a message. Commands: /reset, /exit\n",
      "\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "# === Simple REPL loop for Jupyter ===\n",
    "def chat_loop():\n",
    "    history = []\n",
    "    print(\"Type a message. Commands: /reset, /exit\\n\")\n",
    "    while True:\n",
    "        user = input(\"You: \").strip()\n",
    "        if not user:\n",
    "            continue\n",
    "        if user.lower() in {\"/exit\", \"/quit\"}:\n",
    "            print(\"Bye!\")\n",
    "            break\n",
    "        if user.lower() == \"/reset\":\n",
    "            history = []\n",
    "            print(\"(conversation cleared)\")\n",
    "            continue\n",
    "\n",
    "        # Append user turn\n",
    "        inputs = {\"messages\": history + [(\"user\", user)]}\n",
    "\n",
    "        # Use YOUR print_stream exactly as written\n",
    "        print_stream(graph.stream(inputs, stream_mode=\"values\"))\n",
    "\n",
    "        # Capture updated history so context persists\n",
    "        final_state = graph.invoke(inputs)\n",
    "        history = final_state[\"messages\"]\n",
    "\n",
    "# Run this cell in your notebook to start chatting\n",
    "chat_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b2695b",
   "metadata": {},
   "source": [
    "### Using gradio with langchain\n",
    "\n",
    "\n",
    "\n",
    "This is a simple general-purpose chatbot built on top of LangChain and Gradio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb3340ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage  \n",
    "import gradio as gr\n",
    "\n",
    "lightModel = init_chat_model(\n",
    "    model=\"ollama:llama3.1\",       \n",
    "    temperature=0.2  # lower = more deterministic\n",
    ")\n",
    "\n",
    "def predict(message, history):\n",
    "    history_langchain_format = []\n",
    "    for msg in history:\n",
    "        if msg['role'] == \"user\":\n",
    "            history_langchain_format.append(HumanMessage(content=msg['content']))\n",
    "        elif msg['role'] == \"assistant\":\n",
    "            history_langchain_format.append(AIMessage(content=msg['content']))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "    llm_response = lightModel.invoke(history_langchain_format)\n",
    "    return llm_response.content\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    predict,\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f2da58",
   "metadata": {},
   "source": [
    "### Gradio and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbff1846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(\n",
    "        type=\"messages\",\n",
    "        value=[\n",
    "            gr.ChatMessage(\n",
    "                role=\"user\", \n",
    "                content=\"What is the weather in San Francisco?\"\n",
    "            ),\n",
    "            gr.ChatMessage(\n",
    "                role=\"assistant\", \n",
    "                content=\"I need to use the weather API tool?\",\n",
    "                metadata={\"title\":  \"🧠 Thinking\"}\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24711716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1ce7e4",
   "metadata": {},
   "source": [
    "### Working UI with tool calling and sources\n",
    "\n",
    "Images is next "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ded796d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from gradio import ChatMessage\n",
    "import gradio as gr\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "lightModel = init_chat_model(\n",
    "    model=\"ollama:llama3.1\",       \n",
    "    temperature=0.2, # lower = more deterministic\n",
    "    streaming = True  \n",
    ")\n",
    "\n",
    "heavyModel = init_chat_model(\n",
    "    model=\"ollama:gpt-oss:20b\",       \n",
    "    temperature=0.2  # lower = more deterministic\n",
    ")\n",
    "\n",
    "tools = TOOLS\n",
    "\n",
    "\n",
    "from langchain.tools.render import render_text_description\n",
    "\n",
    "# 1) Render tool descriptions and names\n",
    "tool_desc = render_text_description(TOOLS)         # Render the tool name and description in plain text.\n",
    "tool_names = \", \".join([t.name for t in TOOLS])    # exact callable names\n",
    "\n",
    "# 2) Build the prompt and PARTIAL the missing variables\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"You are a helpful assistant. \n",
    "You can use the following tools if they are useful:\n",
    "{tools}\n",
    "\n",
    "Always call tools by their exact name from: {tool_names}.\n",
    "If no tool is needed, just answer directly.\n",
    "\"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "]).partial(tools=tool_desc, tool_names=tool_names)\n",
    "\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=heavyModel,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,        # helpful while wiring things up\n",
    ").with_config({\"run_name\": \"Agent\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b2943",
   "metadata": {},
   "source": [
    "# OLD UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b80f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _format_sources_from_context_obs(obs: dict) -> str:\n",
    "    # obs is `context` tool return;  it is a dict.\n",
    "    chunks = obs.get(\"chunks\", []) if isinstance(obs, dict) else []\n",
    "    if not chunks:\n",
    "        return \"\"\n",
    "    lines = []\n",
    "    for ch in chunks:\n",
    "        file_id = ch.get(\"file_id\", \"unknown\")\n",
    "        path = ch.get(\"path\", \"\")\n",
    "        cites = ch.get(\"citations\", [])\n",
    "        cite_str = \", \".join(cites) if cites else \"—\"\n",
    "        lines.append(f\"- `{file_id}` · {path} — {cite_str}\")\n",
    "    return \"**Sources**\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def _maybe_render_image_from_obs(obs) -> ChatMessage | None:\n",
    "    \"\"\"\n",
    "    Accepts a tool observation dict and returns a ChatMessage with an image\n",
    "    if it finds a usable image path/url. Otherwise returns None.\n",
    "    \"\"\"\n",
    "    if not isinstance(obs, dict):\n",
    "        return None\n",
    "\n",
    "    # Common keys your chain/logs might produce\n",
    "    path = (\n",
    "        obs.get(\"image_path\")\n",
    "        or obs.get(\"path\")\n",
    "        or obs.get(\"file_path\")\n",
    "        or obs.get(\"local_path\")\n",
    "        or None\n",
    "    )\n",
    "    url = obs.get(\"image_url\") or obs.get(\"url\")\n",
    "\n",
    "    # Prefer local path if available; Gradio will serve files inside the app folder.\n",
    "    media = path or url\n",
    "    if not media:\n",
    "        return None\n",
    "\n",
    "    # Option A: return a component (nice rendering)\n",
    "    return ChatMessage(role=\"assistant\", content=gr.Image(value=media))\n",
    "\n",
    "    # Option B (alternative): return a file dict that Gradio renders as an image\n",
    "    # return ChatMessage(role=\"assistant\", content={\"path\": media})\n",
    "\n",
    "# --- Async streaming handler for Gradio Chatbot(type=\"messages\") ---\n",
    "async def interact_with_langchain_agent(user_text, history):\n",
    "    # history is a list[ChatMessage]\n",
    "    history.append(ChatMessage(role=\"user\", content=user_text))\n",
    "    yield history\n",
    "\n",
    "    # Stream agent steps & final output\n",
    "    async for chunk in agent_executor.astream({\"input\": user_text}):\n",
    "        if \"steps\" in chunk:\n",
    "            for step in chunk[\"steps\"]:\n",
    "                history.append(ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=step.action.log,\n",
    "                    metadata={\"title\": f\"🛠️ Used tool {step.action.tool}\"}\n",
    "                ))\n",
    "\n",
    "\n",
    "                # 2) If we have an observation/result for this step, show it.\n",
    "                #    LangChain includes it on the same step object in many runners;\n",
    "                #    otherwise it may appear on a subsequent step — this handles both.\n",
    "                obs = getattr(step, \"observation\", None)\n",
    "                if obs is not None:\n",
    "\n",
    "                    # NEW: try rendering an image if present\n",
    "                    img_msg = _maybe_render_image_from_obs(obs)\n",
    "                    if img_msg is not None:\n",
    "                        history.append(img_msg)\n",
    "                        yield history\n",
    "                    # If it's the `context` tool, render sources from its dict payload\n",
    "                    if step.action.tool == \"context\":\n",
    "                        sources_md = _format_sources_from_context_obs(obs)\n",
    "                        if sources_md:\n",
    "                            history.append(ChatMessage(role=\"assistant\", content=sources_md))\n",
    "                            yield history\n",
    "                    else:\n",
    "                        # For other tools you can also render their observations if you want\n",
    "                        pass\n",
    "\n",
    "\n",
    "                yield history\n",
    "        if \"output\" in chunk:\n",
    "            history.append(ChatMessage(role=\"assistant\", content=chunk[\"output\"]))\n",
    "            yield history\n",
    "\n",
    "\n",
    "# --- Gradio UI ---\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Beacon - Offline Knolwedge Agent(Village First Aid Demo)\")\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        type=\"messages\",\n",
    "        label=\"Agent\",\n",
    "        avatar_images=(None, \"https://image.pngaaa.com/10/5148010-middle.png\"),\n",
    "        height=400,\n",
    "    )\n",
    "    textbox = gr.Textbox(lines=1, label=\"Chat Message\", placeholder=\"Ask something…\")\n",
    "    # Clear the textbox after submit so it feels chatty\n",
    "    def _clear():\n",
    "        return gr.update(value=\"\")\n",
    "\n",
    "    # Submit: wire the *actual* components you created\n",
    "    textbox.submit(\n",
    "        interact_with_langchain_agent,\n",
    "        inputs=[textbox, chatbot],     # (prompt, messages)\n",
    "        outputs=[chatbot]              # updates the Chatbot\n",
    "    ).then(\n",
    "        _clear, None, [textbox]\n",
    "    )\n",
    "\n",
    "demo.queue().launch()   # queue() is recommended for async callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48f77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a6aaba0",
   "metadata": {},
   "source": [
    "# Curr UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99779dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new Agent chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mIf you’re experiencing heavy bleeding, it’s important to seek medical help right away. Call emergency services (for example, 911 in the U.S.) or go to the nearest emergency department. If you’re unable to call, ask someone nearby to help you get urgent care. Stay calm, keep the area clean, and avoid any activities that could worsen the bleeding until you can be evaluated by a healthcare professional.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new Agent chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `context` with `{'k': 4, 'locale': 'en', 'query': 'bleeding first aid', 'topic_id': None}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'bleeding first aid', 'k': 4, 'filters': {}, 'context_block': '### Retrieved Context (use only what is relevant)\\n[1] bleed-control · guide-bleed-overview · hi_en · core/bleed-control/hi_en/bleeding_control_overview.md\\n# Severe Bleeding Control\\nSevere bleeding can quickly become life-threatening if not controlled.  \\nApply firm direct pressure with a clean cloth or sterile gauze.  \\nIf bleeding soaks through, add more cloths without removing the first.  \\nElevate the injured limb if possible while maintaining pressure.  \\nUse a tourniquet if direct pressure fails and bleeding is from an arm or leg.  \\nNote the time a …\\nSource(s): WHO Basic Emergency Care (B.E.C.)\\n\\n[2] flood-wounds · guide-flood-infection · hi_en · core/flood-wounds/hi_en/flood_infection_signs.md\\n# Flood-Related Wounds & Infection\\nFlood water often carries dirt, sewage, and harmful bacteria.  \\nClean all cuts and wounds immediately with clean running water and soap.  \\nApply antiseptic if available and cover with a clean dressing.  \\nChange dressings daily or if they become wet or dirty.  \\nWatch for signs of infection: redness, swelling, pus, fever.  \\nEnsure tetanus vaccination is up to date …\\nSource(s): ICMR – Leptospirosis Guidelines\\n\\n[3] maternal · guide-maternal-signs · hi_en · core/maternal/hi_en/maternal_danger_signs.md\\n# Maternal Danger Signs\\nPregnancy complications can develop suddenly and be life-threatening.  \\nRed flags include severe bleeding, convulsions, and unconsciousness.  \\nOther danger signs: severe headache, blurred vision, swelling of face/hands.  \\nProlonged labor beyond 12 hours in first pregnancy is a risk.  \\nFoul-smelling discharge or high fever may indicate infection.  \\nEncourage women to seek sk …\\nSource(s): WHO Basic Emergency Care (B.E.C.)\\n\\n[4] cpr · guide-cpr-steps · hi_en · core/cpr/hi_en/cpr_steps.md\\n# Hands-Only CPR\\nCardiac arrest means the heart has stopped pumping effectively.  \\nCheck for responsiveness and breathing; call for help immediately.  \\nPlace the heel of one hand on the center of the chest, other hand on top.  \\nPush hard and fast at 100–120 compressions per minute.  \\nAllow full chest recoil after each push.  \\nDo not stop until help arrives or the person moves.  \\nHands-only CPR is …\\nSource(s): WHO Basic Emergency Care (B.E.C.)', 'chunks': [{'id': 'guide-bleed-overview::chunk::0', 'topic_id': 'bleed-control', 'file_id': 'guide-bleed-overview', 'locale': 'hi_en', 'path': 'core/bleed-control/hi_en/bleeding_control_overview.md', 'citations': ['WHO Basic Emergency Care (B.E.C.)'], 'text': '# Severe Bleeding Control\\nSevere bleeding can quickly become life-threatening if not controlled.  \\nApply firm direct pressure with a clean cloth or sterile gauze.  \\nIf bleeding soaks through, add more cloths without removing the first.  \\nElevate the injured limb if possible while maintaining pressure.  \\nUse a tourniquet if direct pressure fails and bleeding is from an arm or leg.  \\nNote the time a …'}, {'id': 'guide-flood-infection::chunk::0', 'topic_id': 'flood-wounds', 'file_id': 'guide-flood-infection', 'locale': 'hi_en', 'path': 'core/flood-wounds/hi_en/flood_infection_signs.md', 'citations': ['ICMR – Leptospirosis Guidelines'], 'text': '# Flood-Related Wounds & Infection\\nFlood water often carries dirt, sewage, and harmful bacteria.  \\nClean all cuts and wounds immediately with clean running water and soap.  \\nApply antiseptic if available and cover with a clean dressing.  \\nChange dressings daily or if they become wet or dirty.  \\nWatch for signs of infection: redness, swelling, pus, fever.  \\nEnsure tetanus vaccination is up to date …'}, {'id': 'guide-maternal-signs::chunk::0', 'topic_id': 'maternal', 'file_id': 'guide-maternal-signs', 'locale': 'hi_en', 'path': 'core/maternal/hi_en/maternal_danger_signs.md', 'citations': ['WHO Basic Emergency Care (B.E.C.)'], 'text': '# Maternal Danger Signs\\nPregnancy complications can develop suddenly and be life-threatening.  \\nRed flags include severe bleeding, convulsions, and unconsciousness.  \\nOther danger signs: severe headache, blurred vision, swelling of face/hands.  \\nProlonged labor beyond 12 hours in first pregnancy is a risk.  \\nFoul-smelling discharge or high fever may indicate infection.  \\nEncourage women to seek sk …'}, {'id': 'guide-cpr-steps::chunk::0', 'topic_id': 'cpr', 'file_id': 'guide-cpr-steps', 'locale': 'hi_en', 'path': 'core/cpr/hi_en/cpr_steps.md', 'citations': ['WHO Basic Emergency Care (B.E.C.)'], 'text': '# Hands-Only CPR\\nCardiac arrest means the heart has stopped pumping effectively.  \\nCheck for responsiveness and breathing; call for help immediately.  \\nPlace the heel of one hand on the center of the chest, other hand on top.  \\nPush hard and fast at 100–120 compressions per minute.  \\nAllow full chest recoil after each push.  \\nDo not stop until help arrives or the person moves.  \\nHands-only CPR is …'}]}\u001b[0m\u001b[32;1m\u001b[1;3m**If you or someone else is bleeding, act quickly and calmly.**  \n",
      "Below are the essential first‑aid steps for most types of bleeding.  Follow them until help arrives or the bleeding stops.\n",
      "\n",
      "| Situation | Immediate Actions | What to Do Next |\n",
      "|-----------|-------------------|-----------------|\n",
      "| **External bleeding (cut, scrape, or wound)** | 1. **Apply firm, direct pressure** with a clean cloth, gauze, or your hand. <br>2. If the cloth soaks through, **add more layers** without removing the first. <br>3. **Elevate** the injured limb above heart level if possible (helps reduce blood flow). | • Keep the pressure steady for at least 5–10 minutes. <br>• If bleeding continues, consider a **tourniquet** (only for severe limb bleeding that cannot be controlled by pressure). <br>• Call emergency services (112/911) if bleeding is heavy or does not stop. |\n",
      "| **Internal or severe bleeding (e.g., from a deep wound, puncture, or organ injury)** | 1. **Call emergency services immediately**. <br>2. **Lay the person flat** and keep them still. <br>3. If you can, **apply pressure** to the wound or use a tourniquet if it’s a limb. | • Do **not** give food, drink, or medication. <br>• Monitor breathing and consciousness; be ready to perform CPR if needed. |\n",
      "| **Bleeding from a head or face wound** | 1. **Sit the person up** and keep them calm. <br>2. **Apply pressure** to the wound with a clean cloth. <br>3. **Avoid** touching the wound with dirty hands. | • If the bleeding is heavy, use a **tourniquet** only if it’s a limb; otherwise, keep pressure. <br>• Seek immediate medical help. |\n",
      "| **Bleeding after a fall or accident** | 1. **Check for other injuries** (broken bones, head trauma). <br>2. **Apply pressure** to any visible wounds. <br>3. **Call for help** if the bleeding is significant or if the person is unconscious. | • Keep the person warm and still. <br>• Do not move them unless absolutely necessary. |\n",
      "\n",
      "### Key Points to Remember\n",
      "\n",
      "1. **Direct pressure is the most effective first‑aid technique** for most external bleeding.  \n",
      "2. **Elevate** the injured area when possible to reduce blood flow.  \n",
      "3. **Tourniquets** should only be used when direct pressure fails and the bleeding is from a limb.  \n",
      "4. **Call emergency services** (112/911) if the bleeding is heavy, does not stop, or if the person shows signs of shock (pale, clammy skin, rapid breathing, fainting).  \n",
      "5. **Do not remove** any dressing that is already on the wound; add more layers if needed.  \n",
      "6. **Keep the person calm** and monitor breathing and consciousness until help arrives.\n",
      "\n",
      "> **Sources**  \n",
      "> • WHO Basic Emergency Care (B.E.C.) – Severe Bleeding Control guidelines.  \n",
      "> • WHO Basic Emergency Care (B.E.C.) – General first‑aid recommendations.  \n",
      "\n",
      "If you’re unsure about the severity of the bleeding or if the person’s condition changes, **do not hesitate to seek professional medical help immediately.**\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new Agent chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `getImage` with `{'query': 'tourniquet'}`\n",
      "\n",
      "\n",
      "\u001b[0mFound FOUND\n",
      "\u001b[36;1m\u001b[1;3m{'status': 'OK', 'version': '0.3.1', 'date': '2025-08-23', 'locales': ['hi', 'en'], 'pack_name': 'Eastern Bihar – First Aid & Community Health | पूर्वी बिहार – प्राथमिक चिकित्सा', 'image_path': '/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/first_aid_pack_demo_v2/assets/bleeding/tourniquet.png', 'score': 0.5874264240264893, 'citations': [{'id': 'journal-trauma', 'title': 'Journal of Trauma and Acute Care Surgery - DOI:10.1097/TA.0000000000000485', 'url': 'https://www.researchgate.net/publication/270003434_Improvised_tourniquets_Obsolete_or_obligatory', 'license': 'UNKNOWN IMAGE LICENSE'}]}\u001b[0m\u001b[32;1m\u001b[1;3mHere’s an image of a tourniquet:\n",
      "\n",
      "![Tourniquet](attachment://tourniquet.png)\n",
      "\n",
      "A tourniquet is a device used to apply pressure to a limb to control severe bleeding. It’s typically wrapped around the arm or leg and tightened until the blood flow is stopped. Use it only as a last resort and follow proper first‑aid guidelines.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new Agent chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `getImage` with `{'query': 'tourniquet'}`\n",
      "\n",
      "\n",
      "\u001b[0mFound FOUND\n",
      "\u001b[36;1m\u001b[1;3m{'status': 'OK', 'version': '0.3.1', 'date': '2025-08-23', 'locales': ['hi', 'en'], 'pack_name': 'Eastern Bihar – First Aid & Community Health | पूर्वी बिहार – प्राथमिक चिकित्सा', 'image_path': '/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/first_aid_pack_demo_v2/assets/bleeding/tourniquet.png', 'score': 0.5874264240264893, 'citations': [{'id': 'journal-trauma', 'title': 'Journal of Trauma and Acute Care Surgery - DOI:10.1097/TA.0000000000000485', 'url': 'https://www.researchgate.net/publication/270003434_Improvised_tourniquets_Obsolete_or_obligatory', 'license': 'UNKNOWN IMAGE LICENSE'}]}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `context` with `{'k': 4, 'locale': 'en', 'query': 'bleeding control tourniquet first aid', 'topic_id': 'bleed-control'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'bleeding control tourniquet first aid', 'k': 4, 'filters': {}, 'context_block': '### Retrieved Context (use only what is relevant)\\n[1] bleed-control · guide-bleed-overview · hi_en · core/bleed-control/hi_en/bleeding_control_overview.md\\n# Severe Bleeding Control\\nSevere bleeding can quickly become life-threatening if not controlled.  \\nApply firm direct pressure with a clean cloth or sterile gauze.  \\nIf bleeding soaks through, add more cloths without removing the first.  \\nElevate the injured limb if possible while maintaining pressure.  \\nUse a tourniquet if direct pressure fails and bleeding is from an arm or leg.  \\nNote the time a …\\nSource(s): WHO Basic Emergency Care (B.E.C.)\\n\\n[2] snakebite · guide-snakebite-donts · hi_en · core/snakebite/hi_en/snakebite_dos_and_donts.md\\n# Snakebite\\nKeep the patient calm and lying still to slow the spread of venom.  \\nRemove rings, bangles, or tight clothing from the bitten limb.  \\nImmobilize the limb with a splint and keep it at heart level.  \\nDo not cut, suck, burn, or apply ice to the bite.  \\nDo not apply tight tourniquets; they can cause more harm.  \\nTransport the patient quickly and safely to the nearest hospital with antiveno …\\nSource(s): WHO SEARO – Snakebite Management\\n\\n[3] flood-wounds · guide-flood-infection · hi_en · core/flood-wounds/hi_en/flood_infection_signs.md\\n# Flood-Related Wounds & Infection\\nFlood water often carries dirt, sewage, and harmful bacteria.  \\nClean all cuts and wounds immediately with clean running water and soap.  \\nApply antiseptic if available and cover with a clean dressing.  \\nChange dressings daily or if they become wet or dirty.  \\nWatch for signs of infection: redness, swelling, pus, fever.  \\nEnsure tetanus vaccination is up to date …\\nSource(s): ICMR – Leptospirosis Guidelines\\n\\n[4] cpr · guide-cpr-steps · hi_en · core/cpr/hi_en/cpr_steps.md\\n# Hands-Only CPR\\nCardiac arrest means the heart has stopped pumping effectively.  \\nCheck for responsiveness and breathing; call for help immediately.  \\nPlace the heel of one hand on the center of the chest, other hand on top.  \\nPush hard and fast at 100–120 compressions per minute.  \\nAllow full chest recoil after each push.  \\nDo not stop until help arrives or the person moves.  \\nHands-only CPR is …\\nSource(s): WHO Basic Emergency Care (B.E.C.)', 'chunks': [{'id': 'guide-bleed-overview::chunk::0', 'topic_id': 'bleed-control', 'file_id': 'guide-bleed-overview', 'locale': 'hi_en', 'path': 'core/bleed-control/hi_en/bleeding_control_overview.md', 'citations': ['WHO Basic Emergency Care (B.E.C.)'], 'text': '# Severe Bleeding Control\\nSevere bleeding can quickly become life-threatening if not controlled.  \\nApply firm direct pressure with a clean cloth or sterile gauze.  \\nIf bleeding soaks through, add more cloths without removing the first.  \\nElevate the injured limb if possible while maintaining pressure.  \\nUse a tourniquet if direct pressure fails and bleeding is from an arm or leg.  \\nNote the time a …'}, {'id': 'guide-snakebite-donts::chunk::0', 'topic_id': 'snakebite', 'file_id': 'guide-snakebite-donts', 'locale': 'hi_en', 'path': 'core/snakebite/hi_en/snakebite_dos_and_donts.md', 'citations': ['WHO SEARO – Snakebite Management'], 'text': '# Snakebite\\nKeep the patient calm and lying still to slow the spread of venom.  \\nRemove rings, bangles, or tight clothing from the bitten limb.  \\nImmobilize the limb with a splint and keep it at heart level.  \\nDo not cut, suck, burn, or apply ice to the bite.  \\nDo not apply tight tourniquets; they can cause more harm.  \\nTransport the patient quickly and safely to the nearest hospital with antiveno …'}, {'id': 'guide-flood-infection::chunk::0', 'topic_id': 'flood-wounds', 'file_id': 'guide-flood-infection', 'locale': 'hi_en', 'path': 'core/flood-wounds/hi_en/flood_infection_signs.md', 'citations': ['ICMR – Leptospirosis Guidelines'], 'text': '# Flood-Related Wounds & Infection\\nFlood water often carries dirt, sewage, and harmful bacteria.  \\nClean all cuts and wounds immediately with clean running water and soap.  \\nApply antiseptic if available and cover with a clean dressing.  \\nChange dressings daily or if they become wet or dirty.  \\nWatch for signs of infection: redness, swelling, pus, fever.  \\nEnsure tetanus vaccination is up to date …'}, {'id': 'guide-cpr-steps::chunk::0', 'topic_id': 'cpr', 'file_id': 'guide-cpr-steps', 'locale': 'hi_en', 'path': 'core/cpr/hi_en/cpr_steps.md', 'citations': ['WHO Basic Emergency Care (B.E.C.)'], 'text': '# Hands-Only CPR\\nCardiac arrest means the heart has stopped pumping effectively.  \\nCheck for responsiveness and breathing; call for help immediately.  \\nPlace the heel of one hand on the center of the chest, other hand on top.  \\nPush hard and fast at 100–120 compressions per minute.  \\nAllow full chest recoil after each push.  \\nDo not stop until help arrives or the person moves.  \\nHands-only CPR is …'}]}\u001b[0m\u001b[32;1m\u001b[1;3m**If you’re dealing with severe bleeding, act quickly and follow these steps:**\n",
      "\n",
      "| Step | What to Do | Why it matters |\n",
      "|------|------------|----------------|\n",
      "| 1. **Call for help** | If you’re alone, call emergency services (or have someone else do it). | Professional help is essential for life‑saving care. |\n",
      "| 2. **Wear gloves (if available)** | Protect yourself from bloodborne pathogens. | Reduces risk of infection. |\n",
      "| 3. **Apply firm direct pressure** | Use a clean cloth, gauze, or your hand. Keep pressure steady. | Stops bleeding by compressing the wound. |\n",
      "| 4. **Add more layers if needed** | If the first cloth soaks, add another without removing the first. | Maintains pressure and keeps the wound covered. |\n",
      "| 5. **Elevate the limb (if possible)** | Raise the injured arm or leg above heart level. | Reduces blood flow to the area. |\n",
      "| 6. **Use a tourniquet only if direct pressure fails** | Place it 2–3 cm above the wound (toward the body). Tighten until bleeding stops. | Stops arterial flow when other measures fail. |\n",
      "| 7. **Time the tourniquet** | Note the time it was applied. | Helps medical staff assess how long blood has been blocked. |\n",
      "| 8. **Do not remove the tourniquet** | Keep it in place until professionals arrive. | Prevents re‑bleeding. |\n",
      "| 9. **Keep the patient calm and still** | Movement can worsen bleeding. | Reduces blood loss and shock. |\n",
      "| 10. **Transport to a medical facility** | Even if bleeding stops, the patient needs evaluation. | Prevents complications such as shock or infection. |\n",
      "\n",
      "---\n",
      "\n",
      "### How to Apply a Tourniquet\n",
      "\n",
      "1. **Locate the wound** – Identify the artery (usually the upper arm or thigh).  \n",
      "2. **Position the tourniquet** – Place it 2–3 cm above the wound, toward the body.  \n",
      "3. **Tighten** – Use the strap or handle to apply firm pressure until the bleeding stops.  \n",
      "4. **Secure** – Lock the strap or fasten the handle.  \n",
      "5. **Mark the time** – Write the time on the tourniquet or on a piece of paper.  \n",
      "6. **Keep it in place** – Do not loosen or remove until medical help arrives.\n",
      "\n",
      "> **Citations**  \n",
      "> • WHO Basic Emergency Care (B.E.C.) – Severe Bleeding Control  \n",
      "> • WHO SEARO – Snakebite Management (note: do *not* use a tourniquet for snakebite)  \n",
      "\n",
      "---\n",
      "\n",
      "#### Tourniquet Image\n",
      "\n",
      "![Tourniquet](file:///Users/ktejwani/Personal%20CS%20Projects/Summer%202025/Offline%20AI%20Kiosk/Offline-AI-Kiosk/first_aid_pack_demo_v2/assets/bleeding/tourniquet.png)\n",
      "\n",
      "*Image source: Journal of Trauma and Acute Care Surgery – “Improvised tourniquets: obsolete or obligatory” (license unknown).*\n",
      "\n",
      "---\n",
      "\n",
      "**Quick tip:** If you’re in a rural setting and don’t have a commercial tourniquet, you can improvise with a sturdy belt, a piece of cloth, or a strip of fabric. The key is to apply enough pressure to stop arterial flow and keep it tight until help arrives.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def _format_sources_md(obs: dict, tool_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a single Markdown block with sources/citations for either:\n",
    "      - getImage-style obs: { citations: [{title,id,url,license}] }\n",
    "      - context-style obs: { chunks: [{file_id,path,url,license,citations:[...]}] }\n",
    "    \"\"\"\n",
    "    if not isinstance(obs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    # A) Direct citations (e.g., from getImage)\n",
    "    cites = obs.get(\"citations\", [])\n",
    "    if isinstance(cites, (list, tuple)):\n",
    "        for c in cites:\n",
    "            if not isinstance(c, dict):\n",
    "                continue\n",
    "            title = c.get(\"title\") or c.get(\"id\") or \"Source\"\n",
    "            url = c.get(\"url\")\n",
    "            lic = c.get(\"license\") or \"—\"\n",
    "            if url:\n",
    "                lines.append(f\"- [{title}]({url}) · {lic}\")\n",
    "            else:\n",
    "                lines.append(f\"- {title} · {lic}\")\n",
    "\n",
    "    # B) Context chunks (if this step came from the 'context' tool)\n",
    "    if tool_name == \"context\":\n",
    "        chunks = obs.get(\"chunks\", [])\n",
    "        if isinstance(chunks, (list, tuple)):\n",
    "            for ch in chunks:\n",
    "                if not isinstance(ch, dict):\n",
    "                    continue\n",
    "                file_id = ch.get(\"file_id\", \"\")\n",
    "                path = ch.get(\"path\", \"\")\n",
    "                url = ch.get(\"url\", \"\")\n",
    "                lic = ch.get(\"license\", \"\") or \"—\"\n",
    "                title = file_id or path or \"Source\"\n",
    "                meta_tail = f\" · {path}\" if path and title != path else \"\"\n",
    "                if url:\n",
    "                    lines.append(f\"- [{title}]({url}) · {lic}{meta_tail}\")\n",
    "                else:\n",
    "                    lines.append(f\"- {title} · {lic}{meta_tail}\")\n",
    "\n",
    "                # chunk-level citations (optional)\n",
    "                ch_cites = ch.get(\"citations\", [])\n",
    "                if isinstance(ch_cites, (list, tuple)):\n",
    "                    for c in ch_cites:\n",
    "                        if not isinstance(c, dict):\n",
    "                            continue\n",
    "                        t2 = c.get(\"title\") or c.get(\"id\") or \"Source\"\n",
    "                        u2 = c.get(\"url\", \"\")\n",
    "                        l2 = c.get(\"license\", \"\") or \"—\"\n",
    "                        if u2:\n",
    "                            lines.append(f\"- [{t2}]({u2}) · {l2}\")\n",
    "                        else:\n",
    "                            lines.append(f\"- {t2} · {l2}\")\n",
    "\n",
    "    if not lines:\n",
    "        return \"\"\n",
    "    return \"**Sources**\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Async streaming handler for Gradio Chatbot(type=\"messages\") ---\n",
    "async def interact_with_langchain_agent(user_text, history):\n",
    "    # history is a list[ChatMessage]\n",
    "    history.append(ChatMessage(role=\"user\", content=user_text))\n",
    "    yield history\n",
    "\n",
    "    # Stream agent steps & final output\n",
    "    async for chunk in agent_executor.astream({\"input\": user_text}):\n",
    "        if \"steps\" in chunk:\n",
    "            for step in chunk[\"steps\"]:\n",
    "                history.append(ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=step.action.log,\n",
    "                    metadata={\"title\": f\"🛠️ Used tool {step.action.tool}\"}\n",
    "                ))\n",
    "\n",
    "\n",
    "                obs = getattr(step, \"observation\", None)\n",
    "                if obs is not None:\n",
    "                    # Try rendering an image if present (unchanged)\n",
    "                    img_msg = _maybe_render_image_from_obs(obs)\n",
    "                    if img_msg is not None:\n",
    "                        history.append(img_msg)\n",
    "                        yield history\n",
    "\n",
    "                    # NEW: one merged sources renderer for both getImage + context\n",
    "                    sources_md = _format_sources_md(obs, step.action.tool)\n",
    "                    if sources_md:\n",
    "                        history.append(ChatMessage(role=\"assistant\", content=sources_md))\n",
    "                        yield history\n",
    "\n",
    "                yield history\n",
    "                #     else:\n",
    "                #         # For other tools you can also render their observations if you want\n",
    "                #         pass\n",
    "\n",
    "\n",
    "                # yield history\n",
    "        if \"output\" in chunk:\n",
    "            history.append(ChatMessage(role=\"assistant\", content=chunk[\"output\"]))\n",
    "            yield history\n",
    "\n",
    "\n",
    "# --- Gradio UI ---\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Beacon - Offline Knolwedge Agent(Village First Aid Demo)\")\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        type=\"messages\",\n",
    "        label=\"Agent\",\n",
    "        avatar_images=(None, \"https://image.pngaaa.com/10/5148010-middle.png\"),\n",
    "        height=400,\n",
    "    )\n",
    "    textbox = gr.Textbox(lines=1, label=\"Chat Message\", placeholder=\"Ask something…\")\n",
    "    # Clear the textbox after submit so it feels chatty\n",
    "    def _clear():\n",
    "        return gr.update(value=\"\")\n",
    "\n",
    "    # Submit: wire the *actual* components you created\n",
    "    textbox.submit(\n",
    "        interact_with_langchain_agent,\n",
    "        inputs=[textbox, chatbot],     # (prompt, messages)\n",
    "        outputs=[chatbot]              # updates the Chatbot\n",
    "    ).then(\n",
    "        _clear, None, [textbox]\n",
    "    )\n",
    "\n",
    "demo.queue().launch()   # queue() is recommended for async callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902a19c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
