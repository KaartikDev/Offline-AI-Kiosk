{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c03167",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "This notebook explores how to wrap the chatbot in a gradio chat interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a332a",
   "metadata": {},
   "source": [
    "###  Setting up paths and loading vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d166f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0) Imports & manifest ===\n",
    "from pathlib import Path\n",
    "import yaml, json\n",
    "from typing import Optional, List, Dict\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools import tool\n",
    "from langchain.schema import Document\n",
    "\n",
    "# If you want the prebuilt ReAct agent:\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# If you prefer to also expose tools directly on the LLM:\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# --- Set your pack root + manifest ---\n",
    "ROOT = Path(\"/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/first_aid_pack_demo_v2\")\n",
    "MANIFEST = ROOT / \"manifest.yaml\"\n",
    "\n",
    "with open(MANIFEST, \"r\", encoding=\"utf-8\") as f:\n",
    "    manifest = yaml.safe_load(f)\n",
    "\n",
    "# --- Resolve FAISS paths from manifest ---\n",
    "faiss_dir_text = ROOT / manifest[\"precomputed_indices\"][\"text\"][\"faiss\"][\"dir\"]\n",
    "faiss_dir_image = ROOT / manifest[\"precomputed_indices\"][\"images\"][\"faiss\"][\"dir\"]\n",
    "# --- Create embeddings *matching the store* ---\n",
    "embed_model_name = manifest[\"embedding_config\"][\"text\"][\"model\"]     #same for text and image\n",
    "emb = OllamaEmbeddings(model=embed_model_name)\n",
    "\n",
    "# --- Load FAISS store + retriever ---\n",
    "text_vs = FAISS.load_local(str(faiss_dir_text), emb, allow_dangerous_deserialization=True)\n",
    "text_retriever = text_vs.as_retriever(search_kwargs={\"k\": 4})  # default k; tool will override if provided\n",
    "\n",
    "image_vs = FAISS.load_local(str(faiss_dir_image), emb, allow_dangerous_deserialization=True)\n",
    "# results = image_vs.similarity_search_with_score(query, k=4)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93926748",
   "metadata": {},
   "source": [
    "### Defining helper and context() Tool  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a18ea3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) Helpers ===\n",
    "def format_chunk(doc: Document, max_chars: int = 400) -> Dict:\n",
    "    \"\"\"Return a dict with compact text + key metadata for prompting & audit.\"\"\"\n",
    "    txt = doc.page_content.strip()\n",
    "    if len(txt) > max_chars:\n",
    "        txt = txt[:max_chars].rstrip() + \" …\"\n",
    "    m = doc.metadata\n",
    "    return {\n",
    "        \"id\": m.get(\"chunk_id\"),\n",
    "        \"topic_id\": m.get(\"topic_id\"),\n",
    "        \"file_id\": m.get(\"file_id\"),\n",
    "        \"locale\": m.get(\"locale\"),\n",
    "        \"path\": m.get(\"path\"),\n",
    "        \"citations\": [c.get(\"title\", \"\") for c in m.get(\"citations\", [])],\n",
    "        \"text\": txt\n",
    "    }\n",
    "\n",
    "def format_context_block(chunks: List[Dict]) -> str:\n",
    "    \"\"\"Human/LLM-friendly context block the agent can drop into its reasoning.\"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"### Retrieved Context (use only what is relevant)\")\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        cite_str = \"; \".join([t for t in c[\"citations\"] if t]) or \"—\"\n",
    "        head = f\"[{i}] {c['topic_id']} · {c['file_id']} · {c['locale']} · {c['path']}\"\n",
    "        lines.append(head)\n",
    "        lines.append(c[\"text\"])\n",
    "        lines.append(f\"Source(s): {cite_str}\")\n",
    "        lines.append(\"\")  # blank line\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "\n",
    "# === 2) The @tool: context() ===\n",
    "@tool\n",
    "def context(\n",
    "    query: str,\n",
    "    k: int = 4,\n",
    "    topic_id: Optional[str] = None,\n",
    "    locale: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve up to k relevant knowledge-pack chunks for 'query' and return a formatted\n",
    "    context block + structured per-chunk data for citations. You must use this tool for any prompt that is \n",
    "    important to wellbeing or safety of user. \n",
    "\n",
    "    Args:\n",
    "        query: Natural language question or keywords.\n",
    "        k: Top-k chunks to return (default 4).\n",
    "        topic_id: Optional manifest topic filter (e.g., 'bleed-control').\n",
    "        locale: Optional locale filter (e.g., 'hi_en' or 'en').\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"query\": str,\n",
    "          \"k\": int,\n",
    "          \"filters\": {\"topic_id\":..., \"locale\":...},\n",
    "          \"context_block\": str,     # pasteable into prompts\n",
    "          \"chunks\": [ {id, topic_id, file_id, path, locale, citations[], text}, ... ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Build a metadata filter if provided\n",
    "    _filter = {}\n",
    "    # if topic_id:\n",
    "    #     _filter[\"topic_id\"] = topic_id\n",
    "    # if locale:\n",
    "    #     _filter[\"locale\"] = locale\n",
    "\n",
    "    # Run retrieval (override k)\n",
    "    local_ret = text_vs.as_retriever(search_kwargs={\"k\": k})\n",
    "    hits: List[Document] = local_ret.invoke(query) if not _filter else local_ret.invoke(query, filter=_filter)\n",
    "\n",
    "    formatted = [format_chunk(d) for d in hits]\n",
    "    ctx_block = format_context_block(formatted)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"k\": k,\n",
    "        \"filters\": _filter,\n",
    "        \"context_block\": ctx_block,\n",
    "        \"chunks\": formatted\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b651",
   "metadata": {},
   "source": [
    "### Math Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fbe93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract first number by second number.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a : float, b: float) -> float:\n",
    "    \"\"\"Divide first number by second number.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return a/b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69135961",
   "metadata": {},
   "source": [
    "### Defining knowledgeMeta() tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b002ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from typing import Optional\n",
    "\n",
    "@tool\n",
    "def knowledgeMeta(pack_dir: Optional[str] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Read a knowledge pack manifest and return metadata for trust and recency. \n",
    "\n",
    "    Args:\n",
    "      pack_dir: Absolute or relative path to the pack folder (containing manifest.yaml).\n",
    "                If omitted, uses the default ROOT pack path.\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"name\": str,\n",
    "        \"version\": str,\n",
    "        \"date\": str,\n",
    "        \"locales\": [..],\n",
    "        \"topics_count\": int,\n",
    "        \"manifest_path\": str\n",
    "      }\n",
    "    \"\"\"\n",
    "    # default to your earlier ROOT if not provided\n",
    "    base = Path(pack_dir) if pack_dir else ROOT\n",
    "    manifest_path = base / \"manifest.yaml\"\n",
    "    if not manifest_path.exists():\n",
    "        return {\"error\": f\"manifest.yaml not found at {manifest_path}\"}\n",
    "\n",
    "    with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        m = yaml.safe_load(f)\n",
    "\n",
    "    name = m.get(\"name\", str(base.name))\n",
    "    version = m.get(\"version\", \"unknown\")\n",
    "    date = m.get(\"date\", \"unknown\")\n",
    "    locales = m.get(\"locales\", [])\n",
    "    topics = m.get(\"index_of_topics\", []) or []\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"version\": version,\n",
    "        \"date\": date,\n",
    "        \"locales\": locales,\n",
    "        \"topics_count\": len(topics),\n",
    "        \"manifest_path\": str(manifest_path)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781c4fc",
   "metadata": {},
   "source": [
    "### getImage tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d162578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "HIGH_SCORE_THRESHOLD = 0.55  # tune as needed\n",
    "\n",
    "@tool\n",
    "def getImage(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve and display exactly one image for the user's query. Don't render images. \n",
    "    Applies a high-confidence threshold; returns NO_IMAGE if nothing clears the bar.\n",
    "    \"\"\"\n",
    "    q = (query or \"\").strip()\n",
    "    pack_name    = manifest.get(\"name\", \"\")\n",
    "    pack_ver     = manifest.get(\"version\", \"\")\n",
    "    pack_date    = manifest.get(\"date\", \"\")\n",
    "    pack_locales = manifest.get(\"locales\", [])\n",
    "\n",
    "    if not q:\n",
    "        return {\n",
    "            \"status\": \"NO_IMAGE\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": \"\",\n",
    "            \"score\": None,\n",
    "            \"citations\": []\n",
    "        }\n",
    "\n",
    "    # Use similarity_search_with_score to get confidence scores\n",
    "    foundImage = False\n",
    "    minScore = 0.32\n",
    "    finalDoc  = Document(page_content=\"\")\n",
    "    results = image_vs.similarity_search_with_score(query, k=4)\n",
    "    finScore = 0\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    for i, (d, score) in enumerate(results, 1):\n",
    "        \n",
    "        if score >= minScore:\n",
    "            finScore = score\n",
    "            finalDoc = d\n",
    "            foundImage = True\n",
    "            break\n",
    "    \n",
    "\n",
    "    if not foundImage:\n",
    "        print(\"NOT FOUND\")\n",
    "        return {\n",
    "            \"status\": \"NO_IMAGE\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": \"\",\n",
    "            \"score\": None,\n",
    "            \"citations\": []\n",
    "        }\n",
    "    else:\n",
    "        print(\"Found FOUND\")\n",
    "        img_path = ROOT / finalDoc.metadata['path']\n",
    "        # try:\n",
    "        #     display(Image(filename=img_path))\n",
    "        # except Exception:\n",
    "        #     pass\n",
    "            \n",
    "        return {\n",
    "            \"status\": \"OK\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": str(img_path),\n",
    "            \"score\": float(finScore),\n",
    "            \"citations\": finalDoc.metadata.get(\"citations\", [])\n",
    "        }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad48bd",
   "metadata": {},
   "source": [
    "### List of Available Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14c9825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a Tools list for whichever orchestration you choose:\n",
    "TOOLS = [context,add,multiply,subtract,divide,knowledgeMeta, getImage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22fb2c",
   "metadata": {},
   "source": [
    "### Setting Up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14a99997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from gradio import ChatMessage\n",
    "import gradio as gr\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "lightModel = init_chat_model(\n",
    "    model=\"ollama:llama3.1\",       \n",
    "    temperature=0.2, # lower = more deterministic\n",
    "    streaming = True  \n",
    ")\n",
    "\n",
    "heavyModel = init_chat_model(\n",
    "    model=\"ollama:gpt-oss:20b\",       \n",
    "    temperature=0.2  # lower = more deterministic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b2695b",
   "metadata": {},
   "source": [
    "### Using gradio with langchain\n",
    "\n",
    "\n",
    "\n",
    "This is a simple general-purpose chatbot built on top of LangChain and Gradio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb3340ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage  \n",
    "import gradio as gr\n",
    "\n",
    "lightModel = init_chat_model(\n",
    "    model=\"ollama:llama3.1\",       \n",
    "    temperature=0.2  # lower = more deterministic\n",
    ")\n",
    "\n",
    "def predict(message, history):\n",
    "    history_langchain_format = []\n",
    "    for msg in history:\n",
    "        if msg['role'] == \"user\":\n",
    "            history_langchain_format.append(HumanMessage(content=msg['content']))\n",
    "        elif msg['role'] == \"assistant\":\n",
    "            history_langchain_format.append(AIMessage(content=msg['content']))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "    llm_response = lightModel.invoke(history_langchain_format)\n",
    "    return llm_response.content\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    predict,\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ce7e4",
   "metadata": {},
   "source": [
    "### Working UI with tool calling and sources\n",
    "\n",
    "Images is next "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded796d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools = TOOLS\n",
    "\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from gradio import ChatMessage\n",
    "import gradio as gr\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools.render import render_text_description\n",
    "\n",
    "# Render tool descriptions and names\n",
    "tool_desc = render_text_description(TOOLS)         # Render the tool name and description in plain text.\n",
    "tool_names = \", \".join([t.name for t in TOOLS])    # exact callable names\n",
    "\n",
    "# Build agent prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"You are a helpful assistant. \n",
    "You can use the following tools if they are useful:\n",
    "{tools}\n",
    "\n",
    "Always call tools by their exact name from: {tool_names}.\n",
    "If no tool is needed, just answer directly. \n",
    "\"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "]).partial(tools=tool_desc, tool_names=tool_names)\n",
    "\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=heavyModel,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,        # helpful while wiring things up\n",
    ").with_config({\"run_name\": \"Agent\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6aaba0",
   "metadata": {},
   "source": [
    "### Current UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99779dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _format_sources_md(obs: dict, tool_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a single Markdown block with sources/citations for either:\n",
    "      - getImage-style obs: { citations: [{title,id,url,license}] }\n",
    "      - context-style obs: { chunks: [{file_id,path,url,license,citations:[...]}] }\n",
    "    \"\"\"\n",
    "    if not isinstance(obs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    # A) Direct citations (e.g., from getImage)\n",
    "    cites = obs.get(\"citations\", [])\n",
    "    if isinstance(cites, (list, tuple)):\n",
    "        for c in cites:\n",
    "            if not isinstance(c, dict):\n",
    "                continue\n",
    "            title = c.get(\"title\") or c.get(\"id\") or \"Source\"\n",
    "            url = c.get(\"url\")\n",
    "            lic = c.get(\"license\") or \"—\"\n",
    "            if url:\n",
    "                lines.append(f\"- [{title}]({url}) · {lic}\")\n",
    "            else:\n",
    "                lines.append(f\"- {title} · {lic}\")\n",
    "\n",
    "    # B) Context chunks (if this step came from the 'context' tool)\n",
    "    if tool_name == \"context\":\n",
    "        chunks = obs.get(\"chunks\", [])\n",
    "        if isinstance(chunks, (list, tuple)):\n",
    "            for ch in chunks:\n",
    "                if not isinstance(ch, dict):\n",
    "                    continue\n",
    "                file_id = ch.get(\"file_id\", \"\")\n",
    "                path = ch.get(\"path\", \"\")\n",
    "                url = ch.get(\"url\", \"\")\n",
    "                lic = ch.get(\"license\", \"\") or \"—\"\n",
    "                title = file_id or path or \"Source\"\n",
    "                meta_tail = f\" · {path}\" if path and title != path else \"\"\n",
    "                if url:\n",
    "                    lines.append(f\"- [{title}]({url}) · {lic}{meta_tail}\")\n",
    "                else:\n",
    "                    lines.append(f\"- {title} · {lic}{meta_tail}\")\n",
    "\n",
    "                # chunk-level citations (optional)\n",
    "                ch_cites = ch.get(\"citations\", [])\n",
    "                if isinstance(ch_cites, (list, tuple)):\n",
    "                    for c in ch_cites:\n",
    "                        if not isinstance(c, dict):\n",
    "                            continue\n",
    "                        t2 = c.get(\"title\") or c.get(\"id\") or \"Source\"\n",
    "                        u2 = c.get(\"url\", \"\")\n",
    "                        l2 = c.get(\"license\", \"\") or \"—\"\n",
    "                        if u2:\n",
    "                            lines.append(f\"- [{t2}]({u2}) · {l2}\")\n",
    "                        else:\n",
    "                            lines.append(f\"- {t2} · {l2}\")\n",
    "\n",
    "    if not lines:\n",
    "        return \"\"\n",
    "    return \"**Sources**\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def _maybe_render_image_from_obs(obs) -> ChatMessage | None:\n",
    "    \"\"\"\n",
    "    Accepts a tool observation dict and returns a ChatMessage with an image\n",
    "    if it finds a usable image path/url. Otherwise returns None.\n",
    "    \"\"\"\n",
    "    if not isinstance(obs, dict):\n",
    "        return None\n",
    "\n",
    "    # Common keys your chain/logs might produce\n",
    "    path = (\n",
    "        obs.get(\"image_path\")\n",
    "        or obs.get(\"path\")\n",
    "        or obs.get(\"file_path\")\n",
    "        or obs.get(\"local_path\")\n",
    "        or None\n",
    "    )\n",
    "    url = obs.get(\"image_url\") or obs.get(\"url\")\n",
    "\n",
    "    # Prefer local path if available; Gradio will serve files inside the app folder.\n",
    "    media = path or url\n",
    "    if not media:\n",
    "        return None\n",
    "\n",
    "    # Option A: return a component (nice rendering)\n",
    "    return ChatMessage(role=\"assistant\", content=gr.Image(value=media))\n",
    "\n",
    "\n",
    "# --- Async streaming handler for Gradio Chatbot(type=\"messages\") ---\n",
    "async def interact_with_langchain_agent(user_text, history):\n",
    "    # history is a list[ChatMessage]\n",
    "    history.append(ChatMessage(role=\"user\", content=user_text))\n",
    "    yield history\n",
    "\n",
    "    # Stream agent steps & final output\n",
    "    async for chunk in agent_executor.astream({\"input\": user_text}):\n",
    "        if \"steps\" in chunk:\n",
    "            for step in chunk[\"steps\"]:\n",
    "                history.append(ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=step.action.log,\n",
    "                    metadata={\"title\": f\"🛠️ Used tool {step.action.tool}\"}\n",
    "                ))\n",
    "\n",
    "\n",
    "                obs = getattr(step, \"observation\", None)\n",
    "                if obs is not None:\n",
    "                    # Try rendering an image if present (unchanged)\n",
    "                    img_msg = _maybe_render_image_from_obs(obs)\n",
    "                    if img_msg is not None:\n",
    "                        history.append(img_msg)\n",
    "                        yield history\n",
    "\n",
    "                    # NEW: one merged sources renderer for both getImage + context\n",
    "                    sources_md = _format_sources_md(obs, step.action.tool)\n",
    "                    if sources_md:\n",
    "                        history.append(ChatMessage(role=\"assistant\", content=sources_md))\n",
    "                        yield history\n",
    "\n",
    "                yield history\n",
    "                #     else:\n",
    "                #         # For other tools you can also render their observations if you want\n",
    "                #         pass\n",
    "\n",
    "\n",
    "                # yield history\n",
    "        if \"output\" in chunk:\n",
    "            history.append(ChatMessage(role=\"assistant\", content=chunk[\"output\"]))\n",
    "            yield history\n",
    "\n",
    "\n",
    "# --- Gradio UI ---\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Beacon - Offline Knolwedge Agent (Village First Aid Demo)\")\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        type=\"messages\",\n",
    "        label=\"Agent\",\n",
    "        avatar_images=(None, \"https://image.pngaaa.com/10/5148010-middle.png\"),\n",
    "        height=400,\n",
    "    )\n",
    "    textbox = gr.Textbox(lines=1, label=\"Chat Message\", placeholder=\"Ask something…\")\n",
    "    # Clear the textbox after submit so it feels chatty\n",
    "    def _clear_now(_msg, _chat):\n",
    "        return gr.update(value=\"\")\n",
    "\n",
    "    # streaming submit\n",
    "    textbox.submit(\n",
    "        interact_with_langchain_agent,\n",
    "        inputs=[textbox, chatbot],\n",
    "        outputs=[chatbot],\n",
    "    )\n",
    "\n",
    "    # instant clear\n",
    "    textbox.submit(\n",
    "        _clear_now,\n",
    "        inputs=[textbox, chatbot],   # must match the event’s inputs (2 args)\n",
    "        outputs=[textbox],\n",
    "        queue=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "demo.queue().launch()   # queue() is recommended for async callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902a19c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e1ee5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46401930",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
