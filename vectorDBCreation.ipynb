{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82abbd78",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook takes the manifest and uses to embed all core documents + citations avaible in a FAISS vector databse with langchain and granite-embedding:30m\n",
    "\n",
    "```\n",
    "ollama pull granite-embedding:30m\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d0e07",
   "metadata": {},
   "source": [
    "### Importing and Paths\n",
    "\n",
    "Change the ROOT  paths as needed. It should point to to the main knowledge pack dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad972735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/first_aid_pack_demo_v2\n",
      "/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/first_aid_pack_demo_v2/manifest.yaml\n"
     ]
    }
   ],
   "source": [
    "# --- A. Imports & config ---\n",
    "from pathlib import Path\n",
    "import json, hashlib, uuid, yaml\n",
    "from typing import List, Dict\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Paths (adapt for your pack root)\n",
    "ROOT = Path(\"/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/first_aid_pack_demo_v2\")\n",
    "MANIFEST = ROOT / \"manifest.yaml\"\n",
    "print(ROOT)\n",
    "print(MANIFEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb7439",
   "metadata": {},
   "source": [
    "### Parsing YAML, Embedding Documents, and Creating Vector Store\n",
    "\n",
    "NOTES: \n",
    "1. Below cell will create a new directory inisde the knolwedge pack:\n",
    "- Example: first_aid_pack_demo_v2/vector_db/text/faiss_index <br>\n",
    "This directory will have the actual .faiss store and index pickle file\n",
    "\n",
    "2. embeddings.jsonl, index.bin, and meta.json under first_aid_pack_demo_v2/vector_db/text/faiss_index will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d36beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 11 chunks\n",
      "Saved FAISS + JSONL export\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(MANIFEST, \"r\", encoding=\"utf-8\") as f:\n",
    "    manifest = yaml.safe_load(f)\n",
    "\n",
    "# Choose model from manifest\n",
    "embed_model_name = manifest[\"embedding_config\"][\"text\"][\"model\"]  # \"granite-embedding:278m\"\n",
    "normalize = bool(manifest[\"embedding_config\"][\"text\"].get(\"normalize\", True))\n",
    "max_tokens   = int(manifest[\"embedding_config\"][\"text\"][\"chunking\"][\"max_tokens\"])\n",
    "overlap_toks = int(manifest[\"embedding_config\"][\"text\"][\"chunking\"][\"overlap_tokens\"])\n",
    "\n",
    "# --- A.1 Embeddings (Ollama + Granite) ---\n",
    "emb = OllamaEmbeddings(model=embed_model_name, # served by local Ollama\n",
    "                       # normalize embeddings to unit length if you want cosine = dot\n",
    "                       # LangChain’s FAISS uses inner product; normalized vectors ≈ cosine\n",
    "                       # Some versions expose \"show_progress_bar\" etc.\n",
    "                      )\n",
    "\n",
    "# --- A.2 Chunking (approx \"semantic+fixed\") ---\n",
    "# We approximate \"semantic+fixed\" by:\n",
    "#   1) small paragraphs/sentences splits (heuristics), then\n",
    "#   2) fixed-size merge with overlap\n",
    "# This is a pragmatic compromise without extra libs.\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    # Sensible boundaries; tweak as needed\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"। \", \". \", \"?\", \"!\", \" \"],\n",
    "    chunk_size=2000,     # ~ tokens proxy; adjust to your docs (we don't have tokenizer here)\n",
    "    chunk_overlap=250,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "def file_bytes(path: Path) -> bytes:\n",
    "    return path.read_bytes()\n",
    "\n",
    "def hash_bytes(b: bytes) -> str:\n",
    "    return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "# --- A.3 Build LangChain Documents with METADATA ---\n",
    "docs: List[Document] = []\n",
    "pack_name   = manifest[\"name\"]\n",
    "pack_ver    = manifest[\"version\"]\n",
    "pack_locales= manifest[\"locales\"]\n",
    "\n",
    "# Build a quick citation lookup\n",
    "citations = {c[\"id\"]: c for c in manifest.get(\"citations\", [])}\n",
    "\n",
    "for topic in manifest[\"index_of_topics\"]:\n",
    "    topic_id = topic[\"id\"]\n",
    "    for fmeta in topic[\"core_files\"]:\n",
    "        fpath = ROOT / fmeta[\"path\"]\n",
    "        if not fpath.exists():\n",
    "            # skip missing dummy files gracefully\n",
    "            continue\n",
    "        raw = fpath.read_text(encoding=\"utf-8\")\n",
    "        chunks = splitter.split_text(raw)\n",
    "        # Derive a locale label from folder name if you like (hi_en)\n",
    "        locale = fmeta[\"path\"].split(\"/\")[2] if \"/hi_en/\" in fmeta[\"path\"] else \"en\"\n",
    "\n",
    "        # Expand citation IDs -> full objs\n",
    "        c_full = [citations[cid] for cid in fmeta.get(\"citations\", []) if cid in citations]\n",
    "\n",
    "        # Make one Document per chunk with rich metadata\n",
    "        for i, text in enumerate(chunks):\n",
    "            docs.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"pack_name\": pack_name,\n",
    "                    \"pack_version\": pack_ver,\n",
    "                    \"topic_id\": topic_id,\n",
    "                    \"file_id\": fmeta[\"id\"],\n",
    "                    \"path\": str(fmeta[\"path\"]),\n",
    "                    \"media_type\": fmeta[\"media_type\"],\n",
    "                    \"locale\": locale,\n",
    "                    \"citations\": c_full,    # keep full objects for traceability\n",
    "                    \"chunk_index\": i,\n",
    "                    # Stable ID for your own bookkeeping\n",
    "                    \"chunk_id\": f\"{fmeta['id']}::chunk::{i}\",\n",
    "                }\n",
    "            ))\n",
    "\n",
    "print(f\"Prepared {len(docs)} chunks\")\n",
    "\n",
    "# --- A.4 Create FAISS & persist ---\n",
    "# Note: FAISS persists two artifacts: \"index.faiss\" and \"index.pkl\" (docstore+index_to_docstore_id)\n",
    "# We'll also export jsonl embeddings/meta to match your manifest's 'precomputed_indices'.\n",
    "faiss_dir = ROOT / \"vector_db\" / \"text\" / \"faiss_index\"\n",
    "faiss_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "vs = FAISS.from_documents(docs, emb)\n",
    "vs.save_local(str(faiss_dir))  # writes index.faiss + index.pkl\n",
    "\n",
    "# --- A.5 (Optional) Export JSONL embeddings + meta to align with manifest paths ---\n",
    "# This performs a forward pass to dump raw vectors & metadata for audit/portability.\n",
    "# Note: it re-embeds; for huge corpora you'd capture vectors in one pass.\n",
    "embeddings_path = ROOT / manifest[\"precomputed_indices\"][\"text\"][\"embeddings\"]      # vector_db/text/embeddings.jsonl\n",
    "meta_path       = ROOT / manifest[\"precomputed_indices\"][\"text\"][\"meta\"]            # vector_db/text/meta.json\n",
    "index_bin_path  = ROOT / manifest[\"precomputed_indices\"][\"text\"][\"index\"]           # vector_db/text/index.bin\n",
    "\n",
    "embeddings_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pull documents back out (the FAISS docstore keeps your metadata)\n",
    "# vs.docstore._dict is a mapping of doc_id -> Document (internal but stable enough for export)\n",
    "records = []\n",
    "for doc_id, doc in vs.docstore._dict.items():\n",
    "    vec = emb.embed_query(doc.page_content)  # one more embed for export\n",
    "    rec = {\n",
    "        \"id\": doc_id,\n",
    "        \"embedding\": vec,\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"text\": doc.page_content\n",
    "    }\n",
    "    records.append(rec)\n",
    "\n",
    "with open(embeddings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in records:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"model\": embed_model_name,\n",
    "        \"dim\": manifest[\"embedding_config\"][\"text\"][\"dim\"],\n",
    "        \"normalize\": normalize,\n",
    "        \"count\": len(records),\n",
    "        \"pack\": {\"name\": pack_name, \"version\": pack_ver, \"locales\": pack_locales}\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Optionally copy the FAISS binary to the path your manifest expects:\n",
    "# (FAISS writes 'index.faiss' -> we copy as 'index.bin' to match your field)\n",
    "import shutil\n",
    "shutil.copyfile(faiss_dir / \"index.faiss\", index_bin_path)\n",
    "print(\"Saved FAISS + JSONL export\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158dcd9",
   "metadata": {},
   "source": [
    "### Testing it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad040d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1]\n",
      "Topic: bleed-control\n",
      "File: guide-bleed-overview\n",
      "Locale: hi_en\n",
      "Citations: ['WHO Basic Emergency Care (B.E.C.)']\n",
      "Chunk text:\n",
      "# Severe Bleeding Control\n",
      "Severe bleeding can quickly become life-threatening if not controlled.  \n",
      "Apply firm direct pressure with a clean cloth or sterile gauze.  \n",
      "If bleeding soaks through, add more cloths without removing the first.  \n",
      "Elevate the injured limb if possible while maintaining pressur ...\n",
      "\n",
      "[2]\n",
      "Topic: fracture-splint\n",
      "File: guide-fracture-overview\n",
      "Locale: hi_en\n",
      "Citations: ['WHO Basic Emergency Care (B.E.C.)']\n",
      "Chunk text:\n",
      "# Fracture & Splinting\n",
      "Fractures may present with pain, swelling, deformity, and inability to use the limb.  \n",
      "Immobilize the joint above and below the suspected fracture.  \n",
      "Use local materials such as bamboo sticks, boards, or rolled newspapers as splints.  \n",
      "Pad the splints with cloth before tying t ...\n",
      "\n",
      "[3]\n",
      "Topic: snakebite\n",
      "File: guide-snakebite-donts\n",
      "Locale: hi_en\n",
      "Citations: ['WHO SEARO – Snakebite Management']\n",
      "Chunk text:\n",
      "# Snakebite\n",
      "Keep the patient calm and lying still to slow the spread of venom.  \n",
      "Remove rings, bangles, or tight clothing from the bitten limb.  \n",
      "Immobilize the limb with a splint and keep it at heart level.  \n",
      "Do not cut, suck, burn, or apply ice to the bite.  \n",
      "Do not apply tight tourniquets; they c ...\n",
      "\n",
      "[4]\n",
      "Topic: burns\n",
      "File: guide-burns-dont\n",
      "Locale: hi_en\n",
      "Citations: ['Sphere Handbook – Health Standards']\n",
      "Chunk text:\n",
      "# Burns (Flame/Scald/Electrical)\n",
      "Cool the burn immediately with clean running water for at least 20 minutes.  \n",
      "Remove jewelry or tight clothing near the burned area before swelling begins.  \n",
      "Do not apply butter, oil, toothpaste, or traditional remedies.  \n",
      "Cover the burn with a clean, non-stick cloth ...\n"
     ]
    }
   ],
   "source": [
    "# Typical retriever usage\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 4})  # if you used Option A 'vs'\n",
    "query = \"What to do for bleeding?\"  #\n",
    "hits = retriever.invoke(query)\n",
    "\n",
    "for i, d in enumerate(hits, 1):\n",
    "    print(d)\n",
    "    print(f\"\\n[{i}]\")\n",
    "    print(\"Topic:\", d.metadata[\"topic_id\"])\n",
    "    print(\"File:\", d.metadata[\"file_id\"])\n",
    "    print(\"Locale:\", d.metadata[\"locale\"])\n",
    "    print(\"Citations:\", [c[\"title\"] for c in d.metadata.get(\"citations\", [])])\n",
    "    print(\"Chunk text:\")\n",
    "    print(d.page_content[:300], \"...\" if len(d.page_content) > 300 else \"\")\n",
    "\n",
    "# Filter to a topic or locale:\n",
    "hits = retriever.invoke(\"tourniquet steps\", filter={\"topic_id\": \"bleed-control\", \"locale\": \"en\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe1544",
   "metadata": {},
   "source": [
    ":)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609f501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
