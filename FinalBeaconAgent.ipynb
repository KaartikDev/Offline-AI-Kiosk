{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c03167",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "This notebook explores how to wrap the chatbot in a gradio chat interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a332a",
   "metadata": {},
   "source": [
    "###  Setting up paths and loading vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d166f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0) Imports & manifest ===\n",
    "from pathlib import Path\n",
    "import yaml, json\n",
    "from typing import Optional, List, Dict\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools import tool\n",
    "from langchain.schema import Document\n",
    "\n",
    "# If you want the prebuilt ReAct agent:\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# If you prefer to also expose tools directly on the LLM:\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# --- Set your pack root + manifest ---\n",
    "which_pack = input(\"Input 'FLORIDA' to choose hurricane kpack or'INDIA' village support kpack: \")\n",
    "ROOT = Path('.')\n",
    "if which_pack == 'FLORIDA':\n",
    "    ROOT = Path('Knowledge Packs/Pinellas County Floirda Hurricane Response Kpack')\n",
    "else:\n",
    "    ROOT = Path('Knowledge Packs/Bihar India Support Kpack')\n",
    "\n",
    "MANIFEST = ROOT / \"manifest.yaml\"\n",
    "\n",
    "with open(MANIFEST, \"r\", encoding=\"utf-8\") as f:\n",
    "    manifest = yaml.safe_load(f)\n",
    "\n",
    "# --- Resolve FAISS paths from manifest ---\n",
    "faiss_dir_text = ROOT / manifest[\"precomputed_indices\"][\"text\"][\"faiss\"][\"dir\"]\n",
    "faiss_dir_image = ROOT / manifest[\"precomputed_indices\"][\"images\"][\"faiss\"][\"dir\"]\n",
    "# --- Create embeddings *matching the store* ---\n",
    "embed_model_name = manifest[\"embedding_config\"][\"text\"][\"model\"]     #same for text and image\n",
    "emb = OllamaEmbeddings(model=embed_model_name)\n",
    "\n",
    "# --- Load FAISS store + retriever ---\n",
    "text_vs = FAISS.load_local(str(faiss_dir_text), emb, allow_dangerous_deserialization=True)\n",
    "text_retriever = text_vs.as_retriever(search_kwargs={\"k\": 4})  # default k; tool will override if provided\n",
    "\n",
    "image_vs = FAISS.load_local(str(faiss_dir_image), emb, allow_dangerous_deserialization=True)\n",
    "# results = image_vs.similarity_search_with_score(query, k=4)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93926748",
   "metadata": {},
   "source": [
    "### Defining helper and context() Tool  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a18ea3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) Helpers ===\n",
    "def format_chunk(doc: Document, max_chars: int = 400) -> Dict:\n",
    "    \"\"\"Return a dict with compact text + key metadata for prompting & audit.\"\"\"\n",
    "    txt = doc.page_content.strip()\n",
    "    if len(txt) > max_chars:\n",
    "        txt = txt[:max_chars].rstrip() + \" …\"\n",
    "    m = doc.metadata\n",
    "    return {\n",
    "        \"id\": m.get(\"chunk_id\"),\n",
    "        \"topic_id\": m.get(\"topic_id\"),\n",
    "        \"file_id\": m.get(\"file_id\"),\n",
    "        \"locale\": m.get(\"locale\"),\n",
    "        \"path\": m.get(\"path\"),\n",
    "        \"citations\": [c.get(\"title\", \"\") for c in m.get(\"citations\", [])],\n",
    "        \"text\": txt\n",
    "    }\n",
    "\n",
    "def format_context_block(chunks: List[Dict]) -> str:\n",
    "    \"\"\"Human/LLM-friendly context block the agent can drop into its reasoning.\"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"### Retrieved Context (use only what is relevant)\")\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        cite_str = \"; \".join([t for t in c[\"citations\"] if t]) or \"—\"\n",
    "        head = f\"[{i}] {c['topic_id']} · {c['file_id']} · {c['locale']} · {c['path']}\"\n",
    "        lines.append(head)\n",
    "        lines.append(c[\"text\"])\n",
    "        lines.append(f\"Source(s): {cite_str}\")\n",
    "        lines.append(\"\")  # blank line\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "\n",
    "# === 2) The @tool: context() ===\n",
    "@tool\n",
    "def context(\n",
    "    query: str,\n",
    "    k: int = 4,\n",
    "    topic_id: Optional[str] = None,\n",
    "    locale: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    PURPOSE\n",
    "    Retrieve Pack knowledge for the user's query. \n",
    "    TRIGGERS — YOU MUST CALL THIS TOOL BEFORE ANSWERING OR REFUSING IF:\n",
    "      1) The query involves local geography or place-specific info:\n",
    "         - “nearby”, “closest”, “where is…”, shelters/clinics/transport, checkpoints, routes, hours, hazards\n",
    "      2) The query is wellbeing/safety-critical or time-sensitive:\n",
    "         - first aid, symptoms, medications, exposure, heat/cold, water/food safety, wounds, evacuation, flooding, snakebite, pesticides, hazardous materials\n",
    "      3) The query is high-uncertainty and a mistake could harm the user.\n",
    "\n",
    "    Do NOT refuse until you have called this tool at least once.\n",
    "\n",
    "    Args:\n",
    "        query: Natural-language question or keywords.\n",
    "        k: Top-k chunks to return (default 4).\n",
    "        topic_id: Optional manifest topic filter (e.g. \"bleed-control\").\n",
    "        locale: Optional locale (e.g. \"en\", \"hi_en\").\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"query\": str,\n",
    "          \"k\": int,\n",
    "          \"filters\": {\"topic_id\":..., \"locale\":...},\n",
    "          \"context_block\": str,   # pasteable summary of chunks\n",
    "          \"chunks\": [ {id, topic_id, file_id, path, locale, citations[], text}, ... ]\n",
    "        }\n",
    "\n",
    "    USAGE NOTES\n",
    "    - Use retrieved content to ground your answer \n",
    "    - If nothing relevant is found, say so and offer next-best actions present in the Pack.\n",
    "    \"\"\"\n",
    "    # Build a metadata filter if provided\n",
    "    _filter = {}\n",
    "    # if topic_id:\n",
    "    #     _filter[\"topic_id\"] = topic_id\n",
    "    # if locale:\n",
    "    #     _filter[\"locale\"] = locale\n",
    "\n",
    "    # Run retrieval (override k)\n",
    "    local_ret = text_vs.as_retriever(search_kwargs={\"k\": k})\n",
    "    hits: List[Document] = local_ret.invoke(query) if not _filter else local_ret.invoke(query, filter=_filter)\n",
    "\n",
    "    formatted = [format_chunk(d) for d in hits]\n",
    "    ctx_block = format_context_block(formatted)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"k\": k,\n",
    "        \"filters\": _filter,\n",
    "        \"context_block\": ctx_block,\n",
    "        \"chunks\": formatted\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b651",
   "metadata": {},
   "source": [
    "### Math Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fbe93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract first number by second number.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a : float, b: float) -> float:\n",
    "    \"\"\"Divide first number by second number.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return a/b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69135961",
   "metadata": {},
   "source": [
    "### Defining knowledgeMeta() tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b002ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from typing import Optional\n",
    "\n",
    "@tool\n",
    "def knowledgeMeta(pack_dir: Optional[str] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Read a knowledge pack manifest and return metadata for trust and recency. \n",
    "\n",
    "    Args:\n",
    "      pack_dir: Absolute or relative path to the pack folder (containing manifest.yaml).\n",
    "                If omitted, uses the default ROOT pack path.\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"name\": str,\n",
    "        \"version\": str,\n",
    "        \"date\": str,\n",
    "        \"locales\": [..],\n",
    "        \"topics_count\": int,\n",
    "        \"manifest_path\": str\n",
    "      }\n",
    "\n",
    "    \"\"\"\n",
    "    # default to your earlier ROOT if not provided\n",
    "    base = Path(pack_dir) if pack_dir else ROOT\n",
    "    manifest_path = base / \"manifest.yaml\"\n",
    "    if not manifest_path.exists():\n",
    "        return {\"error\": f\"manifest.yaml not found at {manifest_path}\"}\n",
    "\n",
    "    with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        m = yaml.safe_load(f)\n",
    "\n",
    "    name = m.get(\"name\", str(base.name))\n",
    "    version = m.get(\"version\", \"unknown\")\n",
    "    date = m.get(\"date\", \"unknown\")\n",
    "    locales = m.get(\"locales\", [])\n",
    "    topics = m.get(\"index_of_topics\", []) or []\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"version\": version,\n",
    "        \"date\": date,\n",
    "        \"locales\": locales,\n",
    "        \"topics_count\": len(topics),\n",
    "        \"manifest_path\": str(manifest_path)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781c4fc",
   "metadata": {},
   "source": [
    "### getImage tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d162578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "HIGH_SCORE_THRESHOLD = 0.55  # tune as needed\n",
    "\n",
    "@tool\n",
    "def getImage(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    PURPOSE\n",
    "    Retrieve a single high-confidence Pack image for the query. DO NOT DISPLAY IMAGE, IT IS DONE SO AUTOMATICALLY AT THE TOP OF YOUR MESSAGE.\n",
    "\n",
    "    TRIGGERS — YOU MUST CALL THIS TOOL WHEN:\n",
    "      1) The user asks to \"show\" or \"see\" something (e.g., “show me the Heimlich position”),\n",
    "      2) The user requests a diagram or visual (diagram / illustrate / picture / visual / map),\n",
    "      3) A visual guide would materially improve a physical technique (CPR posture, tourniquet placement, splinting, boiling water, wound cleaning, snakebite immobilization).\n",
    "\n",
    "    If no suitable image is found, return NO_IMAGE and proceed with clear step-by-step text (and cite “context” if used).\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"status\": \"OK\" | \"NO_IMAGE\",\n",
    "          \"version\": str,\n",
    "          \"date\": str,\n",
    "          \"locales\": [str],\n",
    "          \"pack_name\": str,\n",
    "          \"image_path\": str,      # absolute or pack-relative path\n",
    "          \"score\": float | None,\n",
    "          \"citations\": list       # e.g., [{\"title\": \"...\", ...}]\n",
    "        }\n",
    "    \"\"\"\n",
    "    q = (query or \"\").strip()\n",
    "    pack_name    = manifest.get(\"name\", \"\")\n",
    "    pack_ver     = manifest.get(\"version\", \"\")\n",
    "    pack_date    = manifest.get(\"date\", \"\")\n",
    "    pack_locales = manifest.get(\"locales\", [])\n",
    "\n",
    "    if not q:\n",
    "        return {\n",
    "            \"status\": \"NO_IMAGE\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": \"\",\n",
    "            \"score\": None,\n",
    "            \"citations\": []\n",
    "        }\n",
    "\n",
    "    # Use similarity_search_with_score to get confidence scores\n",
    "    foundImage = False\n",
    "    minScore = 0.32\n",
    "    finalDoc  = Document(page_content=\"\")\n",
    "    results = image_vs.similarity_search_with_score(query, k=4)\n",
    "    finScore = 0\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    for i, (d, score) in enumerate(results, 1):\n",
    "        \n",
    "        if score >= minScore:\n",
    "            finScore = score\n",
    "            finalDoc = d\n",
    "            foundImage = True\n",
    "            break\n",
    "    \n",
    "\n",
    "    if not foundImage:\n",
    "        print(\"NOT FOUND\")\n",
    "        return {\n",
    "            \"status\": \"NO_IMAGE\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": \"\",\n",
    "            \"score\": None,\n",
    "            \"citations\": []\n",
    "        }\n",
    "    else:\n",
    "        print(\"Found FOUND\")\n",
    "        img_path = ROOT / finalDoc.metadata['path']\n",
    "        # try:\n",
    "        #     display(Image(filename=img_path))\n",
    "        # except Exception:\n",
    "        #     pass\n",
    "            \n",
    "        return {\n",
    "            \"status\": \"OK\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": str(img_path),\n",
    "            \"score\": float(finScore),\n",
    "            \"citations\": finalDoc.metadata.get(\"citations\", [])\n",
    "        }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad48bd",
   "metadata": {},
   "source": [
    "### List of Available Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14c9825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a Tools list for whichever orchestration you choose:\n",
    "TOOLS = [context,add,multiply,subtract,divide,knowledgeMeta, getImage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22fb2c",
   "metadata": {},
   "source": [
    "### Setting Up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14a99997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from gradio import ChatMessage\n",
    "import gradio as gr\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "lightModel = init_chat_model(\n",
    "    model=\"ollama:llama3.1\",       \n",
    "    temperature=0.2, # lower = more deterministic\n",
    "    streaming = True  \n",
    ")\n",
    "\n",
    "heavyModel = init_chat_model(\n",
    "    model=\"ollama:gpt-oss:20b\",       \n",
    "    temperature=0.2  # lower = more deterministic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902a19c",
   "metadata": {},
   "source": [
    "### Final Beacon Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3ea942e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new Agent chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello! How can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new Agent chain...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# --- citations: manifest-only (title, url, license) ---\n",
    "def _format_sources_md(obs: Dict[str, Any]) -> str:\n",
    "    if not isinstance(obs, dict):\n",
    "        return \"\"\n",
    "    collected: List[Dict[str, Any]] = []\n",
    "\n",
    "    # top-level citations (e.g., getImage)\n",
    "    top = obs.get(\"citations\")\n",
    "    if isinstance(top, list):\n",
    "        for c in top:\n",
    "            if isinstance(c, dict):\n",
    "                collected.append(c)\n",
    "\n",
    "    # chunk-level citations (e.g., context)\n",
    "    chunks = obs.get(\"chunks\")\n",
    "    if isinstance(chunks, list):\n",
    "        for ch in chunks:\n",
    "            if not isinstance(ch, dict):\n",
    "                continue\n",
    "            ch_cites = ch.get(\"citations\")\n",
    "            if isinstance(ch_cites, list):\n",
    "                for c in ch_cites:\n",
    "                    if isinstance(c, dict):\n",
    "                        collected.append(c)\n",
    "\n",
    "    seen = set()\n",
    "    lines: List[str] = []\n",
    "    for c in collected:\n",
    "        title = (c.get(\"title\") or c.get(\"id\") or \"Source\").strip()\n",
    "        url   = (c.get(\"url\") or \"\").strip()\n",
    "        lic   = (c.get(\"license\") or \"—\").strip()\n",
    "        key = (url or title).lower()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        if url:\n",
    "            lines.append(f\"- [{title}]({url}) · {lic}\")\n",
    "        else:\n",
    "            lines.append(f\"- {title} · {lic}\")\n",
    "\n",
    "    return \"**Sources**\\n\" + \"\\n\".join(lines) if lines else \"\"\n",
    "\n",
    "\n",
    "def _maybe_render_image_from_obs(obs) -> ChatMessage | None:\n",
    "    if not isinstance(obs, dict):\n",
    "        return None\n",
    "    path = (\n",
    "        obs.get(\"image_path\")\n",
    "        or obs.get(\"path\")\n",
    "        or obs.get(\"file_path\")\n",
    "        or obs.get(\"local_path\")\n",
    "        or None\n",
    "    )\n",
    "    url = obs.get(\"image_url\") or obs.get(\"url\")\n",
    "    media = path or url\n",
    "    if not media:\n",
    "        return None\n",
    "    return ChatMessage(role=\"assistant\", content=gr.Image(value=media))\n",
    "\n",
    "\n",
    "# --- Async streaming handler for Gradio Chatbot(type=\"messages\") ---\n",
    "async def interact_with_langchain_agent(user_text, history):\n",
    "    \"\"\"\n",
    "    Streams a conversation turn:\n",
    "      - append user msg\n",
    "      - show ⏳ placeholder\n",
    "      - stream agent tool results (image + Sources only)\n",
    "      - stream final answer\n",
    "    Robust to exceptions; will surface errors in-chat and close cleanly.\n",
    "    \"\"\"\n",
    "    # 1) user message\n",
    "    history.append(ChatMessage(role=\"user\", content=user_text))\n",
    "    yield history\n",
    "\n",
    "    # 2) thinking placeholder\n",
    "    thinking_msg = ChatMessage(role=\"assistant\", content=\"⏳ Thinking...\")\n",
    "    history.append(thinking_msg)\n",
    "    yield history\n",
    "\n",
    "    try:\n",
    "        # 3) stream agent\n",
    "        async for chunk in agent_executor.astream({\"input\": user_text}):\n",
    "\n",
    "            # remove placeholder on first real activity\n",
    "            if thinking_msg in history:\n",
    "                try:\n",
    "                    history.remove(thinking_msg)\n",
    "                except ValueError:\n",
    "                    pass  # already removed elsewhere\n",
    "\n",
    "            # show ONLY user-facing results from tools\n",
    "            if \"steps\" in chunk:\n",
    "                for step in chunk[\"steps\"]:\n",
    "                    # DO NOT append step.action.log (keeps tool calls hidden)\n",
    "                    obs = getattr(step, \"observation\", None)\n",
    "                    if not isinstance(obs, dict):\n",
    "                        continue\n",
    "\n",
    "                    # image\n",
    "                    img_msg = _maybe_render_image_from_obs(obs)\n",
    "                    if img_msg is not None:\n",
    "                        history.append(img_msg)\n",
    "                        yield history\n",
    "\n",
    "                    # sources\n",
    "                    sources_md = _format_sources_md(obs)\n",
    "                    if sources_md:\n",
    "                        history.append(ChatMessage(role=\"assistant\", content=sources_md))\n",
    "                        yield history\n",
    "\n",
    "                # yield after processing this chunk\n",
    "                yield history\n",
    "\n",
    "            # final assistant output\n",
    "            if \"output\" in chunk:\n",
    "                history.append(ChatMessage(role=\"assistant\", content=chunk[\"output\"]))\n",
    "                yield history\n",
    "\n",
    "        # finished normally — ensure placeholder is gone\n",
    "        if thinking_msg in history:\n",
    "            try:\n",
    "                history.remove(thinking_msg)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        # surface the real error so you can see it instead of a vague aclose warning\n",
    "        if thinking_msg in history:\n",
    "            try:\n",
    "                history.remove(thinking_msg)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        history.append(ChatMessage(role=\"assistant\", content=f\"⚠️ Error: {e}\"))\n",
    "        yield history\n",
    "\n",
    "    finally:\n",
    "        # give Gradio a tick to close the async generator cleanly\n",
    "        await asyncio.sleep(0)\n",
    "        return\n",
    "\n",
    "\n",
    "# --- Gradio UI ---\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Beacon - Knowledge Agent (Village Bihar, India)\")\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        type=\"messages\",\n",
    "        label=\"Agent\",\n",
    "        avatar_images=(None, \"/Users/ktejwani/Personal CS Projects/Summer 2025/Offline AI Kiosk/Offline-AI-Kiosk/avatar.png\"),\n",
    "        height=650,\n",
    "    )\n",
    "    textbox = gr.Textbox(lines=1, label=\"Chat Message\", placeholder=\"Ask something…\")\n",
    "\n",
    "    # Clear the textbox after submit so it feels chatty\n",
    "    def _clear_now(_msg, _chat):\n",
    "        return gr.update(value=\"\")\n",
    "\n",
    "    # streaming submit\n",
    "    textbox.submit(\n",
    "        interact_with_langchain_agent,\n",
    "        inputs=[textbox, chatbot],\n",
    "        outputs=[chatbot],\n",
    "    )\n",
    "\n",
    "    # instant clear\n",
    "    textbox.submit(\n",
    "        _clear_now,\n",
    "        inputs=[textbox, chatbot],   # must match the event’s inputs (2 args)\n",
    "        outputs=[textbox],\n",
    "        queue=False,\n",
    "    )\n",
    "\n",
    "demo.queue().launch()  # queue() is recommended for async callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a28f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1ee5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46401930",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d076a8fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e3ba199",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
