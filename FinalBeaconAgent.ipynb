{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c03167",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is the final Beacon demo notebook.  \n",
    "\n",
    "Before running, please ensure you have:\n",
    "\n",
    "**Ollama** installed with the required models:  \n",
    "```\n",
    "ollama pull gpt-oss:20b\n",
    "ollama pull nomic-embed-text:v1.5\n",
    "```\n",
    "All dependencies from ```requirements.txt``` installed.  \n",
    "\n",
    "Run the notebook from top to bottom. The last cell will launch the Beacon UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a332a",
   "metadata": {},
   "source": [
    "###  Setting Up Paths and Loading Knowledge Pack Stores\n",
    "\n",
    "Enter **\"FLORIDA\"** to load the Hurricane Response pack for Pinellas County, FL,  \n",
    "or **\"INDIA\"** to load the Rural Support pack for Bihar, India.  \n",
    "\n",
    "- **Hurricane Response Pack (Florida):** covers emergency preparedness, communications, flood safety, evacuation maps, and local resources.  \n",
    "- **Rural Support Pack (India):** covers first aid (maternal care, burns, snakebites, etc.), safe water storage and sanitation, train and rainfall maps, and more.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d166f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0) Imports & manifest ===\n",
    "from pathlib import Path\n",
    "import yaml, json\n",
    "from typing import Optional, List, Dict\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools import tool\n",
    "from langchain.schema import Document\n",
    "\n",
    "# If you want the prebuilt ReAct agent:\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# If you prefer to also expose tools directly on the LLM:\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# --- Set pack root + manifest ---\n",
    "which_pack = input(\"Input 'FLORIDA' to choose hurricane kpack or'INDIA' village support kpack: \")\n",
    "ROOT = Path('.')\n",
    "if which_pack == 'FLORIDA':\n",
    "    ROOT = Path('Knowledge Packs/Pinellas County Floirda Hurricane Response Kpack')\n",
    "else:\n",
    "    ROOT = Path('Knowledge Packs/Bihar India Support Kpack')\n",
    "\n",
    "MANIFEST = ROOT / \"manifest.yaml\"\n",
    "\n",
    "with open(MANIFEST, \"r\", encoding=\"utf-8\") as f:\n",
    "    manifest = yaml.safe_load(f)\n",
    "\n",
    "# --- Resolve FAISS paths from manifest ---\n",
    "faiss_dir_text = ROOT / manifest[\"precomputed_indices\"][\"text\"][\"faiss\"][\"dir\"]\n",
    "faiss_dir_image = ROOT / manifest[\"precomputed_indices\"][\"images\"][\"faiss\"][\"dir\"]\n",
    "# --- Create embeddings *matching the store* ---\n",
    "embed_model_name = manifest[\"embedding_config\"][\"text\"][\"model\"]     #same for text and image\n",
    "emb = OllamaEmbeddings(model=embed_model_name)\n",
    "\n",
    "# --- Load FAISS store + retriever ---\n",
    "text_vs = FAISS.load_local(str(faiss_dir_text), emb, allow_dangerous_deserialization=True)\n",
    "text_retriever = text_vs.as_retriever(search_kwargs={\"k\": 4})  # default k; tool will override if provided\n",
    "\n",
    "image_vs = FAISS.load_local(str(faiss_dir_image), emb, allow_dangerous_deserialization=True)\n",
    "# results = image_vs.similarity_search_with_score(query, k=4)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93926748",
   "metadata": {},
   "source": [
    "### Defining helper and context() Tool  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18ea3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) Helpers ===\n",
    "def format_chunk(doc: Document, max_chars: int = 400) -> Dict:\n",
    "    \"\"\"Return a dict with compact text + key metadata for prompting & audit.\"\"\"\n",
    "    txt = doc.page_content.strip()\n",
    "    if len(txt) > max_chars:\n",
    "        txt = txt[:max_chars].rstrip() + \" …\"\n",
    "    m = doc.metadata\n",
    "    return {\n",
    "        \"id\": m.get(\"chunk_id\"),\n",
    "        \"topic_id\": m.get(\"topic_id\"),\n",
    "        \"file_id\": m.get(\"file_id\"),\n",
    "        \"locale\": m.get(\"locale\"),\n",
    "        \"path\": m.get(\"path\"),\n",
    "        \"citations\": [c.get(\"title\", \"\") for c in m.get(\"citations\", [])],\n",
    "        \"text\": txt\n",
    "    }\n",
    "\n",
    "def format_context_block(chunks: List[Dict]) -> str:\n",
    "    \"\"\"Human/LLM-friendly context block the agent can drop into its reasoning.\"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"### Retrieved Context (use only what is relevant)\")\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        cite_str = \"; \".join([t for t in c[\"citations\"] if t]) or \"—\"\n",
    "        head = f\"[{i}] {c['topic_id']} · {c['file_id']} · {c['locale']} · {c['path']}\"\n",
    "        lines.append(head)\n",
    "        lines.append(c[\"text\"])\n",
    "        lines.append(f\"Source(s): {cite_str}\")\n",
    "        lines.append(\"\")  # blank line\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "\n",
    "# === 2) The @tool: context() ===\n",
    "@tool\n",
    "def context(\n",
    "    query: str,\n",
    "    k: int = 4,\n",
    "    topic_id: Optional[str] = None,\n",
    "    locale: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    PURPOSE\n",
    "    Retrieve Pack knowledge for the user's query. \n",
    "    TRIGGERS — YOU MUST CALL THIS TOOL BEFORE ANSWERING OR REFUSING IF:\n",
    "      1) The query involves local geography or place-specific info:\n",
    "         - “nearby”, “closest”, “where is…”, shelters/clinics/transport, checkpoints, routes, hours, hazards\n",
    "      2) The query is wellbeing/safety-critical or time-sensitive:\n",
    "         - first aid, symptoms, medications, exposure, heat/cold, water/food safety, wounds, evacuation, flooding, snakebite, pesticides, hazardous materials\n",
    "      3) The query is high-uncertainty and a mistake could harm the user.\n",
    "\n",
    "    Do NOT refuse until you have called this tool at least once.\n",
    "\n",
    "    Args:\n",
    "        query: Natural-language question or keywords.\n",
    "        k: Top-k chunks to return (default 4).\n",
    "        topic_id: Optional manifest topic filter (e.g. \"bleed-control\").\n",
    "        locale: Optional locale (e.g. \"en\", \"hi_en\").\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"query\": str,\n",
    "          \"k\": int,\n",
    "          \"filters\": {\"topic_id\":..., \"locale\":...},\n",
    "          \"context_block\": str,   # pasteable summary of chunks\n",
    "          \"chunks\": [ {id, topic_id, file_id, path, locale, citations[], text}, ... ]\n",
    "        }\n",
    "\n",
    "    USAGE NOTES\n",
    "    - Use retrieved content to ground your answer \n",
    "    - If nothing relevant is found, say so and offer next-best actions present in the Pack.\n",
    "    \"\"\"\n",
    "    # (Deprecated) Metadata filters were once used here but are not anymore\n",
    "    # Keeping placeholder for future extensions if topic/locale filters return.\n",
    "    _filter = {}\n",
    "\n",
    "    # Run retrieval (override k)\n",
    "    local_ret = text_vs.as_retriever(search_kwargs={\"k\": k})\n",
    "    hits: List[Document] = local_ret.invoke(query) if not _filter else local_ret.invoke(query, filter=_filter)\n",
    "\n",
    "    formatted = [format_chunk(d) for d in hits]\n",
    "    ctx_block = format_context_block(formatted)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"k\": k,\n",
    "        \"filters\": _filter,\n",
    "        \"context_block\": ctx_block,\n",
    "        \"chunks\": formatted\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b651",
   "metadata": {},
   "source": [
    "### Math Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fbe93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract first number by second number.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\n",
    "\n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a : float, b: float) -> float:\n",
    "    \"\"\"Divide first number by second number.\n",
    "    \n",
    "    Args:\n",
    "        a: First float\n",
    "        b: Second float\n",
    "    \"\"\"\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return a/b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69135961",
   "metadata": {},
   "source": [
    "### Defining knowledgeMeta() tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b002ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from typing import Optional\n",
    "\n",
    "@tool\n",
    "def knowledgeMeta(pack_dir: Optional[str] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Read a knowledge pack manifest and return metadata for trust and recency. \n",
    "\n",
    "    Args:\n",
    "      pack_dir: Absolute or relative path to the pack folder (containing manifest.yaml).\n",
    "                If omitted, uses the default ROOT pack path.\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"name\": str,\n",
    "        \"version\": str,\n",
    "        \"date\": str,\n",
    "        \"locales\": [..],\n",
    "        \"topics_count\": int,\n",
    "        \"manifest_path\": str\n",
    "      }\n",
    "\n",
    "    \"\"\"\n",
    "    # default to your earlier ROOT if not provided\n",
    "    base = Path(pack_dir) if pack_dir else ROOT\n",
    "    manifest_path = base / \"manifest.yaml\"\n",
    "    if not manifest_path.exists():\n",
    "        return {\"error\": f\"manifest.yaml not found at {manifest_path}\"}\n",
    "\n",
    "    with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        m = yaml.safe_load(f)\n",
    "\n",
    "    name = m.get(\"name\", str(base.name))\n",
    "    version = m.get(\"version\", \"unknown\")\n",
    "    date = m.get(\"date\", \"unknown\")\n",
    "    locales = m.get(\"locales\", [])\n",
    "    topics = m.get(\"index_of_topics\", []) or []\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"version\": version,\n",
    "        \"date\": date,\n",
    "        \"locales\": locales,\n",
    "        \"topics_count\": len(topics),\n",
    "        \"manifest_path\": str(manifest_path)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781c4fc",
   "metadata": {},
   "source": [
    "### getImage tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d162578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "HIGH_SCORE_THRESHOLD = 0.55  # tune as needed\n",
    "\n",
    "@tool\n",
    "def getImage(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    PURPOSE\n",
    "    Retrieve a single high-confidence Pack image for the query. DO NOT DISPLAY IMAGE, IT IS DONE SO AUTOMATICALLY AT THE TOP OF YOUR MESSAGE.\n",
    "\n",
    "    TRIGGERS — YOU MUST CALL THIS TOOL WHEN:\n",
    "      1) The user asks to \"show\" or \"see\" something (e.g., “show me the Heimlich position”),\n",
    "      2) The user requests a diagram or visual (diagram / illustrate / picture / visual / map),\n",
    "      3) A visual guide would materially improve a physical technique (CPR posture, tourniquet placement, splinting, boiling water, wound cleaning, snakebite immobilization).\n",
    "\n",
    "    If no suitable image is found, return NO_IMAGE and proceed with clear step-by-step text (and cite “context” if used).\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"status\": \"OK\" | \"NO_IMAGE\",\n",
    "          \"version\": str,\n",
    "          \"date\": str,\n",
    "          \"locales\": [str],\n",
    "          \"pack_name\": str,\n",
    "          \"image_path\": str,      # absolute or pack-relative path\n",
    "          \"score\": float | None,\n",
    "          \"citations\": list       # e.g., [{\"title\": \"...\", ...}]\n",
    "        }\n",
    "    \"\"\"\n",
    "    q = (query or \"\").strip()\n",
    "    pack_name    = manifest.get(\"name\", \"\")\n",
    "    pack_ver     = manifest.get(\"version\", \"\")\n",
    "    pack_date    = manifest.get(\"date\", \"\")\n",
    "    pack_locales = manifest.get(\"locales\", [])\n",
    "\n",
    "    if not q:\n",
    "        return {\n",
    "            \"status\": \"NO_IMAGE\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": \"\",\n",
    "            \"score\": None,\n",
    "            \"citations\": []\n",
    "        }\n",
    "\n",
    "    # Use similarity_search_with_score to get confidence scores\n",
    "    foundImage = False\n",
    "    minScore = 0.32\n",
    "    finalDoc  = Document(page_content=\"\")\n",
    "    results = image_vs.similarity_search_with_score(query, k=4)\n",
    "    finScore = 0\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    for i, (d, score) in enumerate(results, 1):\n",
    "        \n",
    "        if score >= minScore:\n",
    "            finScore = score\n",
    "            finalDoc = d\n",
    "            foundImage = True\n",
    "            break\n",
    "    \n",
    "\n",
    "    if not foundImage:\n",
    "        print(\"NOT FOUND\")\n",
    "        return {\n",
    "            \"status\": \"NO_IMAGE\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": \"\",\n",
    "            \"score\": None,\n",
    "            \"citations\": []\n",
    "        }\n",
    "    else:\n",
    "        print(\"Found FOUND\")\n",
    "        img_path = ROOT / finalDoc.metadata['path']\n",
    "        # try:\n",
    "        #     display(Image(filename=img_path))\n",
    "        # except Exception:\n",
    "        #     pass\n",
    "            \n",
    "        return {\n",
    "            \"status\": \"OK\",\n",
    "            \"version\": pack_ver,\n",
    "            \"date\": pack_date,\n",
    "            \"locales\": pack_locales,\n",
    "            \"pack_name\": pack_name,\n",
    "            \"image_path\": str(img_path),\n",
    "            \"score\": float(finScore),\n",
    "            \"citations\": finalDoc.metadata.get(\"citations\", [])\n",
    "        }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad48bd",
   "metadata": {},
   "source": [
    "### List of Available Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c9825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a Tools list for whichever orchestration you choose:\n",
    "TOOLS = [context,add,multiply,subtract,divide,knowledgeMeta, getImage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22fb2c",
   "metadata": {},
   "source": [
    "### Setting Up LLM (GPT OSS USED HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a99997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from gradio import ChatMessage\n",
    "import gradio as gr\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "\n",
    "heavyModel = init_chat_model(\n",
    "    model=\"ollama:gpt-oss:20b\",       \n",
    "    temperature=0.2  # lower = more deterministic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4f448",
   "metadata": {},
   "source": [
    "### System Prompt and Agent Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcdcaa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools = TOOLS\n",
    "\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from gradio import ChatMessage\n",
    "import gradio as gr\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools.render import render_text_description\n",
    "\n",
    "# Render tool descriptions and names\n",
    "tool_desc = render_text_description(TOOLS)         # Render the tool name and description in plain text.\n",
    "tool_names = \", \".join([t.name for t in TOOLS])    # exact callable names\n",
    "\n",
    "# Build agent prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"You are Beacon, a helpful assistant that answers using the active Knowledge Pack (domain- and locale-specific, offline-first).\n",
    "\n",
    "HARD RULES — TOOL USE\n",
    "1) You MUST call the tool named “context” BEFORE answering or refusing if:\n",
    "   • The user asks about local geography, “near me/nearby/closest”, directions, hours, routes, checkpoints, shelters, clinics, water points, transport, or place-specific availability, OR\n",
    "   • The user’s request affects health, safety, or wellbeing (first aid, symptoms, medications, exposure, heat/cold, water safety, food safety, wound care, evacuation decisions, flooding, snakebite, pesticides, hazardous materials), OR\n",
    "   • The query is high-stakes, time-sensitive, or ambiguous in a way that could impact safety.\n",
    "   → Do not refuse UNTIL you have called “context”.\n",
    "\n",
    "2) You MUST call the tool named “getImage” when:\n",
    "   • The user says “show me …”, “what does … look like?”, “diagram”, “visual”, “illustrate”, OR\n",
    "   • A visual guide would materially improve understanding for a physical technique (e.g., CPR posture, Heimlich, tourniquet placement, splinting, water boiling steps, winding a bandage).\n",
    "   If “getImage” returns NO_IMAGE, continue with clear, step-by-step text and cite sources from “context” if available.\n",
    "\n",
    "3) Tool names must match exactly from {tool_names}. Prefer the smallest sufficient k. If a Pack locale or topic is clear, pass it.\n",
    "\n",
    "4) Citations: When “context” is used, include a short “Sources” line drawn from its returned citations when you give final guidance.\n",
    "\n",
    "5) Refusals: Only refuse after calling “context” if (a) the Pack lacks relevant guidance or (b) the request is outside your guardrails. In refusals, suggest the nearest safe alternative or escalation path if the Pack provides one.\n",
    "\n",
    "OUTPUT STYLE\n",
    "• Be concise, stepwise, and actionable. If life/safety-critical, front-load DOs and DON’Ts and emphasize time-critical steps.\n",
    "• If the Pack is ambiguous, state assumptions briefly and continue.\n",
    "• NEVER hallucinate locations or phone numbers—use “context”. If none found, say so and offer next-best actions present in the Pack.\n",
    "\n",
    "You MUST use the following tools at least once if the above rules apply:\n",
    "{tools}\n",
    "(Always call tools by exact name from: {tool_names}. If no rule triggers, you may answer directly.)\n",
    "\"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "]).partial(tools=tool_desc, tool_names=tool_names)\n",
    "\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=heavyModel,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,        # helpful while wiring things up\n",
    ").with_config({\"run_name\": \"Agent\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902a19c",
   "metadata": {},
   "source": [
    "### Streaming Function for Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ea942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# --- citations: (title, url, license) ---\n",
    "def _format_sources_md(obs: Dict[str, Any]) -> str:\n",
    "    if not isinstance(obs, dict):\n",
    "        return \"\"\n",
    "    collected: List[Dict[str, Any]] = []\n",
    "\n",
    "    # top-level citations (e.g., getImage)\n",
    "    top = obs.get(\"citations\")\n",
    "    if isinstance(top, list):\n",
    "        for c in top:\n",
    "            if isinstance(c, dict):\n",
    "                collected.append(c)\n",
    "\n",
    "    # chunk-level citations (e.g., context)\n",
    "    chunks = obs.get(\"chunks\")\n",
    "    if isinstance(chunks, list):\n",
    "        for ch in chunks:\n",
    "            if not isinstance(ch, dict):\n",
    "                continue\n",
    "            ch_cites = ch.get(\"citations\")\n",
    "            if isinstance(ch_cites, list):\n",
    "                for c in ch_cites:\n",
    "                    if isinstance(c, dict):\n",
    "                        collected.append(c)\n",
    "\n",
    "    seen = set()\n",
    "    lines: List[str] = []\n",
    "    for c in collected:\n",
    "        title = (c.get(\"title\") or c.get(\"id\") or \"Source\").strip()\n",
    "        url   = (c.get(\"url\") or \"\").strip()\n",
    "        lic   = (c.get(\"license\") or \"—\").strip()\n",
    "        key = (url or title).lower()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        if url:\n",
    "            lines.append(f\"- [{title}]({url}) · {lic}\")\n",
    "        else:\n",
    "            lines.append(f\"- {title} · {lic}\")\n",
    "\n",
    "    return \"**Sources**\\n\" + \"\\n\".join(lines) if lines else \"\"\n",
    "\n",
    "\n",
    "def _maybe_render_image_from_obs(obs) -> ChatMessage | None:\n",
    "    if not isinstance(obs, dict):\n",
    "        return None\n",
    "    path = (\n",
    "        obs.get(\"image_path\")\n",
    "        or obs.get(\"path\")\n",
    "        or obs.get(\"file_path\")\n",
    "        or obs.get(\"local_path\")\n",
    "        or None\n",
    "    )\n",
    "    url = obs.get(\"image_url\") or obs.get(\"url\")\n",
    "    media = path or url\n",
    "    if not media:\n",
    "        return None\n",
    "    return ChatMessage(role=\"assistant\", content=gr.Image(value=media))\n",
    "\n",
    "\n",
    "# --- Async streaming handler for Gradio Chatbot(type=\"messages\") ---\n",
    "async def interact_with_langchain_agent(user_text, history):\n",
    "    \"\"\"\n",
    "    Streams a conversation turn:\n",
    "      - append user msg\n",
    "      - show ⏳ placeholder\n",
    "      - stream agent tool results (image + Sources only)\n",
    "      - stream final answer\n",
    "    Robust to exceptions; will surface errors in-chat and close cleanly.\n",
    "    \"\"\"\n",
    "    # 1) user message\n",
    "    history.append(ChatMessage(role=\"user\", content=user_text))\n",
    "    yield history\n",
    "\n",
    "    # 2) thinking placeholder\n",
    "    thinking_msg = ChatMessage(role=\"assistant\", content=\"⏳ Thinking...\")\n",
    "    history.append(thinking_msg)\n",
    "    yield history\n",
    "\n",
    "    try:\n",
    "        # 3) stream agent\n",
    "        async for chunk in agent_executor.astream({\"input\": user_text}):\n",
    "\n",
    "            # remove placeholder on first real activity\n",
    "            if thinking_msg in history:\n",
    "                try:\n",
    "                    history.remove(thinking_msg)\n",
    "                except ValueError:\n",
    "                    pass  # already removed elsewhere\n",
    "\n",
    "            # show ONLY user-facing results from tools\n",
    "            if \"steps\" in chunk:\n",
    "                for step in chunk[\"steps\"]:\n",
    "                    \n",
    "                    obs = getattr(step, \"observation\", None)\n",
    "                    if not isinstance(obs, dict):\n",
    "                        continue\n",
    "\n",
    "                    # image\n",
    "                    img_msg = _maybe_render_image_from_obs(obs)\n",
    "                    if img_msg is not None:\n",
    "                        history.append(img_msg)\n",
    "                        yield history\n",
    "\n",
    "                    # sources\n",
    "                    sources_md = _format_sources_md(obs)\n",
    "                    if sources_md:\n",
    "                        history.append(ChatMessage(role=\"assistant\", content=sources_md))\n",
    "                        yield history\n",
    "\n",
    "                # yield after processing this chunk\n",
    "                yield history\n",
    "\n",
    "            # final assistant output\n",
    "            if \"output\" in chunk:\n",
    "                history.append(ChatMessage(role=\"assistant\", content=chunk[\"output\"]))\n",
    "                yield history\n",
    "\n",
    "        # finished normally — ensure placeholder is gone\n",
    "        if thinking_msg in history:\n",
    "            try:\n",
    "                history.remove(thinking_msg)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        # surface the real error so you can see it instead of a vague aclose warning\n",
    "        if thinking_msg in history:\n",
    "            try:\n",
    "                history.remove(thinking_msg)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        history.append(ChatMessage(role=\"assistant\", content=f\"⚠️ Error: {e}\"))\n",
    "        yield history\n",
    "\n",
    "    finally:\n",
    "        # give Gradio a tick to close the async generator cleanly\n",
    "        await asyncio.sleep(0)\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d410fb7",
   "metadata": {},
   "source": [
    "### Final Interface\n",
    "\n",
    "**WARNING** Responses took 1-2 minutes to load (M4 MacBook Pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c842eb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_title_str = \"\"\n",
    "# which pack defined in cell 1\n",
    "if which_pack == \"FLORIDA\":\n",
    "    demo_title_str = \"Pinellas, Fl Hurricane\"\n",
    "else:\n",
    "    demo_title_str = \"Village Bihar, India\"\n",
    "\n",
    "# --- Gradio UI ---\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Beacon - Knowledge Agent \" + demo_title_str)\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        type=\"messages\",\n",
    "        label=\"Agent\",\n",
    "        avatar_images=(None, \"Beacon Media Assets/avatar.png\"),\n",
    "        height=650,\n",
    "    )\n",
    "    textbox = gr.Textbox(lines=1, label=\"Chat Message\", placeholder=\"Ask something…\")\n",
    "\n",
    "    # Clear the textbox after submit so it feels chatty\n",
    "    def _clear_now(_msg, _chat):\n",
    "        return gr.update(value=\"\")\n",
    "\n",
    "    # streaming submit\n",
    "    textbox.submit(\n",
    "        interact_with_langchain_agent,\n",
    "        inputs=[textbox, chatbot],\n",
    "        outputs=[chatbot],\n",
    "    )\n",
    "\n",
    "    # instant clear\n",
    "    textbox.submit(\n",
    "        _clear_now,\n",
    "        inputs=[textbox, chatbot],   \n",
    "        outputs=[textbox],\n",
    "        queue=False,\n",
    "    )\n",
    "\n",
    "demo.queue().launch()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1ee5e",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
